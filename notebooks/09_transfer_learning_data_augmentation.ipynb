{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import vgg16, vgg16_bn, resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "from IPython.display import display, clear_output, Image as IPython_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../data/images'\n",
    "LABEL_PATH = '../data/annotations'\n",
    "\n",
    "# GIVEN DATASET\n",
    "MEAN = (0.43672, 0.40107, 0.36762)\n",
    "STD = (0.30139, 0.28781, 0.29236)\n",
    "\n",
    "# IMAGENET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# RESNET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "       \n",
    "# Define default pos_weights for nn.BCEWithLogitsLoss(pos_weights).\n",
    "label_pos_weights_for_loss = np.array([209.52631579, 55.87203791, 58.40594059, 16.77777778, 44.80152672, 5.25, 25.14379085, 5.75675676, 33.09090909, 2.15540363, 5.51465798, 163.38356164, 119., 37.46153846], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_classes():\n",
    "    return len(listdir(LABEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_map():\n",
    "    ret = {}\n",
    "\n",
    "    i = 0\n",
    "    for fname in listdir(LABEL_PATH):\n",
    "        img_class, _ = fname.split('.')\n",
    "        ret[img_class] = i\n",
    "        i += 1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels_to_csv(name_of_set, label_array):\n",
    "    filepath = f'../data/labels_{name_of_set}.csv'\n",
    "    \n",
    "    label_arr = np.array(label_array).astype(int)\n",
    "\n",
    "    # Save 2D numpy array to csv file\n",
    "    np.savetxt(filepath, label_arr, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_fr=.6, max_images_per_class=1e9):\n",
    "    # mapping from class names to integers\n",
    "    class_map = get_class_map()\n",
    "\n",
    "    # create a dictionary to hold our label vectors\n",
    "    n_classes = len(class_map.keys())\n",
    "    img_to_class = defaultdict(lambda: np.zeros(n_classes))\n",
    "\n",
    "    # another dictionary to hold the actual image data\n",
    "    img_to_data = dict()\n",
    "    \n",
    "    # loop through all the annotations\n",
    "    for fname in listdir(LABEL_PATH):\n",
    "        img_class, _ = fname.split('.')\n",
    "        print(f'Reading class: {img_class}')\n",
    "        \n",
    "        # open the annotation file\n",
    "        i = 0\n",
    "        with open(f'{LABEL_PATH}/{fname}', 'r') as fh:\n",
    "\n",
    "            # get image ids from annotation file\n",
    "            img_ids = fh.read().splitlines()\n",
    "            \n",
    "            # gather the images with labels\n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                \n",
    "                # let's not process images unnecessarily\n",
    "                if not img_id in img_to_data:\n",
    "\n",
    "                    img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "                    img = Image.open(img_path)\n",
    "\n",
    "                    # append to dict\n",
    "                    img_to_data[img_id] = img.convert('RGB')\n",
    "\n",
    "                # get one-hot encoded vector of image classes\n",
    "                img_classes = img_to_class[img_id]\n",
    "\n",
    "                # add new class to image vector\n",
    "                img_class_id = class_map[img_class]\n",
    "                img_classes[img_class_id] = 1\n",
    "\n",
    "                # store the updated vector back\n",
    "                img_to_class[img_id] = img_classes\n",
    "\n",
    "                if i >= max_images_per_class:\n",
    "                    break\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    # load also all the images that do not have any labels\n",
    "    i = 0\n",
    "    print(f'Reading images without labels..')\n",
    "    for fname in listdir(IMAGE_PATH):\n",
    "        m = re.match('im(\\d+)', fname)\n",
    "        img_id = m.group(1)\n",
    "\n",
    "        if img_id not in img_to_data:\n",
    "            img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # append to dict\n",
    "            img_to_data[img_id] = img.convert('RGB')\n",
    "\n",
    "            if i >= max_images_per_class:\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    print('Creating train/valid/test split..')\n",
    "    # collect data to a single array\n",
    "    X = []\n",
    "    y = []\n",
    "    for img_id in img_to_data.keys():\n",
    "        X.append(img_to_data[img_id])\n",
    "        y.append(img_to_class[img_id])\n",
    "\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, train_size=train_fr, random_state=42)\n",
    "    X_test, X_valid, y_test, y_valid = train_test_split(X_tmp, y_tmp, train_size=.5, test_size=.5, random_state=42)\n",
    "    \n",
    "    print('Done.')\n",
    "\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, transforms=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_data = self.X[idx]\n",
    "        img_class = self.y[idx]\n",
    "\n",
    "        if transforms:\n",
    "            img_data = self.transforms(img_data)\n",
    "\n",
    "        return img_data, img_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_classes):\n",
    "        super().__init__()\n",
    "        self.bs = bs\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden1 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden2 = nn.Linear(n_hidden2, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.hidden2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class OneLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'X.SHAPE: {x.shape}')\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class ConvNetModel(nn.Module):\n",
    "    def __init__(self, n_classes, keep_prob=.5):\n",
    "        super(ConvNetModel, self).__init__()\n",
    "        # Common layers used multiple times\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=1-keep_prob)\n",
    "        \n",
    "        # Unique layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=3, stride=1, padding=1) #(n samples, channels, height, width)\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=14, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc3 = nn.Linear(in_features=256*4*14, out_features=n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3, 128, 128)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)  # Flatten for FC\n",
    "        out = self.fc3(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, device, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    f1_scores = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                score = f1_score(y.cpu() == 1, y_pred.cpu() > threshold, average='micro')\n",
    "                f1_scores.append(score)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, model, optimizer, scheduler, criterion, device, n_epochs=50, verbose=True):\n",
    "    model.train()\n",
    "\n",
    "    if verbose:\n",
    "        fmt = '{:<5} {:12} {:12} {:<9} {:<9}'\n",
    "        print(fmt.format('Epoch', 'Train loss', 'Valid loss', 'Train F1', 'Valid F1'))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            X, y = batch\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            print(f'Epoch: {epoch+1}, iteration: {i+1}, loss: {loss}')\n",
    "\n",
    "        if verbose:\n",
    "            train_loss, train_score = evaluate(train_dataloader, model, criterion, device)\n",
    "            valid_loss, valid_score = evaluate(valid_dataloader, model, criterion, device)\n",
    "\n",
    "            fmt = '{:<5} {:03.10f} {:03.10f} {:02.7f} {:02.7f}'\n",
    "            print(fmt.format(epoch, train_loss, valid_loss, train_score, valid_score))\n",
    "            \n",
    "    print('Done training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_true_positives(img_label, model, device, dataloader, n_to_show=10, threshold=0.75):\n",
    "\n",
    "    class_map = get_class_map()\n",
    "    img_class = class_map[img_label]\n",
    "\n",
    "    def predicate(pred_classes, true_classes):\n",
    "        return True if img_class in pred_classes and img_class in true_classes else False\n",
    "\n",
    "    visualize_predictions(model, device, dataloader, n_to_show=n_to_show, predicate=predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_true_negatives(img_label, model, device, dataloader, n_to_show=10, threshold=0.75):\n",
    "\n",
    "    class_map = get_class_map()\n",
    "    img_class = class_map[img_label]\n",
    "\n",
    "    def predicate(pred_classes, true_classes):\n",
    "        return True if not img_class in pred_classes and not img_class in true_classes else False\n",
    "\n",
    "    visualize_predictions(model, device, dataloader, n_to_show=n_to_show, predicate=predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_false_positives(img_label, model, device, dataloader, n_to_show=10, threshold=0.75):\n",
    "\n",
    "    class_map = get_class_map()\n",
    "    img_class = class_map[img_label]\n",
    "\n",
    "    def predicate(pred_classes, true_classes):\n",
    "        return True if img_class in pred_classes and not img_class in true_classes else False\n",
    "\n",
    "    visualize_predictions(model, device, dataloader, n_to_show=n_to_show, predicate=predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_false_negatives(img_label, model, device, dataloader, n_to_show=10, threshold=0.75):\n",
    "\n",
    "    class_map = get_class_map()\n",
    "    img_class = class_map[img_label]\n",
    "\n",
    "    def predicate(pred_classes, true_classes):\n",
    "        return True if not img_class in pred_classes and img_class in true_classes else False\n",
    "\n",
    "    visualize_predictions(model, device, dataloader, n_to_show=n_to_show, predicate=predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, device, dataloader, mean=MEAN, std=STD, n_to_show=10, threshold=0.5, predicate=None):\n",
    "    \n",
    "    if predicate == None:\n",
    "        predicate = lambda p1, p2: True\n",
    "\n",
    "    class_to_label = { v: k for k, v in get_class_map().items() }\n",
    "    \n",
    "    # https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/5\n",
    "    inv_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean = -1 * np.multiply(mean, std), std=np.divide(1, std))\n",
    "    ])\n",
    "    \n",
    "    n_shown = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        X, y = batch\n",
    "        X = X.to(device)\n",
    "\n",
    "        y_pred_raw = model(X).cpu()\n",
    "        y_pred = y_pred_raw > threshold \n",
    "        y = y == 1\n",
    "        \n",
    "        for i in range(len(y)):\n",
    "            pred_classes = np.where(y_pred[i] == 1)[0]\n",
    "            true_classes = np.where(y[i] == 1)[0]\n",
    "            \n",
    "            if not predicate(pred_classes, true_classes):\n",
    "                continue\n",
    "\n",
    "            show_image_with_predictions(X[i], true_classes, pred_classes)\n",
    "\n",
    "            n_shown += 1\n",
    "            \n",
    "            if n_shown >= n_to_show:\n",
    "                return            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_predictions(X, true_classes, pred_classes, mean=MEAN, std=STD):\n",
    "    \n",
    "    class_map = get_class_map()\n",
    "    class_to_label = { v: k for k, v in class_map.items() }\n",
    "\n",
    "    true_classes_str = ', '.join([class_to_label[i] for i in true_classes])\n",
    "    pred_classes_str = ', '.join([class_to_label[i] for i in pred_classes])\n",
    "\n",
    "    # https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/5\n",
    "    inv_transform = transforms.Normalize(mean = -1 * np.multiply(mean, std), std=np.divide(1, std))\n",
    "\n",
    "    img = inv_transform(X.cpu()) # inverse transforms\n",
    "    img = img.permute(2, 1, 0)   # BGR -> RGB\n",
    "    img = np.rot90(img, 3)\n",
    "\n",
    "    plt.title(f'True: {true_classes_str}, Predictions: {pred_classes_str}')\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def predict_X(fr, threshold=0.5):\n",
    "    \n",
    "    y_hat = fr > threshold\n",
    "    \n",
    "    return y_hat if (np.sum(y_hat) > 0) else fr == np.max(fr)\n",
    "\n",
    "def predict(model, device, dataloader):\n",
    "    \n",
    "    ys_all = []  # Array of np.array(14) \n",
    "    y_hats_all = []\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        Xs, ys = batch\n",
    "        Xs = model(Xs.to(device))\n",
    "        y_hats = np.apply_along_axis(predict_X, axis=1, arr=Xs.cpu().detach().numpy())\n",
    "        \n",
    "        y_hats_all.extend(y_hat for y_hat in y_hats)\n",
    "        ys_all.extend(y.numpy() for y in ys==1)\n",
    "\n",
    "    return np.array(ys_all), np.array(y_hats_all)\n",
    "\n",
    "def visualize_confusion_matrix(y_true, y_pred, labels, file_path):\n",
    "\n",
    "    plt.ioff()\n",
    "    \n",
    "    # Get confusion matrices\n",
    "    cn_tensor = skm.multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Get precision, recall, f1-score\n",
    "    scores = skm.classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=5, ncols=3,sharey=True, figsize=(20, 20), \n",
    "                           gridspec_kw={'hspace': 0.3, 'wspace': 0.0})\n",
    "    gn = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    n = cn_tensor[0].sum()\n",
    "    \n",
    "    # Loop all labels\n",
    "    for i, cn_matrix in enumerate(cn_tensor):\n",
    "\n",
    "        j, k = int(i/3), i%3\n",
    "        \n",
    "        # Annotations\n",
    "        annot = np.asarray(\n",
    "            ['{}\\n{:0.0f}\\n{:.2%}'.format(gn[i], x, x/n) for i, x in enumerate(cn_matrix.flatten())]\n",
    "        ).reshape(2,2)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sn.heatmap(cn_matrix, annot=annot, fmt='', cmap='Blues', ax=ax[j, k])\n",
    "        \n",
    "        # Precision, recall, f1-score\n",
    "        title = '{}\\nprec.={:.3}, rec.={:.3}, f1={:.3}'.format(\n",
    "            labels[i], scores[str(i)]['precision'], scores[str(i)]['recall'], scores[str(i)]['f1-score'])\n",
    "        ax[j, k].set_title(title)\n",
    "        \n",
    "    plt.savefig(file_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Using GPU!')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "lr = 0.01\n",
    "n_epochs = 20\n",
    "bs = 64\n",
    "n_classes = len(get_class_map().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save / load dataloaders from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomIndividualApply():\n",
    "    \"\"\"Apply randomly a list of transformations with a given probability\n",
    "\n",
    "    Args:\n",
    "        transforms (list or tuple): list of transformations\n",
    "        p (float): probability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms, p=0.5):\n",
    "        self.p = p\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            if self.p < np.random.random():\n",
    "                continue\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        format_string += '\\n    p={}'.format(self.p)\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved torch dump from disk.\n",
      " - Done.\n"
     ]
    }
   ],
   "source": [
    "max_images_per_class = int(1e9)\n",
    "#max_images_per_class = 200\n",
    "\n",
    "transformations = {\n",
    "    'train': transforms.Compose([\n",
    "        RandomIndividualApply([\n",
    "            transforms.RandomHorizontalFlip(p=1),\n",
    "            transforms.RandomRotation((-10, 10)),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.RandomPerspective(),\n",
    "        ], p=0.5),\n",
    "        transforms.ToTensor(),                \n",
    "        transforms.Normalize(mean=MEAN, std=STD)            \n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "if not os.path.isfile(f'../data/X_train_n{max_images_per_class}.dat'):\n",
    "    print('Loading all data from disk.')\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = get_data(max_images_per_class=max_images_per_class)\n",
    "    torch.save(X_train, f'../data/X_train_n{max_images_per_class}.dat')\n",
    "    torch.save(X_valid, f'../data/X_valid_n{max_images_per_class}.dat')\n",
    "    torch.save(X_test, f'../data/X_test_n{max_images_per_class}.dat')\n",
    "    torch.save(y_train, f'../data/y_train_n{max_images_per_class}.dat')\n",
    "    torch.save(y_valid, f'../data/y_valid_n{max_images_per_class}.dat')\n",
    "    torch.save(y_test, f'../data/y_test_n{max_images_per_class}.dat')\n",
    "    print(' - Done.')\n",
    "else:\n",
    "    print('Loading saved torch dump from disk.')\n",
    "    X_train = torch.load(f'../data/X_train_n{max_images_per_class}.dat')\n",
    "    X_valid = torch.load(f'../data/X_valid_n{max_images_per_class}.dat')\n",
    "    X_test = torch.load(f'../data/X_test_n{max_images_per_class}.dat')\n",
    "    y_train = torch.load(f'../data/y_train_n{max_images_per_class}.dat')\n",
    "    y_valid = torch.load(f'../data/y_valid_n{max_images_per_class}.dat')\n",
    "    y_test = torch.load(f'../data/y_test_n{max_images_per_class}.dat')\n",
    "    print(' - Done.')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    TransformingDataset(X_train, y_train, transforms=transformations['train']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    TransformingDataset(X_valid, y_valid, transforms=transformations['valid']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    TransformingDataset(X_test, y_test, transforms=transformations['test']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: The mean and std in transformations most probably need to be the same as for VGG and RESNET. Not 100% sure about this. Something to investigate!\n",
    "\n",
    "More models here: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "\n",
    "If the models do not start to converge, try lowering the learning rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_VGG16_\n",
    "\n",
    "Currently getting validation f1 scores around 0.67. \n",
    "\n",
    "Now around 0.71 with one cycle policy.\n",
    "\n",
    "Surprisingly after quick testing the vgg16_bn (with BatchNorm layers) did not do as well? Maybe more to investigate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = vgg16(pretrained=True).to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(25088, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4096, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 14),\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_RESNET_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet18(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet34(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet50(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(2048, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet101(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(2048, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet152(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(2048, 14).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model or load an existing model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "pos_weight = torch.from_numpy(label_pos_weights_for_loss).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# learning rate and momentum will be overriden by the scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.01,\n",
    "    base_momentum=0.5,\n",
    "    max_momentum=0.95,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for model saving and loading\n",
    "\n",
    "# ResNet 101\n",
    "#model_save_path = '../data/resnet-valid-acc-aug-0.70.pth'\n",
    "#model_save_path = '../data/resnet-101-valid-acc-0.73-2.pth'\n",
    "\n",
    "#model_save_path = '../data/resnet-152-valid-acc-0.722.pth'\n",
    "#model_save_path = '../data/resnet-152-valid-acc-0.727.pth'\n",
    "\n",
    "# VGG16\n",
    "#model_save_path = '../data/vgg16-9epochs-valid-acc-0.66.pth'\n",
    "#model_save_path = '../data/vgg16-valid-acc-0.71.pth'\n",
    "\n",
    "# Whole model\n",
    "#model_whole_save_path = '../data/vgg16-7epochs-valid-acc-0.703.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plain model saving and loading (only state dictionary).\n",
    "#torch.save(model.state_dict(), model_save_path)\n",
    "#model.load_state_dict(torch.load(model_save_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an entire model (not just the state dict)\n",
    "#torch.save(model, model_whole_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an entire model (not just the state dict)\n",
    "#model = torch.load(model_whole_save_path)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for trying to train all the layers...\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train loss   Valid loss   Train F1  Valid F1 \n",
      "Epoch: 1, iteration: 1, loss: 1.8964457185890915\n",
      "Epoch: 1, iteration: 2, loss: 1.5312530066773926\n",
      "Epoch: 1, iteration: 3, loss: 1.205037853074431\n",
      "Epoch: 1, iteration: 4, loss: 2.445477245067998\n",
      "Epoch: 1, iteration: 5, loss: 1.089060573979111\n",
      "Epoch: 1, iteration: 6, loss: 1.3856551632963205\n",
      "Epoch: 1, iteration: 7, loss: 1.4345846468406651\n",
      "Epoch: 1, iteration: 8, loss: 1.5441591819060105\n",
      "Epoch: 1, iteration: 9, loss: 1.07992039354217\n",
      "Epoch: 1, iteration: 10, loss: 2.2999645893906036\n",
      "Epoch: 1, iteration: 11, loss: 1.474575619131359\n",
      "Epoch: 1, iteration: 12, loss: 2.058048578681729\n",
      "Epoch: 1, iteration: 13, loss: 1.5364437105982898\n",
      "Epoch: 1, iteration: 14, loss: 1.8652795995703944\n",
      "Epoch: 1, iteration: 15, loss: 1.4971084997416642\n",
      "Epoch: 1, iteration: 16, loss: 1.1855796237195304\n",
      "Epoch: 1, iteration: 17, loss: 1.457357025970204\n",
      "Epoch: 1, iteration: 18, loss: 1.2528945328905472\n",
      "Epoch: 1, iteration: 19, loss: 1.1795454778585217\n",
      "Epoch: 1, iteration: 20, loss: 1.3036905269502714\n",
      "Epoch: 1, iteration: 21, loss: 1.0065467845541645\n",
      "Epoch: 1, iteration: 22, loss: 1.0547789732424107\n",
      "Epoch: 1, iteration: 23, loss: 0.6651665328145848\n",
      "Epoch: 1, iteration: 24, loss: 0.8583905988998405\n",
      "Epoch: 1, iteration: 25, loss: 0.8874443318852516\n",
      "Epoch: 1, iteration: 26, loss: 0.81698398115834\n",
      "Epoch: 1, iteration: 27, loss: 0.7190686527813458\n",
      "Epoch: 1, iteration: 28, loss: 1.2363889084402007\n",
      "Epoch: 1, iteration: 29, loss: 0.8327128839651351\n",
      "Epoch: 1, iteration: 30, loss: 0.8791767745416791\n",
      "Epoch: 1, iteration: 31, loss: 1.4257555597361016\n",
      "Epoch: 1, iteration: 32, loss: 0.6220496381048857\n",
      "Epoch: 1, iteration: 33, loss: 0.6555619960770364\n",
      "Epoch: 1, iteration: 34, loss: 0.7883985021180734\n",
      "Epoch: 1, iteration: 35, loss: 0.7881829134365234\n",
      "Epoch: 1, iteration: 36, loss: 0.8455142168926094\n",
      "Epoch: 1, iteration: 37, loss: 0.8579571139270563\n",
      "Epoch: 1, iteration: 38, loss: 0.7312388967315712\n",
      "Epoch: 1, iteration: 39, loss: 0.8765256365975701\n",
      "Epoch: 1, iteration: 40, loss: 0.5772391359399899\n",
      "Epoch: 1, iteration: 41, loss: 1.0259936011375284\n",
      "Epoch: 1, iteration: 42, loss: 0.5636320611541481\n",
      "Epoch: 1, iteration: 43, loss: 0.8345147000705705\n",
      "Epoch: 1, iteration: 44, loss: 0.5946425014146967\n",
      "Epoch: 1, iteration: 45, loss: 0.807117274179646\n",
      "Epoch: 1, iteration: 46, loss: 0.7356917803666696\n",
      "Epoch: 1, iteration: 47, loss: 0.6392653094077861\n",
      "Epoch: 1, iteration: 48, loss: 0.6974187053559632\n",
      "Epoch: 1, iteration: 49, loss: 0.5877382022170211\n",
      "Epoch: 1, iteration: 50, loss: 1.0967072538001195\n",
      "Epoch: 1, iteration: 51, loss: 0.5739123426463392\n",
      "Epoch: 1, iteration: 52, loss: 0.6534939417908346\n",
      "Epoch: 1, iteration: 53, loss: 1.4205228894806716\n",
      "Epoch: 1, iteration: 54, loss: 1.0145633499543352\n",
      "Epoch: 1, iteration: 55, loss: 0.6865679505359721\n",
      "Epoch: 1, iteration: 56, loss: 0.5732308634332225\n",
      "Epoch: 1, iteration: 57, loss: 0.8487346589059028\n",
      "Epoch: 1, iteration: 58, loss: 0.946489709246717\n",
      "Epoch: 1, iteration: 59, loss: 1.1374825408542604\n",
      "Epoch: 1, iteration: 60, loss: 0.5386233240587303\n",
      "Epoch: 1, iteration: 61, loss: 1.0426337487941464\n",
      "Epoch: 1, iteration: 62, loss: 0.5792830905320414\n",
      "Epoch: 1, iteration: 63, loss: 0.5860496187655213\n",
      "Epoch: 1, iteration: 64, loss: 1.0417591654871952\n",
      "Epoch: 1, iteration: 65, loss: 1.910057312668043\n",
      "Epoch: 1, iteration: 66, loss: 0.6246663122378098\n",
      "Epoch: 1, iteration: 67, loss: 0.8558501251568118\n",
      "Epoch: 1, iteration: 68, loss: 1.2468434267871495\n",
      "Epoch: 1, iteration: 69, loss: 0.8118021469160366\n",
      "Epoch: 1, iteration: 70, loss: 0.7485862667858635\n",
      "Epoch: 1, iteration: 71, loss: 0.7811810081292998\n",
      "Epoch: 1, iteration: 72, loss: 0.9599840485162473\n",
      "Epoch: 1, iteration: 73, loss: 0.8248996347268003\n",
      "Epoch: 1, iteration: 74, loss: 0.8405238286837333\n",
      "Epoch: 1, iteration: 75, loss: 0.5708776606380306\n",
      "Epoch: 1, iteration: 76, loss: 0.6644129885572632\n",
      "Epoch: 1, iteration: 77, loss: 0.777966582571654\n",
      "Epoch: 1, iteration: 78, loss: 0.6398684964931471\n",
      "Epoch: 1, iteration: 79, loss: 0.8437275487791444\n",
      "Epoch: 1, iteration: 80, loss: 0.7688156459534392\n",
      "Epoch: 1, iteration: 81, loss: 0.633237602200101\n",
      "Epoch: 1, iteration: 82, loss: 0.6896304816259202\n",
      "Epoch: 1, iteration: 83, loss: 0.9612688484794847\n",
      "Epoch: 1, iteration: 84, loss: 0.8947033556558668\n",
      "Epoch: 1, iteration: 85, loss: 0.9510444392670931\n",
      "Epoch: 1, iteration: 86, loss: 0.5832109431996173\n",
      "Epoch: 1, iteration: 87, loss: 0.8745081122166205\n",
      "Epoch: 1, iteration: 88, loss: 1.234331764792836\n",
      "Epoch: 1, iteration: 89, loss: 0.7107421087164928\n",
      "Epoch: 1, iteration: 90, loss: 0.6499802342497387\n",
      "Epoch: 1, iteration: 91, loss: 0.8662937796095372\n",
      "Epoch: 1, iteration: 92, loss: 0.6351313151751176\n",
      "Epoch: 1, iteration: 93, loss: 0.6498680595434702\n",
      "Epoch: 1, iteration: 94, loss: 0.69449490430457\n",
      "Epoch: 1, iteration: 95, loss: 0.719130273583443\n",
      "Epoch: 1, iteration: 96, loss: 0.9028865503281829\n",
      "Epoch: 1, iteration: 97, loss: 0.8045416350031436\n",
      "Epoch: 1, iteration: 98, loss: 0.7093163205002645\n",
      "Epoch: 1, iteration: 99, loss: 0.6152124962084657\n",
      "Epoch: 1, iteration: 100, loss: 0.9546548765764242\n",
      "Epoch: 1, iteration: 101, loss: 0.8371985712525589\n",
      "Epoch: 1, iteration: 102, loss: 0.7910545335048632\n",
      "Epoch: 1, iteration: 103, loss: 0.6801127281031398\n",
      "Epoch: 1, iteration: 104, loss: 0.6199503520209249\n",
      "Epoch: 1, iteration: 105, loss: 0.8007833337107171\n",
      "Epoch: 1, iteration: 106, loss: 0.5940459614477501\n",
      "Epoch: 1, iteration: 107, loss: 0.590806892013207\n",
      "Epoch: 1, iteration: 108, loss: 1.5262868698754593\n",
      "Epoch: 1, iteration: 109, loss: 0.7300584427180024\n",
      "Epoch: 1, iteration: 110, loss: 0.5607284438193081\n",
      "Epoch: 1, iteration: 111, loss: 0.6617820387798664\n",
      "Epoch: 1, iteration: 112, loss: 0.6471789476860041\n",
      "Epoch: 1, iteration: 113, loss: 0.660416893615909\n",
      "Epoch: 1, iteration: 114, loss: 0.6418111071318079\n",
      "Epoch: 1, iteration: 115, loss: 0.7427252458806937\n",
      "Epoch: 1, iteration: 116, loss: 0.754860494343222\n",
      "Epoch: 1, iteration: 117, loss: 0.6017488490786546\n",
      "Epoch: 1, iteration: 118, loss: 0.7386130439149972\n",
      "Epoch: 1, iteration: 119, loss: 0.8467170570325151\n",
      "Epoch: 1, iteration: 120, loss: 0.7572014726486014\n",
      "Epoch: 1, iteration: 121, loss: 0.5900804929887924\n",
      "Epoch: 1, iteration: 122, loss: 0.6339390703086384\n",
      "Epoch: 1, iteration: 123, loss: 0.5576004070007966\n",
      "Epoch: 1, iteration: 124, loss: 0.6010661856238396\n",
      "Epoch: 1, iteration: 125, loss: 0.7578494256664877\n",
      "Epoch: 1, iteration: 126, loss: 0.6811592778175267\n",
      "Epoch: 1, iteration: 127, loss: 0.5393297731148341\n",
      "Epoch: 1, iteration: 128, loss: 0.6252859702623018\n",
      "Epoch: 1, iteration: 129, loss: 1.6622629434221423\n",
      "Epoch: 1, iteration: 130, loss: 0.5741221477584081\n",
      "Epoch: 1, iteration: 131, loss: 0.5832185152634705\n",
      "Epoch: 1, iteration: 132, loss: 0.8249808173512453\n",
      "Epoch: 1, iteration: 133, loss: 0.6253731893555677\n",
      "Epoch: 1, iteration: 134, loss: 0.7105343966532623\n",
      "Epoch: 1, iteration: 135, loss: 0.6489261422366805\n",
      "Epoch: 1, iteration: 136, loss: 0.7323076858989779\n",
      "Epoch: 1, iteration: 137, loss: 0.8095880914751978\n",
      "Epoch: 1, iteration: 138, loss: 0.6664095301143999\n",
      "Epoch: 1, iteration: 139, loss: 0.7214975735014738\n",
      "Epoch: 1, iteration: 140, loss: 0.8060408169036996\n",
      "Epoch: 1, iteration: 141, loss: 0.5673498143826768\n",
      "Epoch: 1, iteration: 142, loss: 0.6206771801494908\n",
      "Epoch: 1, iteration: 143, loss: 0.6582236273632623\n",
      "Epoch: 1, iteration: 144, loss: 0.6226986722793985\n",
      "Epoch: 1, iteration: 145, loss: 0.6565746657163125\n",
      "Epoch: 1, iteration: 146, loss: 0.5030619820419738\n",
      "Epoch: 1, iteration: 147, loss: 0.8532191044296799\n",
      "Epoch: 1, iteration: 148, loss: 0.6654214369789511\n",
      "Epoch: 1, iteration: 149, loss: 0.6259427061014152\n",
      "Epoch: 1, iteration: 150, loss: 0.6495318473732019\n",
      "Epoch: 1, iteration: 151, loss: 0.5159130703037006\n",
      "Epoch: 1, iteration: 152, loss: 1.0170324726806188\n",
      "Epoch: 1, iteration: 153, loss: 0.42710049901805824\n",
      "Epoch: 1, iteration: 154, loss: 0.6023143954770875\n",
      "Epoch: 1, iteration: 155, loss: 0.5990387247987013\n",
      "Epoch: 1, iteration: 156, loss: 0.511120361856718\n",
      "Epoch: 1, iteration: 157, loss: 1.1589392056445604\n",
      "Epoch: 1, iteration: 158, loss: 0.6364700917732912\n",
      "Epoch: 1, iteration: 159, loss: 0.6640910331359203\n",
      "Epoch: 1, iteration: 160, loss: 0.6798134837213818\n",
      "Epoch: 1, iteration: 161, loss: 0.5366742503262472\n",
      "Epoch: 1, iteration: 162, loss: 0.5522258966153942\n",
      "Epoch: 1, iteration: 163, loss: 0.5778569822130861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, iteration: 164, loss: 0.6585588175899216\n",
      "Epoch: 1, iteration: 165, loss: 0.7005810471610256\n",
      "Epoch: 1, iteration: 166, loss: 0.5216516300107259\n",
      "Epoch: 1, iteration: 167, loss: 0.9224948862428145\n",
      "Epoch: 1, iteration: 168, loss: 0.6280418703602068\n",
      "Epoch: 1, iteration: 169, loss: 0.509796948756952\n",
      "Epoch: 1, iteration: 170, loss: 0.5777313053618585\n",
      "Epoch: 1, iteration: 171, loss: 0.6119111796228451\n",
      "Epoch: 1, iteration: 172, loss: 0.8284249457062111\n",
      "Epoch: 1, iteration: 173, loss: 0.4877267180198445\n",
      "Epoch: 1, iteration: 174, loss: 0.5086413955650241\n",
      "Epoch: 1, iteration: 175, loss: 0.920027278738907\n",
      "Epoch: 1, iteration: 176, loss: 0.8503940755240984\n",
      "Epoch: 1, iteration: 177, loss: 0.5287696917742246\n",
      "Epoch: 1, iteration: 178, loss: 0.9926711165641899\n",
      "Epoch: 1, iteration: 179, loss: 0.5896264449406153\n",
      "Epoch: 1, iteration: 180, loss: 0.5527932011733551\n",
      "Epoch: 1, iteration: 181, loss: 0.5241257734239999\n",
      "Epoch: 1, iteration: 182, loss: 0.7001277885407851\n",
      "Epoch: 1, iteration: 183, loss: 0.6774182638053954\n",
      "Epoch: 1, iteration: 184, loss: 0.5846675239865031\n",
      "Epoch: 1, iteration: 185, loss: 0.6619710522156453\n",
      "Epoch: 1, iteration: 186, loss: 0.7604292090551212\n",
      "Epoch: 1, iteration: 187, loss: 0.5977116578288175\n",
      "Epoch: 1, iteration: 188, loss: 0.5615906674115358\n",
      "0     0.6367664761 0.7776911802 0.4936477 0.5253183\n",
      "Done training!\n",
      "CPU times: user 3min 41s, sys: 1min 59s, total: 5min 41s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%time train(train_dataloader, valid_dataloader, model, optimizer, scheduler, criterion, device, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(valid_dataloader, model, criterion, device, threshold=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for finding the optimal threshold.\n",
    "if True:\n",
    "    f1_scores = []\n",
    "    for threshold in np.arange(0.05, 1, 0.05):\n",
    "        _, f1 = evaluate(valid_dataloader, model, criterion, device, threshold=threshold)\n",
    "        f1_scores.append(f1)\n",
    "        print(f'threshold: {threshold}, f1 score: {f1}')\n",
    "\n",
    "    plt.plot(np.arange(0.05, 1, 0.05), f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some images with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, device, test_dataloader, n_to_show=5, threshold=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TP/FP/TN/FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_label = 'baby'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_true_positives(img_label, model, device, valid_dataloader, n_to_show=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_false_positives(img_label, model, device, valid_dataloader, n_to_show=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_true_negatives(img_label, model, device, valid_dataloader, n_to_show=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_false_negatives(img_label, model, device, valid_dataloader, n_to_show=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re_predict = True\n",
    "\n",
    "if re_predict:\n",
    "\n",
    "    # Predict\n",
    "    y_true, y_pred = predict(model, device, valid_dataloader)\n",
    "    np.save(f'../data/valid_true_labels.npy', y_true)\n",
    "    np.save(f'../data/valid_pred_labels.npy', y_pred)\n",
    "    \n",
    "    # Save classification report\n",
    "    with open(f'../data/valid_classification_report.txt', 'w') as file:\n",
    "        file.write(skm.classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Save confusion matrix plot\n",
    "    labels = [k for k, v in get_class_map().items()]\n",
    "    visualize_confusion_matrix(y_true, y_pred, labels, f'../data/valid_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification report\n",
    "with open(f'../data/valid_classification_report.txt', 'r') as file:\n",
    "    report = ''.join(file.readlines())\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confus|ion matrix plot\n",
    "IPython_Image(filename=f'../data/valid_confusion_matrix.png', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
