{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import vgg16, vgg16_bn, resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "from IPython.display import display, clear_output, Image as IPython_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/akir/school/dl/project/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../data/images'\n",
    "LABEL_PATH = '../data/annotations'\n",
    "\n",
    "# GIVEN DATASET\n",
    "MEAN = (0.43672, 0.40107, 0.36762)\n",
    "STD = (0.30139, 0.28781, 0.29236)\n",
    "\n",
    "# IMAGENET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# RESNET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "       \n",
    "# Define default pos_weights for nn.BCEWithLogitsLoss(pos_weights).\n",
    "label_pos_weights_for_loss = np.array([209.52631579, 55.87203791, 58.40594059, 16.77777778, 44.80152672, 5.25, 25.14379085, 5.75675676, 33.09090909, 2.15540363, 5.51465798, 163.38356164, 119., 37.46153846], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_classes():\n",
    "    return len(listdir(LABEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_map():\n",
    "    ret = {}\n",
    "\n",
    "    i = 0\n",
    "    for fname in sorted(listdir(LABEL_PATH)):\n",
    "        img_class, _ = fname.split('.')\n",
    "        ret[img_class] = i\n",
    "        i += 1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels_to_csv(name_of_set, label_array):\n",
    "    filepath = f'../data/labels_{name_of_set}.csv'\n",
    "    \n",
    "    label_arr = np.array(label_array).astype(int)\n",
    "\n",
    "    # Save 2D numpy array to csv file\n",
    "    np.savetxt(filepath, label_arr, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_fr=.6, max_images_per_class=1e9, LABEL_PATH=LABEL_PATH, IMAGE_PATH=IMAGE_PATH):\n",
    "    \n",
    "    # mapping from class names to integers\n",
    "    class_map = get_class_map()\n",
    "\n",
    "    # create a dictionary to hold our label vectors\n",
    "    n_classes = len(class_map.keys())\n",
    "    img_to_class = defaultdict(lambda: np.zeros(n_classes))\n",
    "\n",
    "    # another dictionary to hold the actual image data\n",
    "    img_to_data = dict()\n",
    "    \n",
    "    # loop through all the annotations\n",
    "    for fname in sorted(listdir(LABEL_PATH)):\n",
    "        img_class, _ = fname.split('.')\n",
    "        print(f'Reading class: {img_class}')\n",
    "        \n",
    "        # open the annotation file\n",
    "        i = 0\n",
    "        with open(f'{LABEL_PATH}/{fname}', 'r') as fh:\n",
    "\n",
    "            # get image ids from annotation file\n",
    "            img_ids = fh.read().splitlines()\n",
    "            \n",
    "            # gather the images with labels\n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                \n",
    "                # let's not process images unnecessarily\n",
    "                if not img_id in img_to_data:\n",
    "\n",
    "                    img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "                    img = Image.open(img_path)\n",
    "\n",
    "                    # append to dict\n",
    "                    img_to_data[img_id] = img.convert('RGB')\n",
    "\n",
    "                # get one-hot encoded vector of image classes\n",
    "                img_classes = img_to_class[img_id]\n",
    "\n",
    "                # add new class to image vector\n",
    "                img_class_id = class_map[img_class]\n",
    "                img_classes[img_class_id] = 1\n",
    "\n",
    "                # store the updated vector back\n",
    "                img_to_class[img_id] = img_classes\n",
    "\n",
    "                if i >= max_images_per_class:\n",
    "                    break\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    # load also all the images that do not have any labels\n",
    "    i = 0\n",
    "    print(f'Reading images without labels..')\n",
    "    for fname in listdir(IMAGE_PATH):\n",
    "        m = re.match('im(\\d+)', fname)\n",
    "        img_id = m.group(1)\n",
    "\n",
    "        if img_id not in img_to_data:\n",
    "            img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # append to dict\n",
    "            img_to_data[img_id] = img.convert('RGB')\n",
    "\n",
    "            if i >= max_images_per_class:\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    print('Creating train/valid/test split..')\n",
    "    # collect data to a single array\n",
    "    X = []\n",
    "    y = []\n",
    "    for img_id in img_to_data.keys():\n",
    "        X.append(img_to_data[img_id])\n",
    "        y.append(img_to_class[img_id])\n",
    "\n",
    "    if train_fr == 1:\n",
    "        print('Done.')\n",
    "        return X, y, [], [], [], []\n",
    "        \n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, train_size=train_fr, random_state=42)\n",
    "    X_test, X_valid, y_test, y_valid = train_test_split(X_tmp, y_tmp, train_size=.5, test_size=.5, random_state=42)\n",
    "    \n",
    "    print('Done.')\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, transforms=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_data = self.X[idx]\n",
    "        img_class = self.y[idx]\n",
    "\n",
    "        if transforms:\n",
    "            img_data = self.transforms(img_data)\n",
    "\n",
    "        return img_data, img_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_classes):\n",
    "        super().__init__()\n",
    "        self.bs = bs\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden1 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden2 = nn.Linear(n_hidden2, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.hidden2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class OneLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'X.SHAPE: {x.shape}')\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class ConvNetModel(nn.Module):\n",
    "    def __init__(self, n_classes, keep_prob=.5):\n",
    "        super(ConvNetModel, self).__init__()\n",
    "        # Common layers used multiple times\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=1-keep_prob)\n",
    "        \n",
    "        # Unique layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=3, stride=1, padding=1) #(n samples, channels, height, width)\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=14, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc3 = nn.Linear(in_features=256*4*14, out_features=n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3, 128, 128)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)  # Flatten for FC\n",
    "        out = self.fc3(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, device, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    f1_scores = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = torch.sigmoid(model(X))\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                score = f1_score(y.cpu() == 1, y_pred.cpu() > threshold, average='micro')\n",
    "                f1_scores.append(score)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, model, optimizer, scheduler, criterion, device, n_epochs=50, verbose=True, threshold=0.5, n_report_fr=10):\n",
    "    model.train()\n",
    "    \n",
    "    train_losses, valid_losses = [], []\n",
    "    train_scores, valid_scores = [], []\n",
    "\n",
    "    fmt = '{:<5} {:12} {:12} {:<9} {:<9}'\n",
    "    print(fmt.format('Epoch', 'Train loss', 'Valid loss', 'Train F1', 'Valid F1'))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            X, y = batch\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if verbose and i % n_report_fr == 0:\n",
    "                print(f'Epoch: {epoch+1}, iteration: {i+1}, loss: {loss}')\n",
    "            \n",
    "        if verbose:            \n",
    "            print(f'Epoch: {epoch+1}, iteration: {i+1}, loss: {loss}')\n",
    "\n",
    "        train_loss, train_score = evaluate(train_dataloader, model, criterion, device, threshold)\n",
    "        valid_loss, valid_score = evaluate(valid_dataloader, model, criterion, device, threshold)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_scores.append(train_score)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_scores.append(valid_score)\n",
    "\n",
    "        fmt = '{:<5} {:03.10f} {:03.10f} {:02.7f} {:02.7f}'\n",
    "        print(fmt.format(epoch+1, train_loss, valid_loss, train_score, valid_score))\n",
    "            \n",
    "    print('Done training!')\n",
    "    \n",
    "    return train_losses, valid_losses, train_scores, valid_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_true_positives(img_label, model, device, dataloader, n_to_show=10, threshold=0.75):\n",
    "\n",
    "    class_map = get_class_map()\n",
    "    img_class = class_map[img_label]\n",
    "\n",
    "    def predicate(pred_classes, true_classes):\n",
    "        return True if img_class in pred_classes and img_class in true_classes else False\n",
    "\n",
    "    visualize_predictions(model, device, dataloader, n_to_show=n_to_show, predicate=predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_true_negatives(img_label, model, device, dataloader, n_to_show=10, threshold=0.75):\n",
    "\n",
    "    class_map = get_class_map()\n",
    "    img_class = class_map[img_label]\n",
    "\n",
    "    def predicate(pred_classes, true_classes):\n",
    "        return True if not img_class in pred_classes and not img_class in true_classes else False\n",
    "\n",
    "    visualize_predictions(model, device, dataloader, n_to_show=n_to_show, predicate=predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_false_positives(img_label, model, device, dataloader, n_to_show=10, threshold=0.75):\n",
    "\n",
    "    class_map = get_class_map()\n",
    "    img_class = class_map[img_label]\n",
    "\n",
    "    def predicate(pred_classes, true_classes):\n",
    "        return True if img_class in pred_classes and not img_class in true_classes else False\n",
    "\n",
    "    visualize_predictions(model, device, dataloader, n_to_show=n_to_show, predicate=predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_false_negatives(img_label, model, device, dataloader, n_to_show=10, threshold=0.75):\n",
    "\n",
    "    class_map = get_class_map()\n",
    "    img_class = class_map[img_label]\n",
    "\n",
    "    def predicate(pred_classes, true_classes):\n",
    "        return True if not img_class in pred_classes and img_class in true_classes else False\n",
    "\n",
    "    visualize_predictions(model, device, dataloader, n_to_show=n_to_show, predicate=predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, device, dataloader, mean=MEAN, std=STD, n_to_show=10, threshold=0.5, predicate=None):\n",
    "    \n",
    "    if predicate == None:\n",
    "        predicate = lambda p1, p2: True\n",
    "\n",
    "    class_to_label = { v: k for k, v in get_class_map().items() }\n",
    "    \n",
    "    # https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/5\n",
    "    inv_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean = -1 * np.multiply(mean, std), std=np.divide(1, std))\n",
    "    ])\n",
    "    \n",
    "    n_shown = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        X, y = batch\n",
    "        X = X.to(device)\n",
    "\n",
    "        y_pred_raw = torch.sigmoid(model(X).cpu())\n",
    "        y_pred = y_pred_raw > threshold \n",
    "        y = y == 1\n",
    "        \n",
    "        for i in range(len(y)):\n",
    "            pred_classes = np.where(y_pred[i] == 1)[0]\n",
    "            true_classes = np.where(y[i] == 1)[0]\n",
    "            \n",
    "            if not predicate(pred_classes, true_classes):\n",
    "                continue\n",
    "\n",
    "            show_image_with_predictions(X[i], true_classes, pred_classes)\n",
    "\n",
    "            n_shown += 1\n",
    "            \n",
    "            if n_shown >= n_to_show:\n",
    "                return            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_predictions(X, true_classes, pred_classes, mean=MEAN, std=STD):\n",
    "    \n",
    "    class_map = get_class_map()\n",
    "    class_to_label = { v: k for k, v in class_map.items() }\n",
    "\n",
    "    true_classes_str = ', '.join([class_to_label[i] for i in true_classes])\n",
    "    pred_classes_str = ', '.join([class_to_label[i] for i in pred_classes])\n",
    "\n",
    "    # https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/5\n",
    "    inv_transform = transforms.Normalize(mean = -1 * np.multiply(mean, std), std=np.divide(1, std))\n",
    "\n",
    "    img = inv_transform(X.cpu()) # inverse transforms\n",
    "    img = img.permute(2, 1, 0)   # BGR -> RGB\n",
    "    img = np.rot90(img, 3)\n",
    "\n",
    "    plt.title(f'True: {true_classes_str}, Predictions: {pred_classes_str}')\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_X(pred, threshold=1): # pred is in logit scale\n",
    "    return pred > threshold\n",
    "    #return softmax(pred) > threshold\n",
    "\n",
    "def predict(model, device, dataloader):\n",
    "\n",
    "    ys_all, y_hats_all = [], []  \n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        Xs, ys = batch\n",
    "        Xs = model(Xs.to(device))\n",
    "        y_hats = np.apply_along_axis(predict_X, axis=1, arr=Xs.cpu().detach().numpy())\n",
    "        \n",
    "        y_hats_all.extend(y_hat for y_hat in y_hats)\n",
    "        ys_all.extend(y.numpy() for y in ys==1)\n",
    "\n",
    "    return np.array(ys_all), np.array(y_hats_all)\n",
    "\n",
    "def get_prediction_metrics(y_true, y_pred):\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    (tn, fp, fn, tp) = tuple(cm.flatten())\n",
    "    \n",
    "    # Precision, recall, f1-score, support\n",
    "    prec, rec, support = tp/(tp+fp), tp/(tp+fn), tp+fn\n",
    "    f1 = 2*prec*rec/(prec+rec)\n",
    "    score = dict([(k, v[1]) for k, v in zip(['precision', 'recall', 'f1-score', 'support'], \n",
    "         precision_recall_fscore_support(y_true, y_pred, average=None))])\n",
    "    \n",
    "    return cm, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(y_true, y_pred, labels, file_path, no_label_cn=None, no_label_score=None):\n",
    "\n",
    "    plt.ioff()\n",
    "    \n",
    "    # Get confusion matrices\n",
    "    cn_tensor = skm.multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Get precision, recall, f1-score\n",
    "    scores = skm.classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    # Add non-label scores\n",
    "    if no_label_cn is not None and no_label_score is not None:\n",
    "        cn_tensor = np.concatenate((cn_tensor, no_label_cn[None]), axis=0)\n",
    "        scores['14'] = no_label_score\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=6, ncols=3, sharey=True, figsize=(20, 24), \n",
    "                           gridspec_kw={'hspace': 0.3, 'wspace': 0.0})\n",
    "    gn = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    n = cn_tensor[0].sum()\n",
    "\n",
    "    # Loop all labels\n",
    "    for i, cn_matrix in enumerate(cn_tensor):\n",
    "\n",
    "        j, k = int(i/3), i%3\n",
    "        \n",
    "        # Annotations\n",
    "        annot = np.asarray(\n",
    "            ['{}\\n{:0.0f}\\n{:.2%}'.format(gn[i], x, x/n) for i, x in enumerate(cn_matrix.flatten())]\n",
    "        ).reshape(2,2)\n",
    "        \n",
    "        # Precision, recall, f1-score\n",
    "        title = '{}\\nprec.={:.3}, rec.={:.3}, f1={:.3}'.format(\n",
    "            labels[i], scores[str(i)]['precision'], scores[str(i)]['recall'], scores[str(i)]['f1-score'])\n",
    "        ax[j, k].set_title(title)\n",
    "        ax[j, k].set_ylim([0,2])\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sn.heatmap(cn_matrix, annot=annot, fmt='', cmap='Blues', ax=ax[j, k])\n",
    "        \n",
    "    # Dirty hack: to fix matplotlib and sns incompatibility (positioning annotations)\n",
    "    sn.heatmap(np.array([[0,0],[0,0]]), annot=np.array([['',''],['','']]), fmt='', cmap='Blues', ax=ax[5, 0])\n",
    "        \n",
    "    plt.savefig(file_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Fix image size after hack\n",
    "    img = Image.open(file_path).crop((0, 0, 1200, 1150)).save(file_path, 'png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Using GPU!')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "lr = 0.01\n",
    "n_epochs = 1\n",
    "bs = 64\n",
    "n_classes = len(get_class_map().keys())\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save / load dataloaders from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomIndividualApply():\n",
    "    \"\"\"Apply randomly a list of transformations with a given probability\n",
    "\n",
    "    Args:\n",
    "        transforms (list or tuple): list of transformations\n",
    "        p (float): probability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms, p=0.5):\n",
    "        self.p = p\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            if self.p < np.random.random():\n",
    "                continue\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        format_string += '\\n    p={}'.format(self.p)\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved torch dump from disk.\n",
      " - Done.\n"
     ]
    }
   ],
   "source": [
    "max_images_per_class = int(1e9)\n",
    "#max_images_per_class = 200\n",
    "\n",
    "transformations = {\n",
    "    'train': transforms.Compose([\n",
    "        RandomIndividualApply([\n",
    "            transforms.RandomHorizontalFlip(p=1),\n",
    "            transforms.RandomRotation((-10, 10)),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.RandomPerspective(),\n",
    "        ], p=0.5),\n",
    "        transforms.ToTensor(),                \n",
    "        transforms.Normalize(mean=MEAN, std=STD)            \n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "if not os.path.isfile(f'../data/X_train_n{max_images_per_class}.dat'):\n",
    "    print('Loading all data from disk.')\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = get_data(max_images_per_class=max_images_per_class)\n",
    "    torch.save(X_train, f'../data/X_train_n{max_images_per_class}.dat')\n",
    "    torch.save(X_valid, f'../data/X_valid_n{max_images_per_class}.dat')\n",
    "    torch.save(X_test, f'../data/X_test_n{max_images_per_class}.dat')\n",
    "    torch.save(y_train, f'../data/y_train_n{max_images_per_class}.dat')\n",
    "    torch.save(y_valid, f'../data/y_valid_n{max_images_per_class}.dat')\n",
    "    torch.save(y_test, f'../data/y_test_n{max_images_per_class}.dat')\n",
    "    print(' - Done.')\n",
    "else:\n",
    "    print('Loading saved torch dump from disk.')\n",
    "    X_train = torch.load(f'../data/X_train_n{max_images_per_class}.dat')\n",
    "    X_valid = torch.load(f'../data/X_valid_n{max_images_per_class}.dat')\n",
    "    X_test = torch.load(f'../data/X_test_n{max_images_per_class}.dat')\n",
    "    y_train = torch.load(f'../data/y_train_n{max_images_per_class}.dat')\n",
    "    y_valid = torch.load(f'../data/y_valid_n{max_images_per_class}.dat')\n",
    "    y_test = torch.load(f'../data/y_test_n{max_images_per_class}.dat')\n",
    "    print(' - Done.')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    TransformingDataset(X_train, y_train, transforms=transformations['train']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    TransformingDataset(X_valid, y_valid, transforms=transformations['valid']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    TransformingDataset(X_test, y_test, transforms=transformations['test']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: The mean and std in transformations most probably need to be the same as for VGG and RESNET. Not 100% sure about this. Something to investigate!\n",
    "\n",
    "From: https://discuss.pytorch.org/t/about-normalization-using-pre-trained-vgg16-networks/23560 \n",
    "*Usually if your use case stays in the same data domain, the mean and std wonâ€™t be that different and you can try to use the ImageNet statistics.\n",
    "I would recommend to use your own data statistics if you are dealing with another domain, e.g. medical images.*\n",
    "\n",
    "\n",
    "More models here: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "\n",
    "If the models do not start to converge, try lowering the learning rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_VGG16_\n",
    "\n",
    "Currently getting validation f1 scores around 0.67. \n",
    "\n",
    "Now around 0.71 with one cycle policy.\n",
    "\n",
    "Surprisingly after quick testing the vgg16_bn (with BatchNorm layers) did not do as well? Maybe more to investigate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = vgg16(pretrained=True).to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(25088, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4096, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 14),\n",
    "    ).to(device)\n",
    "    \n",
    "    #https://discuss.pytorch.org/t/vgg-output-layer-no-softmax/9273/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_RESNET_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet18(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet34(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet50(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(2048, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet101(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(2048, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet152(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(2048, 14).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model or load an existing model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "pos_weight = torch.from_numpy(label_pos_weights_for_loss).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# learning rate and momentum will be overriden by the scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.01,\n",
    "    base_momentum=0.5,\n",
    "    max_momentum=0.95,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for model saving and loading\n",
    "\n",
    "# ResNet 101\n",
    "#model_save_path = '../data/resnet-valid-acc-aug-0.70.pth'\n",
    "model_save_path = '../data/resnet-101-valid-acc-0.73-2.pth'\n",
    "\n",
    "#model_save_path = '../data/resnet-152-valid-acc-0.722.pth'\n",
    "#model_save_path = '../data/resnet-152-valid-acc-0.727.pth'\n",
    "\n",
    "# VGG16\n",
    "#model_save_path = '../data/vgg16-9epochs-valid-acc-0.66.pth'\n",
    "#model_save_path = '../data/vgg16-valid-acc-0.71.pth'\n",
    "\n",
    "# Whole model\n",
    "#model_whole_save_path = '../data/vgg16-7epochs-valid-acc-0.703.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plain model saving and loading (only state dictionary).\n",
    "#torch.save(model.state_dict(), model_save_path)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an entire model (not just the state dict)\n",
    "#torch.save(model, model_whole_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an entire model (not just the state dict)\n",
    "#model = torch.load(model_whole_save_path)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for trying to train all the layers...\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model from disk to continue training\n",
    "#model = torch.load('../data/vgg16-25epochs-64bs-lr0.01-valid-F1-0.683.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tr_losses, val_losses, tr_scores, val_scores = \\\n",
    "    train(train_dataloader, valid_dataloader, model, optimizer, scheduler, criterion, device, \\\n",
    "          n_epochs=n_epochs, \\\n",
    "          verbose=False, \\\n",
    "          threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model on disk to continue training later\n",
    "if True:\n",
    "    f1 = val_scores[-1]\n",
    "    model_whole_save_path = f'../data/vgg16-epochs{n_epochs}-bs{bs}-lr{lr}-valid-F1-{f1:.3}.pth'\n",
    "    #model_whole_save_path = f'../data/vgg16-epochs25|{n_epochs}-bs64|{bs}-lr0.01|{lr}-valid-F1-{f1:.3}.pth'\n",
    "    print(f'Saving model to {model_whole_save_path}')\n",
    "    torch.save(model, model_whole_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.7791, dtype=torch.float64), tensor(0.7323, dtype=torch.float64))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(valid_dataloader, model, criterion, device, threshold=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.05, f1 score: 0.6257869649822454\n",
      "threshold: 0.1, f1 score: 0.6672068007219064\n",
      "threshold: 0.15000000000000002, f1 score: 0.6858991922815018\n",
      "threshold: 0.2, f1 score: 0.6996209429401092\n",
      "threshold: 0.25, f1 score: 0.7105481915126662\n",
      "threshold: 0.3, f1 score: 0.7171493850486839\n",
      "threshold: 0.35000000000000003, f1 score: 0.7209325560542775\n",
      "threshold: 0.4, f1 score: 0.7228994513973324\n",
      "threshold: 0.45, f1 score: 0.7262487763054637\n",
      "threshold: 0.5, f1 score: 0.7274743568152366\n",
      "threshold: 0.55, f1 score: 0.7314873614845571\n",
      "threshold: 0.6000000000000001, f1 score: 0.7291734814627706\n",
      "threshold: 0.6500000000000001, f1 score: 0.7294388342483105\n",
      "threshold: 0.7000000000000001, f1 score: 0.7331308158269648\n",
      "threshold: 0.7500000000000001, f1 score: 0.7307053757889209\n",
      "threshold: 0.8, f1 score: 0.7234334484634519\n",
      "threshold: 0.8500000000000001, f1 score: 0.7181855900002084\n",
      "threshold: 0.9000000000000001, f1 score: 0.7170973464014673\n",
      "threshold: 0.9500000000000001, f1 score: 0.7053768205553871\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is for finding the optimal threshold.\n",
    "if True:\n",
    "    f1_scores = []\n",
    "    for threshold in np.arange(0.05, 1, 0.05):\n",
    "        _, f1 = evaluate(valid_dataloader, model, criterion, device, threshold=threshold)\n",
    "        f1_scores.append(f1)\n",
    "        print(f'threshold: {threshold}, f1 score: {f1}')\n",
    "\n",
    "    plt.plot(np.arange(0.05, 1, 0.05), f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some images with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    visualize_predictions(model, device, test_dataloader, n_to_show=5, threshold=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TP/FP/TN/FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tpfptnfn = False\n",
    "img_label = 'baby'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_tpfptnfn:\n",
    "    visualize_true_positives(img_label, model, device, valid_dataloader, n_to_show=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_tpfptnfn:\n",
    "    visualize_false_positives(img_label, model, device, valid_dataloader, n_to_show=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_tpfptnfn:\n",
    "    visualize_true_negatives(img_label, model, device, valid_dataloader, n_to_show=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_tpfptnfn:\n",
    "    visualize_false_negatives(img_label, model, device, valid_dataloader, n_to_show=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model against test set\n",
    "\n",
    "## Load our best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../data/vgg16-epochs20-bs64-lr0.01-valid-F1-0.691.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH2 = '../test_data'\n",
    "DATA_PATH2 = '../data'\n",
    "IMAGE_PATH2 = f'{DATA_PATH2}/images'\n",
    "LABEL_PATH2 = f'{DATA_PATH2}/annotations'\n",
    "\n",
    "# Load testset\n",
    "if not os.path.isfile(f'{DATA_PATH2}/X_test_n{max_images_per_class}.dat'):\n",
    "    print('Loading test data from disk.')\n",
    "    X_test2, y_test2, _, _, _, _ = get_data(max_images_per_class=max_images_per_class, train_fr=1, IMAGE_PATH=IMAGE_PATH2, LABEL_PATH=LABEL_PATH2)\n",
    "    torch.save(X_test2, f'{DATA_PATH2}/X_test_n{max_images_per_class}.dat')\n",
    "    torch.save(y_test2, f'{DATA_PATH2}/y_test_n{max_images_per_class}.dat')\n",
    "else:\n",
    "    print('Loading saved torch dump from disk.')\n",
    "    X_test2 = torch.load(f'{DATA_PATH2}/X_test_n{max_images_per_class}.dat')\n",
    "    y_test2 = torch.load(f'{DATA_PATH2}/y_test_n{max_images_per_class}.dat')\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    TransformingDataset(X_test2, y_test2, transforms=transformations['test']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict \n",
    "y_true, y_pred = predict(model, device, test_dataloader)\n",
    "\n",
    "# Save results to disk\n",
    "path_pred, path_true = f'{DATA_PATH2}/results_testset2_pred.txt', f'{DATA_PATH2}/results_testset2_true.txt'\n",
    "np.savetxt(path_pred, y_pred, fmt='%d')\n",
    "np.savetxt(path_true, y_true, fmt='%d')\n",
    "print(f'Saved results in {path_pred}, {path_true}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test_eval.py '../data/results_testset2_true.txt' '../data/results_testset2_pred.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save classification report\n",
    "with open(f'../data/results_testset2_classification_report.txt', 'w') as file:\n",
    "    file.write(skm.classification_report(y_true, y_pred))\n",
    "    \n",
    "# Show classification report\n",
    "with open(f'../data/results_testset2_classification_report.txt', 'r') as file:\n",
    "    report = ''.join(file.readlines())\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix plot for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save confusion matrix plot\n",
    "labels = [k for k, v in get_class_map().items()] + ['unlabelled']\n",
    "\n",
    "#tn, fp, fn, tp = 3750, 157, 21, 72\n",
    "#y_true_no = [0]*tn + [0]*fp + [1]*fn + [1]*tp\n",
    "#y_pred_no = [0]*tn + [1]*fp + [0]*fn + [1]*tp\n",
    "\n",
    "# images and predictions that have no labels\n",
    "y_true_no = (np.sum(y_true, axis=1) == 0).astype(int)\n",
    "y_pred_no = (np.sum(y_pred, axis=1) == 0).astype(int)\n",
    "\n",
    "# confusion matrix and scores for non-labelled images\n",
    "cm, score = get_prediction_metrics(y_true_no, y_pred_no)\n",
    "\n",
    "visualize_confusion_matrix(y_true, y_pred, labels, f'../data/results_testset2_confusion_matrix.png', \n",
    "                           no_label_cn=cm, no_label_score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix plot\n",
    "IPython_Image(filename=f'../data/results_testset2_confusion_matrix.png', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
