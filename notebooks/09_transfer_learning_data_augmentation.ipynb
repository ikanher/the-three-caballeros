{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import vgg16, vgg16_bn, resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "from IPython.display import display, clear_output, Image as IPython_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../data/images'\n",
    "LABEL_PATH = '../data/annotations'\n",
    "\n",
    "# GIVEN DATASET\n",
    "MEAN = (0.43672, 0.40107, 0.36762)\n",
    "STD = (0.30139, 0.28781, 0.29236)\n",
    "\n",
    "# IMAGENET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# RESNET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "       \n",
    "# Define default pos_weights for nn.BCEWithLogitsLoss(pos_weights).\n",
    "label_pos_weights_for_loss = np.array([209.52631579, 55.87203791, 58.40594059, 16.77777778, 44.80152672, 5.25, 25.14379085, 5.75675676, 33.09090909, 2.15540363, 5.51465798, 163.38356164, 119., 37.46153846], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_classes():\n",
    "    return len(listdir(LABEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_map():\n",
    "    ret = {}\n",
    "\n",
    "    i = 0\n",
    "    for fname in listdir(LABEL_PATH):\n",
    "        img_class, _ = fname.split('.')\n",
    "        ret[img_class] = i\n",
    "        i += 1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels_to_csv(name_of_set, label_array):\n",
    "    filepath = f'../data/labels_{name_of_set}.csv'\n",
    "    \n",
    "    label_arr = np.array(label_array).astype(int)\n",
    "\n",
    "    # Save 2D numpy array to csv file\n",
    "    np.savetxt(filepath, label_arr, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_fr=.6, max_images_per_class=1e9):\n",
    "    # mapping from class names to integers\n",
    "    class_map = get_class_map()\n",
    "\n",
    "    # create a dictionary to hold our label vectors\n",
    "    n_classes = len(class_map.keys())\n",
    "    img_to_class = defaultdict(lambda: np.zeros(n_classes))\n",
    "\n",
    "    # another dictionary to hold the actual image data\n",
    "    img_to_data = dict()\n",
    "    \n",
    "    # loop through all the annotations\n",
    "    for fname in listdir(LABEL_PATH):\n",
    "        img_class, _ = fname.split('.')\n",
    "        print(f'Reading class: {img_class}')\n",
    "        \n",
    "        # open the annotation file\n",
    "        i = 0\n",
    "        with open(f'{LABEL_PATH}/{fname}', 'r') as fh:\n",
    "\n",
    "            # get image ids from annotation file\n",
    "            img_ids = fh.read().splitlines()\n",
    "            \n",
    "            # gather the images with labels\n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                \n",
    "                # let's not process images unnecessarily\n",
    "                if not img_id in img_to_data:\n",
    "\n",
    "                    img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "                    img = Image.open(img_path)\n",
    "\n",
    "                    # append to dict\n",
    "                    img_to_data[img_id] = img.convert('RGB')\n",
    "\n",
    "                # get one-hot encoded vector of image classes\n",
    "                img_classes = img_to_class[img_id]\n",
    "\n",
    "                # add new class to image vector\n",
    "                img_class_id = class_map[img_class]\n",
    "                img_classes[img_class_id] = 1\n",
    "\n",
    "                # store the updated vector back\n",
    "                img_to_class[img_id] = img_classes\n",
    "\n",
    "                if i >= max_images_per_class:\n",
    "                    break\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    # load also all the images that do not have any labels\n",
    "    i = 0\n",
    "    print(f'Reading images without labels..')\n",
    "    for fname in listdir(IMAGE_PATH):\n",
    "        m = re.match('im(\\d+)', fname)\n",
    "        img_id = m.group(1)\n",
    "\n",
    "        if img_id not in img_to_data:\n",
    "            img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # append to dict\n",
    "            img_to_data[img_id] = img.convert('RGB')\n",
    "\n",
    "            if i >= max_images_per_class:\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    print('Creating train/valid/test split..')\n",
    "    # collect data to a single array\n",
    "    X = []\n",
    "    y = []\n",
    "    for img_id in img_to_data.keys():\n",
    "        X.append(img_to_data[img_id])\n",
    "        y.append(img_to_class[img_id])\n",
    "\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, train_size=train_fr, random_state=42)\n",
    "    X_test, X_valid, y_test, y_valid = train_test_split(X_tmp, y_tmp, train_size=.5, test_size=.5, random_state=42)\n",
    "    \n",
    "    print('Done.')\n",
    "\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, transforms=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_data = self.X[idx]\n",
    "        img_class = self.y[idx]\n",
    "\n",
    "        if transforms:\n",
    "            img_data = self.transforms(img_data)\n",
    "\n",
    "        return img_data, img_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_classes):\n",
    "        super().__init__()\n",
    "        self.bs = bs\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden1 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden2 = nn.Linear(n_hidden2, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.hidden2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class OneLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'X.SHAPE: {x.shape}')\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class ConvNetModel(nn.Module):\n",
    "    def __init__(self, n_classes, keep_prob=.5):\n",
    "        super(ConvNetModel, self).__init__()\n",
    "        # Common layers used multiple times\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=1-keep_prob)\n",
    "        \n",
    "        # Unique layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=3, stride=1, padding=1) #(n samples, channels, height, width)\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=14, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc3 = nn.Linear(in_features=256*4*14, out_features=n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3, 128, 128)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)  # Flatten for FC\n",
    "        out = self.fc3(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, device, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    f1_scores = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                score = f1_score(y.cpu() == 1, y_pred.cpu() > threshold, average='micro')\n",
    "                f1_scores.append(score)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, model, optimizer, scheduler, criterion, device, n_epochs=50, verbose=True):\n",
    "    model.train()\n",
    "\n",
    "    if verbose:\n",
    "        fmt = '{:<5} {:12} {:12} {:<9} {:<9}'\n",
    "        print(fmt.format('Epoch', 'Train loss', 'Valid loss', 'Train F1', 'Valid F1'))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            X, y = batch\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            print(f'Epoch: {epoch+1}, iteration: {i+1}, loss: {loss}')\n",
    "\n",
    "        if verbose:\n",
    "            train_loss, train_score = evaluate(train_dataloader, model, criterion, device)\n",
    "            valid_loss, valid_score = evaluate(valid_dataloader, model, criterion, device)\n",
    "\n",
    "            fmt = '{:<5} {:03.10f} {:03.10f} {:02.7f} {:02.7f}'\n",
    "            print(fmt.format(epoch, train_loss, valid_loss, train_score, valid_score))\n",
    "            \n",
    "    print('Done training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, device, dataloader, mean=MEAN, std=STD, n_to_show=3, threshold=0.5):\n",
    "    \n",
    "    class_to_label = { v: k for k, v in get_class_map().items() }\n",
    "    \n",
    "    # https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/5\n",
    "    inv_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean = -1 * np.multiply(mean, std), std=np.divide(1, std))\n",
    "    ])\n",
    "    \n",
    "    n_shown = 0\n",
    "    for i, batch in enumerate(dataloader):        \n",
    "        X, y = batch\n",
    "        X = X.to(device)\n",
    "\n",
    "        y_pred_raw = model(X).cpu()\n",
    "        y_pred = y_pred_raw > threshold \n",
    "        y = y == 1\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            pred_classes = np.where(y_pred[i] == 1)[0]\n",
    "            true_classes = np.where(y[i] == 1)[0]\n",
    "            \n",
    "            true_classes_str = ', '.join([class_to_label[i] for i in true_classes])\n",
    "            pred_classes_str = ', '.join([class_to_label[i] for i in pred_classes])\n",
    "\n",
    "            img = inv_transform(X[i].cpu())              # inverse transforms\n",
    "            img = img.permute(2, 1, 0)                   # BGR -> RGB\n",
    "            img = np.rot90(img, 3)\n",
    "                                    \n",
    "            plt.title(f'True: {true_classes_str}, Predictions: {pred_classes_str}')\n",
    "            plt.imshow(img)\n",
    "            plt.pause(0.001)\n",
    "\n",
    "            n_shown += 1\n",
    "            \n",
    "            if n_shown >= n_to_show:\n",
    "                return            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to use same naming conventions everywhere. Might be a good idea to rename `y_hat` to `y_pred` or vice versa everywhere (it makes it easier to combine the notebooks). I don't care which way it is. Also the `Xs` and `ys` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def predict_X(fr, threshold=0.5):\n",
    "    \n",
    "    y_hat = fr > threshold\n",
    "    \n",
    "    return y_hat if (np.sum(y_hat) > 0) else fr == np.max(fr)\n",
    "\n",
    "def predict(model, device, dataloader):\n",
    "    \n",
    "    ys_all = []  # Array of np.array(14) \n",
    "    y_hats_all = []\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        Xs, ys = batch\n",
    "        Xs = model(Xs.to(device))\n",
    "        y_hats = np.apply_along_axis(predict_X, axis=1, arr=Xs.cpu().detach().numpy())\n",
    "        \n",
    "        y_hats_all.extend(y_hat for y_hat in y_hats)\n",
    "        ys_all.extend(y.numpy() for y in ys==1)\n",
    "\n",
    "    return np.array(ys_all), np.array(y_hats_all)\n",
    "\n",
    "def visualize_confusion_matrix(y_true, y_pred, labels, file_path):\n",
    "\n",
    "    plt.ioff()\n",
    "    \n",
    "    # Get confusion matrices\n",
    "    cn_tensor = skm.multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Get precision, recall, f1-score\n",
    "    scores = skm.classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=5, ncols=3,sharey=True, figsize=(20, 20), \n",
    "                           gridspec_kw={'hspace': 0.3, 'wspace': 0.0})\n",
    "    gn = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    n = cn_tensor[0].sum()\n",
    "    \n",
    "    # Loop all labels\n",
    "    for i, cn_matrix in enumerate(cn_tensor):\n",
    "\n",
    "        j, k = int(i/3), i%3\n",
    "        \n",
    "        # Annotations\n",
    "        annot = np.asarray(\n",
    "            ['{}\\n{:0.0f}\\n{:.2%}'.format(gn[i], x, x/n) for i, x in enumerate(cn_matrix.flatten())]\n",
    "        ).reshape(2,2)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sn.heatmap(cn_matrix, annot=annot, fmt='', cmap='Blues', ax=ax[j, k])\n",
    "        \n",
    "        # Precision, recall, f1-score\n",
    "        title = '{}\\nprec.={:.3}, rec.={:.3}, f1={:.3}'.format(\n",
    "            labels[i], scores[str(i)]['precision'], scores[str(i)]['recall'], scores[str(i)]['f1-score'])\n",
    "        ax[j, k].set_title(title)\n",
    "        \n",
    "    plt.savefig(file_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Using GPU!')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "lr = 0.01\n",
    "n_epochs = 20\n",
    "bs = 64\n",
    "n_classes = len(get_class_map().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save / load dataloaders from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "max_images_per_class = int(1e9)\n",
    "#max_images_per_class = 200\n",
    "\n",
    "transformations = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomHorizontalFlip(p=1),\n",
    "            transforms.RandomRotation((-10, 10)),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
    "            transforms.RandomGrayscale(p=1),\n",
    "            transforms.RandomPerspective(),\n",
    "        ], p=0.5),\n",
    "        transforms.ToTensor(),                \n",
    "        transforms.Normalize(mean=MEAN, std=STD)            \n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "if os.path.isfile(f'X_train_n{max_images_per_class}.dat'):\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = get_data(max_images_per_class=max_images_per_class)\n",
    "    torch.save(X_train, f'../data/X_train_n{max_images_per_class}.dat')\n",
    "    torch.save(X_valid, f'../data/X_valid_n{max_images_per_class}.dat')\n",
    "    torch.save(X_test, f'../data/X_test_n{max_images_per_class}.dat')\n",
    "    torch.save(y_train, f'../data/y_train_n{max_images_per_class}.dat')\n",
    "    torch.save(y_valid, f'../data/y_valid_n{max_images_per_class}.dat')\n",
    "    torch.save(y_test, f'../data/y_test_n{max_images_per_class}.dat')\n",
    "else:\n",
    "    X_train = torch.load(f'../data/X_train_n{max_images_per_class}.dat')\n",
    "    X_valid = torch.load(f'../data/X_valid_n{max_images_per_class}.dat')\n",
    "    X_test = torch.load(f'../data/X_test_n{max_images_per_class}.dat')\n",
    "    y_train = torch.load(f'../data/y_train_n{max_images_per_class}.dat')\n",
    "    y_valid = torch.load(f'../data/y_valid_n{max_images_per_class}.dat')\n",
    "    y_test = torch.load(f'../data/y_test_n{max_images_per_class}.dat')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    TransformingDataset(X_train, y_train, transforms=transformations['train']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    TransformingDataset(X_valid, y_valid, transforms=transformations['valid']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    TransformingDataset(X_test, y_test, transforms=transformations['test']),\n",
    "    shuffle=True,\n",
    "    batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: The mean and std in transformations most probably need to be the same as for VGG and RESNET. Not 100% sure about this. Something to investigate!\n",
    "\n",
    "More models here: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "\n",
    "If the models do not start to converge, try lowering the learning rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_VGG16_\n",
    "\n",
    "Currently getting validation f1 scores around 0.67. \n",
    "\n",
    "Now around 0.71 with one cycle policy.\n",
    "\n",
    "Surprisingly after quick testing the vgg16_bn (with BatchNorm layers) did not do as well? Maybe more to investigate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = vgg16(pretrained=True).to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(25088, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4096, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 14),\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_RESNET_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet18(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet34(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet50(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(2048, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet101(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(2048, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet152(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(2048, 14).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model or load an existing model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "pos_weight = torch.from_numpy(label_pos_weights_for_loss).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# learning rate and momentum will be overriden by the scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.01,\n",
    "    base_momentum=0.5,\n",
    "    max_momentum=0.99,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for model saving and loading\n",
    "\n",
    "# Only state dictionary\n",
    "#model_save_path = '../data/resnet-valid-acc-aug-0.70.pth'\n",
    "#model_save_path = '../data/resnet-101-valid-acc-0.73.pth'\n",
    "#model_save_path = '../data/resnet-valid-acc-aug-0.716.pth'\n",
    "#model_save_path = '../data/vgg16-9epochs-valid-acc-0.66.pth'\n",
    "\n",
    "# Whole model\n",
    "#model_whole_save_path = '../data/vgg16-7epochs-valid-acc-0.703.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plain model saving and loading (only state dictionary).\n",
    "#torch.save(model.state_dict(), model_save_path)\n",
    "#model.load_state_dict(torch.load(model_save_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an entire model (not just the state dict)\n",
    "#torch.save(model, model_whole_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an entire model (not just the state dict)\n",
    "#model = torch.load(model_whole_save_path)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for trying to train all the layers...\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train loss   Valid loss   Train F1  Valid F1 \n",
      "Epoch: 1, iteration: 1, loss: 4.914584707823632\n",
      "Epoch: 1, iteration: 2, loss: 3.9467705518424125\n",
      "Epoch: 1, iteration: 3, loss: 3.135240022370196\n",
      "Epoch: 1, iteration: 4, loss: 2.4401800074936935\n",
      "Epoch: 1, iteration: 5, loss: 1.8699341844621462\n",
      "Epoch: 1, iteration: 6, loss: 1.6754820654931406\n",
      "Epoch: 1, iteration: 7, loss: 1.575284567548322\n",
      "Epoch: 1, iteration: 8, loss: 1.8948483989164728\n",
      "Epoch: 1, iteration: 9, loss: 2.11453765530687\n",
      "Epoch: 1, iteration: 10, loss: 1.6269044793327798\n",
      "Epoch: 1, iteration: 11, loss: 1.639783476224051\n",
      "Epoch: 1, iteration: 12, loss: 1.6024120041219778\n",
      "Epoch: 1, iteration: 13, loss: 1.6891106679575971\n",
      "Epoch: 1, iteration: 14, loss: 1.9678760716375612\n",
      "Epoch: 1, iteration: 15, loss: 1.7651557487428178\n",
      "Epoch: 1, iteration: 16, loss: 1.6569346953704487\n",
      "Epoch: 1, iteration: 17, loss: 2.0637901058167225\n",
      "Epoch: 1, iteration: 18, loss: 1.9704490387488762\n",
      "Epoch: 1, iteration: 19, loss: 1.883867752456107\n",
      "Epoch: 1, iteration: 20, loss: 2.2969632063059\n",
      "Epoch: 1, iteration: 21, loss: 1.8736526794457997\n",
      "Epoch: 1, iteration: 22, loss: 2.143104229287697\n",
      "Epoch: 1, iteration: 23, loss: 2.242526955232922\n",
      "Epoch: 1, iteration: 24, loss: 2.445416770500596\n",
      "Epoch: 1, iteration: 25, loss: 2.3812010244096764\n",
      "Epoch: 1, iteration: 26, loss: 2.4070608587824536\n",
      "Epoch: 1, iteration: 27, loss: 2.4516988952295757\n",
      "Epoch: 1, iteration: 28, loss: 2.5638164523114364\n",
      "Epoch: 1, iteration: 29, loss: 2.484034839137102\n",
      "Epoch: 1, iteration: 30, loss: 2.4394438212462717\n",
      "Epoch: 1, iteration: 31, loss: 2.5296468621003525\n",
      "Epoch: 1, iteration: 32, loss: 2.4519344663138947\n",
      "Epoch: 1, iteration: 33, loss: 2.5931951208637014\n",
      "Epoch: 1, iteration: 34, loss: 2.3427995069877454\n",
      "Epoch: 1, iteration: 35, loss: 2.4732323831590377\n",
      "Epoch: 1, iteration: 36, loss: 2.5623898490251094\n",
      "Epoch: 1, iteration: 37, loss: 2.296831206534241\n",
      "Epoch: 1, iteration: 38, loss: 2.7786165258548747\n",
      "Epoch: 1, iteration: 39, loss: 2.3090041790612554\n",
      "Epoch: 1, iteration: 40, loss: 2.376796395919005\n",
      "Epoch: 1, iteration: 41, loss: 2.799312248975761\n",
      "Epoch: 1, iteration: 42, loss: 2.1666266319374\n",
      "Epoch: 1, iteration: 43, loss: 2.594304073016412\n",
      "Epoch: 1, iteration: 44, loss: 2.1932338123174886\n",
      "Epoch: 1, iteration: 45, loss: 2.8171846945706234\n",
      "Epoch: 1, iteration: 46, loss: 2.7780802243688534\n",
      "Epoch: 1, iteration: 47, loss: 2.585763316382988\n",
      "Epoch: 1, iteration: 48, loss: 2.07203860124663\n",
      "Epoch: 1, iteration: 49, loss: 2.3245646124807826\n",
      "Epoch: 1, iteration: 50, loss: 2.3558496109760005\n",
      "Epoch: 1, iteration: 51, loss: 2.4344448535170367\n",
      "Epoch: 1, iteration: 52, loss: 2.221257526296573\n",
      "Epoch: 1, iteration: 53, loss: 2.237848714956389\n",
      "Epoch: 1, iteration: 54, loss: 2.308032730620387\n",
      "Epoch: 1, iteration: 55, loss: 2.416198444555924\n",
      "Epoch: 1, iteration: 56, loss: 1.964207330210759\n",
      "Epoch: 1, iteration: 57, loss: 2.2080618691438856\n",
      "Epoch: 1, iteration: 58, loss: 1.9870255697406163\n",
      "Epoch: 1, iteration: 59, loss: 2.1355892172379987\n",
      "Epoch: 1, iteration: 60, loss: 2.4467746448734977\n",
      "Epoch: 1, iteration: 61, loss: 1.8891431279006217\n",
      "Epoch: 1, iteration: 62, loss: 1.8736348389523656\n",
      "Epoch: 1, iteration: 63, loss: 2.0898882347035537\n",
      "Epoch: 1, iteration: 64, loss: 1.9978140017807615\n",
      "Epoch: 1, iteration: 65, loss: 1.7039795161082356\n",
      "Epoch: 1, iteration: 66, loss: 1.85496786260835\n",
      "Epoch: 1, iteration: 67, loss: 1.7012673981338722\n",
      "Epoch: 1, iteration: 68, loss: 1.810614652954092\n",
      "Epoch: 1, iteration: 69, loss: 1.8191366105031492\n",
      "Epoch: 1, iteration: 70, loss: 1.738003071512579\n",
      "Epoch: 1, iteration: 71, loss: 1.8271377774877675\n",
      "Epoch: 1, iteration: 72, loss: 1.8994104117843769\n",
      "Epoch: 1, iteration: 73, loss: 1.5592743160962415\n",
      "Epoch: 1, iteration: 74, loss: 1.824130567389466\n",
      "Epoch: 1, iteration: 75, loss: 1.3929267269652983\n",
      "Epoch: 1, iteration: 76, loss: 1.7567288638124257\n",
      "Epoch: 1, iteration: 77, loss: 1.480954090012365\n",
      "Epoch: 1, iteration: 78, loss: 1.3574965539360277\n",
      "Epoch: 1, iteration: 79, loss: 1.3310418402679773\n",
      "Epoch: 1, iteration: 80, loss: 1.377620150587056\n",
      "Epoch: 1, iteration: 81, loss: 1.5401655440304756\n",
      "Epoch: 1, iteration: 82, loss: 1.7650909011762368\n",
      "Epoch: 1, iteration: 83, loss: 1.4068892775785324\n",
      "Epoch: 1, iteration: 84, loss: 1.4602589881472015\n",
      "Epoch: 1, iteration: 85, loss: 1.2082394543480504\n",
      "Epoch: 1, iteration: 86, loss: 1.7402805495723197\n",
      "Epoch: 1, iteration: 87, loss: 1.567321718746457\n",
      "Epoch: 1, iteration: 88, loss: 1.3597117776033845\n",
      "Epoch: 1, iteration: 89, loss: 1.3335370360863572\n",
      "Epoch: 1, iteration: 90, loss: 1.7076769400214105\n",
      "Epoch: 1, iteration: 91, loss: 1.3287184812105641\n",
      "Epoch: 1, iteration: 92, loss: 1.520785216945333\n",
      "Epoch: 1, iteration: 93, loss: 1.5788789457756531\n",
      "Epoch: 1, iteration: 94, loss: 1.471413357492239\n",
      "Epoch: 1, iteration: 95, loss: 1.5608795894046472\n",
      "Epoch: 1, iteration: 96, loss: 1.4023915175078436\n",
      "Epoch: 1, iteration: 97, loss: 1.6814818897103352\n",
      "Epoch: 1, iteration: 98, loss: 1.503675870150944\n",
      "Epoch: 1, iteration: 99, loss: 1.723903673829487\n",
      "Epoch: 1, iteration: 100, loss: 1.423905637630047\n",
      "Epoch: 1, iteration: 101, loss: 1.2041166666983287\n",
      "Epoch: 1, iteration: 102, loss: 1.2699673121502304\n",
      "Epoch: 1, iteration: 103, loss: 1.444272155690238\n",
      "Epoch: 1, iteration: 104, loss: 1.2669739368784072\n",
      "Epoch: 1, iteration: 105, loss: 1.2623579168358878\n",
      "Epoch: 1, iteration: 106, loss: 1.259713765340165\n",
      "Epoch: 1, iteration: 107, loss: 1.1941639558212378\n",
      "Epoch: 1, iteration: 108, loss: 1.3511822747059223\n",
      "Epoch: 1, iteration: 109, loss: 1.4812798125758786\n",
      "Epoch: 1, iteration: 110, loss: 1.1358216543934925\n",
      "Epoch: 1, iteration: 111, loss: 1.3542032709323961\n",
      "Epoch: 1, iteration: 112, loss: 2.2098054527384763\n",
      "Epoch: 1, iteration: 113, loss: 1.5337012824920113\n",
      "Epoch: 1, iteration: 114, loss: 1.262444200321344\n",
      "Epoch: 1, iteration: 115, loss: 1.174179249411825\n",
      "Epoch: 1, iteration: 116, loss: 1.4241686297014073\n",
      "Epoch: 1, iteration: 117, loss: 1.3371518835813827\n",
      "Epoch: 1, iteration: 118, loss: 1.23394969140839\n",
      "Epoch: 1, iteration: 119, loss: 1.3532841781218499\n",
      "Epoch: 1, iteration: 120, loss: 1.2512987876257633\n",
      "Epoch: 1, iteration: 121, loss: 1.1573940341966298\n",
      "Epoch: 1, iteration: 122, loss: 1.113597062242873\n",
      "Epoch: 1, iteration: 123, loss: 1.297351748811183\n",
      "Epoch: 1, iteration: 124, loss: 1.218468471502552\n",
      "Epoch: 1, iteration: 125, loss: 1.1327957279422083\n",
      "Epoch: 1, iteration: 126, loss: 0.9960929086282303\n",
      "Epoch: 1, iteration: 127, loss: 1.3070620801748005\n",
      "Epoch: 1, iteration: 128, loss: 1.1796067577158253\n",
      "Epoch: 1, iteration: 129, loss: 1.8503754621013082\n",
      "Epoch: 1, iteration: 130, loss: 1.2504961017620333\n",
      "Epoch: 1, iteration: 131, loss: 1.1767189308670845\n",
      "Epoch: 1, iteration: 132, loss: 1.0046595067652815\n",
      "Epoch: 1, iteration: 133, loss: 1.1739803342790356\n",
      "Epoch: 1, iteration: 134, loss: 1.1200548597399786\n",
      "Epoch: 1, iteration: 135, loss: 1.0405262589308033\n",
      "Epoch: 1, iteration: 136, loss: 1.1397811688019608\n",
      "Epoch: 1, iteration: 137, loss: 1.0891099222716432\n",
      "Epoch: 1, iteration: 138, loss: 1.1620261271598946\n",
      "Epoch: 1, iteration: 139, loss: 1.142569441137602\n",
      "Epoch: 1, iteration: 140, loss: 1.0324001464700734\n",
      "Epoch: 1, iteration: 141, loss: 1.0603655862069228\n",
      "Epoch: 1, iteration: 142, loss: 1.0110747229590824\n",
      "Epoch: 1, iteration: 143, loss: 1.6363015410098563\n",
      "Epoch: 1, iteration: 144, loss: 1.1786755840204572\n",
      "Epoch: 1, iteration: 145, loss: 1.0814053826354235\n",
      "Epoch: 1, iteration: 146, loss: 0.9792100598185296\n",
      "Epoch: 1, iteration: 147, loss: 0.9805913856753065\n",
      "Epoch: 1, iteration: 148, loss: 1.1596933674277115\n",
      "Epoch: 1, iteration: 149, loss: 0.9925735969721285\n",
      "Epoch: 1, iteration: 150, loss: 1.3453476813173\n",
      "Epoch: 1, iteration: 151, loss: 1.2678751006825701\n",
      "Epoch: 1, iteration: 152, loss: 1.357114648388233\n",
      "Epoch: 1, iteration: 153, loss: 1.0164022784870625\n",
      "Epoch: 1, iteration: 154, loss: 0.9500076752226734\n",
      "Epoch: 1, iteration: 155, loss: 1.2034788063087594\n",
      "Epoch: 1, iteration: 156, loss: 1.3423029018747836\n",
      "Epoch: 1, iteration: 157, loss: 1.0363895726085648\n",
      "Epoch: 1, iteration: 158, loss: 0.9156866355847366\n",
      "Epoch: 1, iteration: 159, loss: 1.5011594136411364\n",
      "Epoch: 1, iteration: 160, loss: 1.0940296107910439\n",
      "Epoch: 1, iteration: 161, loss: 0.7960100393315507\n",
      "Epoch: 1, iteration: 162, loss: 1.0327329217240981\n",
      "Epoch: 1, iteration: 163, loss: 1.296272010205578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, iteration: 164, loss: 0.949254024001734\n",
      "Epoch: 1, iteration: 165, loss: 1.0683517465718095\n",
      "Epoch: 1, iteration: 166, loss: 1.0458701918302293\n",
      "Epoch: 1, iteration: 167, loss: 0.9607339008968547\n",
      "Epoch: 1, iteration: 168, loss: 1.4059721038962738\n",
      "Epoch: 1, iteration: 169, loss: 1.0788369078929412\n",
      "Epoch: 1, iteration: 170, loss: 1.0269718398876726\n",
      "Epoch: 1, iteration: 171, loss: 1.0207748888242323\n",
      "Epoch: 1, iteration: 172, loss: 1.0332984428003336\n",
      "Epoch: 1, iteration: 173, loss: 1.0209423710650587\n",
      "Epoch: 1, iteration: 174, loss: 1.1497414434849171\n",
      "Epoch: 1, iteration: 175, loss: 0.9103804759156666\n",
      "Epoch: 1, iteration: 176, loss: 0.8959105372573908\n",
      "Epoch: 1, iteration: 177, loss: 1.0858162530922757\n",
      "Epoch: 1, iteration: 178, loss: 0.9776646376154943\n",
      "Epoch: 1, iteration: 179, loss: 1.0203664167423492\n",
      "Epoch: 1, iteration: 180, loss: 0.8764746367031718\n",
      "Epoch: 1, iteration: 181, loss: 1.0412620106737698\n",
      "Epoch: 1, iteration: 182, loss: 1.1785063123382233\n",
      "Epoch: 1, iteration: 183, loss: 1.1200644966679496\n",
      "Epoch: 1, iteration: 184, loss: 1.1022356463921348\n",
      "Epoch: 1, iteration: 185, loss: 0.9595381326391053\n",
      "Epoch: 1, iteration: 186, loss: 1.0745551554404862\n",
      "Epoch: 1, iteration: 187, loss: 1.0229588760343622\n",
      "Epoch: 1, iteration: 188, loss: 0.8873200091721178\n",
      "0     0.9955406394 0.9939009409 0.3981228 0.4023231\n",
      "Epoch: 2, iteration: 1, loss: 1.0259554074193695\n",
      "Epoch: 2, iteration: 2, loss: 0.8684510697190704\n",
      "Epoch: 2, iteration: 3, loss: 0.840744437333177\n",
      "Epoch: 2, iteration: 4, loss: 0.8973478283300256\n",
      "Epoch: 2, iteration: 5, loss: 0.9301950166773092\n",
      "Epoch: 2, iteration: 6, loss: 0.972413146686759\n",
      "Epoch: 2, iteration: 7, loss: 1.152983416505351\n",
      "Epoch: 2, iteration: 8, loss: 0.9690647181811294\n",
      "Epoch: 2, iteration: 9, loss: 0.8512818334344147\n",
      "Epoch: 2, iteration: 10, loss: 0.9333903047479114\n",
      "Epoch: 2, iteration: 11, loss: 0.8872773944037671\n",
      "Epoch: 2, iteration: 12, loss: 0.9602974819830145\n",
      "Epoch: 2, iteration: 13, loss: 0.8628445094998444\n",
      "Epoch: 2, iteration: 14, loss: 0.9218105962866922\n",
      "Epoch: 2, iteration: 15, loss: 1.4238893896233051\n",
      "Epoch: 2, iteration: 16, loss: 0.9118769963228238\n",
      "Epoch: 2, iteration: 17, loss: 0.8434731517094711\n",
      "Epoch: 2, iteration: 18, loss: 0.7728742890163526\n",
      "Epoch: 2, iteration: 19, loss: 0.9911249954678931\n",
      "Epoch: 2, iteration: 20, loss: 1.0631267586061794\n",
      "Epoch: 2, iteration: 21, loss: 1.186226578276007\n",
      "Epoch: 2, iteration: 22, loss: 0.8917868683925034\n",
      "Epoch: 2, iteration: 23, loss: 0.9861644397981407\n",
      "Epoch: 2, iteration: 24, loss: 0.7686974677230868\n",
      "Epoch: 2, iteration: 25, loss: 0.9273785433727162\n",
      "Epoch: 2, iteration: 26, loss: 1.037091786734457\n",
      "Epoch: 2, iteration: 27, loss: 1.1892674247852388\n",
      "Epoch: 2, iteration: 28, loss: 0.9570692737521707\n",
      "Epoch: 2, iteration: 29, loss: 0.8574259553972653\n",
      "Epoch: 2, iteration: 30, loss: 0.9246809505462336\n",
      "Epoch: 2, iteration: 31, loss: 0.9086130925165065\n",
      "Epoch: 2, iteration: 32, loss: 1.2164686492738803\n",
      "Epoch: 2, iteration: 33, loss: 0.9623554294146212\n",
      "Epoch: 2, iteration: 34, loss: 1.3465634769070143\n",
      "Epoch: 2, iteration: 35, loss: 0.885890986973731\n",
      "Epoch: 2, iteration: 36, loss: 0.9342582332328497\n",
      "Epoch: 2, iteration: 37, loss: 0.8692631878316237\n",
      "Epoch: 2, iteration: 38, loss: 0.9483934486343374\n",
      "Epoch: 2, iteration: 39, loss: 0.8567842187949924\n",
      "Epoch: 2, iteration: 40, loss: 0.7670172616356192\n",
      "Epoch: 2, iteration: 41, loss: 1.055982196780385\n",
      "Epoch: 2, iteration: 42, loss: 0.7841869934435771\n",
      "Epoch: 2, iteration: 43, loss: 0.933271602197082\n",
      "Epoch: 2, iteration: 44, loss: 1.035532695725205\n",
      "Epoch: 2, iteration: 45, loss: 0.920323754806643\n",
      "Epoch: 2, iteration: 46, loss: 1.2684487626400707\n",
      "Epoch: 2, iteration: 47, loss: 1.0001767800396673\n",
      "Epoch: 2, iteration: 48, loss: 0.9315972510427963\n",
      "Epoch: 2, iteration: 49, loss: 1.0625371919227427\n",
      "Epoch: 2, iteration: 50, loss: 0.7716304173910099\n",
      "Epoch: 2, iteration: 51, loss: 0.8491371964382886\n",
      "Epoch: 2, iteration: 52, loss: 0.9331385454261322\n",
      "Epoch: 2, iteration: 53, loss: 0.9832780402543574\n",
      "Epoch: 2, iteration: 54, loss: 1.0209874249602584\n",
      "Epoch: 2, iteration: 55, loss: 1.813273668232644\n",
      "Epoch: 2, iteration: 56, loss: 1.0544395754290798\n",
      "Epoch: 2, iteration: 57, loss: 1.294069745988328\n",
      "Epoch: 2, iteration: 58, loss: 1.077655416568461\n",
      "Epoch: 2, iteration: 59, loss: 0.9635523933120839\n",
      "Epoch: 2, iteration: 60, loss: 0.9961195402394739\n",
      "Epoch: 2, iteration: 61, loss: 0.7929872382148941\n",
      "Epoch: 2, iteration: 62, loss: 0.8844703048471069\n",
      "Epoch: 2, iteration: 63, loss: 1.0104469115415378\n",
      "Epoch: 2, iteration: 64, loss: 0.954848440656468\n",
      "Epoch: 2, iteration: 65, loss: 0.7999587847339312\n",
      "Epoch: 2, iteration: 66, loss: 0.9086296363180878\n",
      "Epoch: 2, iteration: 67, loss: 0.9648703282266683\n",
      "Epoch: 2, iteration: 68, loss: 0.8440090782362204\n",
      "Epoch: 2, iteration: 69, loss: 0.9677583923393566\n",
      "Epoch: 2, iteration: 70, loss: 2.26933023382936\n",
      "Epoch: 2, iteration: 71, loss: 0.8954768408878864\n",
      "Epoch: 2, iteration: 72, loss: 0.9208860686657379\n",
      "Epoch: 2, iteration: 73, loss: 0.9855594318088713\n",
      "Epoch: 2, iteration: 74, loss: 0.9972568469692469\n",
      "Epoch: 2, iteration: 75, loss: 0.8417628155350625\n",
      "Epoch: 2, iteration: 76, loss: 0.803546543776078\n",
      "Epoch: 2, iteration: 77, loss: 0.962276791563798\n",
      "Epoch: 2, iteration: 78, loss: 1.0384922778607788\n",
      "Epoch: 2, iteration: 79, loss: 0.8605892441301087\n",
      "Epoch: 2, iteration: 80, loss: 0.9782896785392007\n",
      "Epoch: 2, iteration: 81, loss: 0.9505523821134189\n",
      "Epoch: 2, iteration: 82, loss: 0.9081548854765692\n",
      "Epoch: 2, iteration: 83, loss: 0.8624297943028775\n",
      "Epoch: 2, iteration: 84, loss: 0.9593785867005084\n",
      "Epoch: 2, iteration: 85, loss: 0.9185973086758942\n",
      "Epoch: 2, iteration: 86, loss: 0.801958337601974\n",
      "Epoch: 2, iteration: 87, loss: 1.1273885190551942\n",
      "Epoch: 2, iteration: 88, loss: 1.0675100535234405\n",
      "Epoch: 2, iteration: 89, loss: 1.0120629953129083\n",
      "Epoch: 2, iteration: 90, loss: 1.0925945798501122\n",
      "Epoch: 2, iteration: 91, loss: 0.8862414669649377\n",
      "Epoch: 2, iteration: 92, loss: 1.0009194751402122\n",
      "Epoch: 2, iteration: 93, loss: 0.7740563351153313\n",
      "Epoch: 2, iteration: 94, loss: 0.7434710224578257\n",
      "Epoch: 2, iteration: 95, loss: 0.9252596557423162\n",
      "Epoch: 2, iteration: 96, loss: 0.7321256342497626\n",
      "Epoch: 2, iteration: 97, loss: 1.0460838758868396\n",
      "Epoch: 2, iteration: 98, loss: 0.8029685006815215\n",
      "Epoch: 2, iteration: 99, loss: 1.070479603217124\n",
      "Epoch: 2, iteration: 100, loss: 0.7283931239212599\n",
      "Epoch: 2, iteration: 101, loss: 0.8110559751729961\n",
      "Epoch: 2, iteration: 102, loss: 1.2104780850074863\n",
      "Epoch: 2, iteration: 103, loss: 0.8283038170661255\n",
      "Epoch: 2, iteration: 104, loss: 0.7466545469901169\n",
      "Epoch: 2, iteration: 105, loss: 0.8446456521310698\n",
      "Epoch: 2, iteration: 106, loss: 0.8042143542445476\n",
      "Epoch: 2, iteration: 107, loss: 0.7150707400518989\n",
      "Epoch: 2, iteration: 108, loss: 1.0540175634055986\n",
      "Epoch: 2, iteration: 109, loss: 0.7194260670512509\n",
      "Epoch: 2, iteration: 110, loss: 0.8212349878499237\n",
      "Epoch: 2, iteration: 111, loss: 0.8128195242916185\n",
      "Epoch: 2, iteration: 112, loss: 0.7444788856600527\n",
      "Epoch: 2, iteration: 113, loss: 0.8489345250502568\n",
      "Epoch: 2, iteration: 114, loss: 0.8826418160267747\n",
      "Epoch: 2, iteration: 115, loss: 0.7879367276345124\n",
      "Epoch: 2, iteration: 116, loss: 1.090180104775539\n",
      "Epoch: 2, iteration: 117, loss: 0.8923532284122828\n",
      "Epoch: 2, iteration: 118, loss: 0.8361650041055897\n",
      "Epoch: 2, iteration: 119, loss: 1.7881873144137008\n",
      "Epoch: 2, iteration: 120, loss: 0.8187046400616126\n",
      "Epoch: 2, iteration: 121, loss: 0.7403963435039957\n",
      "Epoch: 2, iteration: 122, loss: 0.7931670660111627\n",
      "Epoch: 2, iteration: 123, loss: 0.7756865514327801\n",
      "Epoch: 2, iteration: 124, loss: 0.7383916805473992\n",
      "Epoch: 2, iteration: 125, loss: 0.6853370932248892\n",
      "Epoch: 2, iteration: 126, loss: 0.8908008293824151\n",
      "Epoch: 2, iteration: 127, loss: 0.8226045361918165\n",
      "Epoch: 2, iteration: 128, loss: 0.8687498787295906\n",
      "Epoch: 2, iteration: 129, loss: 0.799124333221968\n",
      "Epoch: 2, iteration: 130, loss: 1.2509479561731534\n",
      "Epoch: 2, iteration: 131, loss: 1.1172901680134562\n",
      "Epoch: 2, iteration: 132, loss: 0.9956421542776338\n",
      "Epoch: 2, iteration: 133, loss: 0.974342871042746\n",
      "Epoch: 2, iteration: 134, loss: 0.7963777518353033\n",
      "Epoch: 2, iteration: 135, loss: 0.8802391470408167\n",
      "Epoch: 2, iteration: 136, loss: 1.0612465692101203\n",
      "Epoch: 2, iteration: 137, loss: 0.6903867109656818\n",
      "Epoch: 2, iteration: 138, loss: 0.9297022016279959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, iteration: 139, loss: 1.0402844826356974\n",
      "Epoch: 2, iteration: 140, loss: 0.8252338949699444\n",
      "Epoch: 2, iteration: 141, loss: 1.1047490977805021\n",
      "Epoch: 2, iteration: 142, loss: 0.9449351458133544\n",
      "Epoch: 2, iteration: 143, loss: 0.7271217465300271\n",
      "Epoch: 2, iteration: 144, loss: 1.0433470516805101\n",
      "Epoch: 2, iteration: 145, loss: 0.8614337418433536\n",
      "Epoch: 2, iteration: 146, loss: 1.104852122907366\n",
      "Epoch: 2, iteration: 147, loss: 1.1520677948599893\n",
      "Epoch: 2, iteration: 148, loss: 0.8012939914370004\n",
      "Epoch: 2, iteration: 149, loss: 0.7228323916598851\n",
      "Epoch: 2, iteration: 150, loss: 0.9562745221717455\n",
      "Epoch: 2, iteration: 151, loss: 0.7459229751780839\n",
      "Epoch: 2, iteration: 152, loss: 0.7648956749164342\n",
      "Epoch: 2, iteration: 153, loss: 0.7218887192073155\n",
      "Epoch: 2, iteration: 154, loss: 1.2289078662217843\n",
      "Epoch: 2, iteration: 155, loss: 0.8688317506553057\n",
      "Epoch: 2, iteration: 156, loss: 0.8149393392912591\n",
      "Epoch: 2, iteration: 157, loss: 0.865595392033135\n",
      "Epoch: 2, iteration: 158, loss: 0.8517291212489833\n",
      "Epoch: 2, iteration: 159, loss: 0.6573525060184661\n",
      "Epoch: 2, iteration: 160, loss: 0.7973716421271204\n",
      "Epoch: 2, iteration: 161, loss: 0.5894530621730416\n",
      "Epoch: 2, iteration: 162, loss: 0.7788164347208152\n",
      "Epoch: 2, iteration: 163, loss: 0.6741496734311416\n",
      "Epoch: 2, iteration: 164, loss: 0.9238935189429237\n",
      "Epoch: 2, iteration: 165, loss: 0.7717790396365719\n",
      "Epoch: 2, iteration: 166, loss: 0.7784187661151352\n",
      "Epoch: 2, iteration: 167, loss: 0.9438878189223875\n",
      "Epoch: 2, iteration: 168, loss: 0.9036367964234727\n",
      "Epoch: 2, iteration: 169, loss: 0.7621455871679967\n",
      "Epoch: 2, iteration: 170, loss: 0.8064793442678759\n",
      "Epoch: 2, iteration: 171, loss: 0.7821111389385939\n",
      "Epoch: 2, iteration: 172, loss: 0.6585223280809631\n",
      "Epoch: 2, iteration: 173, loss: 0.6646387933346247\n",
      "Epoch: 2, iteration: 174, loss: 0.7676725662904789\n",
      "Epoch: 2, iteration: 175, loss: 0.8702260891073118\n",
      "Epoch: 2, iteration: 176, loss: 0.8957372472983781\n",
      "Epoch: 2, iteration: 177, loss: 0.863303283873014\n",
      "Epoch: 2, iteration: 178, loss: 0.7610168398785336\n",
      "Epoch: 2, iteration: 179, loss: 0.6192272405321725\n",
      "Epoch: 2, iteration: 180, loss: 0.8059883625642508\n",
      "Epoch: 2, iteration: 181, loss: 0.9313151128250914\n",
      "Epoch: 2, iteration: 182, loss: 0.8304757066945408\n",
      "Epoch: 2, iteration: 183, loss: 0.7919221803775414\n",
      "Epoch: 2, iteration: 184, loss: 0.8423223714814012\n",
      "Epoch: 2, iteration: 185, loss: 0.6905987892796454\n",
      "Epoch: 2, iteration: 186, loss: 0.745351394564485\n",
      "Epoch: 2, iteration: 187, loss: 0.893539304084209\n",
      "Epoch: 2, iteration: 188, loss: 0.9074337257243492\n",
      "1     0.7478835918 0.7689433864 0.4318928 0.4581036\n",
      "Epoch: 3, iteration: 1, loss: 0.7343752728331457\n",
      "Epoch: 3, iteration: 2, loss: 0.8267870213372023\n",
      "Epoch: 3, iteration: 3, loss: 0.6328588880597017\n",
      "Epoch: 3, iteration: 4, loss: 0.9853688548283894\n",
      "Epoch: 3, iteration: 5, loss: 0.8400211145631555\n",
      "Epoch: 3, iteration: 6, loss: 0.794894565229583\n",
      "Epoch: 3, iteration: 7, loss: 0.8345892435062549\n",
      "Epoch: 3, iteration: 8, loss: 0.6696804917373632\n",
      "Epoch: 3, iteration: 9, loss: 0.8574906117667404\n",
      "Epoch: 3, iteration: 10, loss: 0.8307390409564432\n",
      "Epoch: 3, iteration: 11, loss: 0.6887835314002406\n",
      "Epoch: 3, iteration: 12, loss: 0.728650456220948\n",
      "Epoch: 3, iteration: 13, loss: 0.8276654935270751\n",
      "Epoch: 3, iteration: 14, loss: 0.6882238019180001\n",
      "Epoch: 3, iteration: 15, loss: 0.8455092965721095\n",
      "Epoch: 3, iteration: 16, loss: 0.8008391583769158\n",
      "Epoch: 3, iteration: 17, loss: 1.1100822803377048\n",
      "Epoch: 3, iteration: 18, loss: 0.718391637491171\n",
      "Epoch: 3, iteration: 19, loss: 0.8508446795872916\n",
      "Epoch: 3, iteration: 20, loss: 0.8099097368252661\n",
      "Epoch: 3, iteration: 21, loss: 0.7831412297988447\n",
      "Epoch: 3, iteration: 22, loss: 0.7504843931094634\n",
      "Epoch: 3, iteration: 23, loss: 0.6670402058518162\n",
      "Epoch: 3, iteration: 24, loss: 0.702262562861744\n",
      "Epoch: 3, iteration: 25, loss: 0.6169699556326683\n",
      "Epoch: 3, iteration: 26, loss: 0.7414651713382518\n",
      "Epoch: 3, iteration: 27, loss: 0.5952302826337843\n",
      "Epoch: 3, iteration: 28, loss: 0.7111883846887398\n",
      "Epoch: 3, iteration: 29, loss: 0.6872778270134214\n",
      "Epoch: 3, iteration: 30, loss: 0.7170763491735775\n",
      "Epoch: 3, iteration: 31, loss: 0.8171343158709348\n",
      "Epoch: 3, iteration: 32, loss: 0.7297615781132085\n",
      "Epoch: 3, iteration: 33, loss: 0.730766473352113\n",
      "Epoch: 3, iteration: 34, loss: 0.5555594983417299\n",
      "Epoch: 3, iteration: 35, loss: 0.7163040046625959\n",
      "Epoch: 3, iteration: 36, loss: 0.6347957649738991\n",
      "Epoch: 3, iteration: 37, loss: 0.8507326785835355\n",
      "Epoch: 3, iteration: 38, loss: 0.8037140733940243\n",
      "Epoch: 3, iteration: 39, loss: 0.7923252289059306\n",
      "Epoch: 3, iteration: 40, loss: 1.0699649434557295\n",
      "Epoch: 3, iteration: 41, loss: 0.6852919123703587\n",
      "Epoch: 3, iteration: 42, loss: 0.8299467948732879\n",
      "Epoch: 3, iteration: 43, loss: 0.661827223954568\n",
      "Epoch: 3, iteration: 44, loss: 0.9015662677574449\n",
      "Epoch: 3, iteration: 45, loss: 0.6740860212886869\n",
      "Epoch: 3, iteration: 46, loss: 0.5767910683613104\n",
      "Epoch: 3, iteration: 47, loss: 0.8274197073414407\n",
      "Epoch: 3, iteration: 48, loss: 0.8403217963810955\n",
      "Epoch: 3, iteration: 49, loss: 0.9622330075128005\n",
      "Epoch: 3, iteration: 50, loss: 0.6770898068469975\n",
      "Epoch: 3, iteration: 51, loss: 0.7886486424961405\n",
      "Epoch: 3, iteration: 52, loss: 0.8504681620485518\n",
      "Epoch: 3, iteration: 53, loss: 0.8604839630826066\n",
      "Epoch: 3, iteration: 54, loss: 0.7121206945308544\n",
      "Epoch: 3, iteration: 55, loss: 0.8248644586726789\n",
      "Epoch: 3, iteration: 56, loss: 0.8576158221385766\n",
      "Epoch: 3, iteration: 57, loss: 0.6278733323992587\n",
      "Epoch: 3, iteration: 58, loss: 0.9641582135502967\n",
      "Epoch: 3, iteration: 59, loss: 0.8610903494955489\n",
      "Epoch: 3, iteration: 60, loss: 0.7249229150894879\n",
      "Epoch: 3, iteration: 61, loss: 0.6724391234483017\n",
      "Epoch: 3, iteration: 62, loss: 0.744327288101406\n",
      "Epoch: 3, iteration: 63, loss: 0.6755370317006472\n",
      "Epoch: 3, iteration: 64, loss: 0.9938564250385543\n",
      "Epoch: 3, iteration: 65, loss: 0.8460652888425133\n",
      "Epoch: 3, iteration: 66, loss: 0.6790854756886635\n",
      "Epoch: 3, iteration: 67, loss: 0.6740863325374203\n",
      "Epoch: 3, iteration: 68, loss: 0.9334429818064994\n",
      "Epoch: 3, iteration: 69, loss: 0.8758953281801549\n",
      "Epoch: 3, iteration: 70, loss: 0.6670894979366537\n",
      "Epoch: 3, iteration: 71, loss: 0.5957158750911209\n",
      "Epoch: 3, iteration: 72, loss: 0.8098481786696107\n",
      "Epoch: 3, iteration: 73, loss: 0.8957063696107017\n",
      "Epoch: 3, iteration: 74, loss: 0.6972334644924345\n",
      "Epoch: 3, iteration: 75, loss: 0.7364995652520252\n",
      "Epoch: 3, iteration: 76, loss: 0.6323352102589689\n",
      "Epoch: 3, iteration: 77, loss: 0.6991287210835583\n",
      "Epoch: 3, iteration: 78, loss: 0.5839242078137405\n",
      "Epoch: 3, iteration: 79, loss: 0.6840658189014343\n",
      "Epoch: 3, iteration: 80, loss: 0.7307942879165507\n",
      "Epoch: 3, iteration: 81, loss: 0.6939923197143415\n",
      "Epoch: 3, iteration: 82, loss: 0.6113746059762324\n",
      "Epoch: 3, iteration: 83, loss: 0.6812836069332696\n",
      "Epoch: 3, iteration: 84, loss: 0.82961503621309\n",
      "Epoch: 3, iteration: 85, loss: 0.7361948070162152\n",
      "Epoch: 3, iteration: 86, loss: 0.7938539691661013\n",
      "Epoch: 3, iteration: 87, loss: 1.0038963658899296\n",
      "Epoch: 3, iteration: 88, loss: 0.8885789991590725\n",
      "Epoch: 3, iteration: 89, loss: 0.8591758566190156\n",
      "Epoch: 3, iteration: 90, loss: 0.6864485361992225\n",
      "Epoch: 3, iteration: 91, loss: 0.5589360753796505\n",
      "Epoch: 3, iteration: 92, loss: 0.6492824046363335\n",
      "Epoch: 3, iteration: 93, loss: 0.672260786538961\n",
      "Epoch: 3, iteration: 94, loss: 0.6359174612772195\n",
      "Epoch: 3, iteration: 95, loss: 0.7401069236402138\n",
      "Epoch: 3, iteration: 96, loss: 0.7053504526819574\n",
      "Epoch: 3, iteration: 97, loss: 0.7655114981374116\n",
      "Epoch: 3, iteration: 98, loss: 0.5683456068811904\n",
      "Epoch: 3, iteration: 99, loss: 0.6227997978019821\n",
      "Epoch: 3, iteration: 100, loss: 0.6835655643630684\n",
      "Epoch: 3, iteration: 101, loss: 0.9374220551232733\n",
      "Epoch: 3, iteration: 102, loss: 0.7547311938283148\n",
      "Epoch: 3, iteration: 103, loss: 0.8589022723079562\n",
      "Epoch: 3, iteration: 104, loss: 0.6431783580145646\n",
      "Epoch: 3, iteration: 105, loss: 0.7816232276859266\n",
      "Epoch: 3, iteration: 106, loss: 0.8154427718562711\n",
      "Epoch: 3, iteration: 107, loss: 0.4858083520388407\n",
      "Epoch: 3, iteration: 108, loss: 0.6481407268218521\n",
      "Epoch: 3, iteration: 109, loss: 0.7753840697208837\n",
      "Epoch: 3, iteration: 110, loss: 0.5796593019036445\n",
      "Epoch: 3, iteration: 111, loss: 0.6436639811664047\n",
      "Epoch: 3, iteration: 112, loss: 0.7245138577224751\n",
      "Epoch: 3, iteration: 113, loss: 0.6098604888644124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, iteration: 114, loss: 0.5569904679769706\n",
      "Epoch: 3, iteration: 115, loss: 0.6588393509521553\n",
      "Epoch: 3, iteration: 116, loss: 0.9458604736624929\n",
      "Epoch: 3, iteration: 117, loss: 0.9684875540335719\n",
      "Epoch: 3, iteration: 118, loss: 0.803687476386638\n",
      "Epoch: 3, iteration: 119, loss: 0.8382183288773241\n",
      "Epoch: 3, iteration: 120, loss: 0.878649528662636\n",
      "Epoch: 3, iteration: 121, loss: 1.2556702101666368\n",
      "Epoch: 3, iteration: 122, loss: 0.5970130349286198\n",
      "Epoch: 3, iteration: 123, loss: 0.7684471997838566\n",
      "Epoch: 3, iteration: 124, loss: 0.6326481187918888\n",
      "Epoch: 3, iteration: 125, loss: 0.7971529117723904\n",
      "Epoch: 3, iteration: 126, loss: 0.7631923839950706\n",
      "Epoch: 3, iteration: 127, loss: 0.8006917042985962\n",
      "Epoch: 3, iteration: 128, loss: 0.9503026222402257\n",
      "Epoch: 3, iteration: 129, loss: 0.7904448613524314\n",
      "Epoch: 3, iteration: 130, loss: 0.8003204659359022\n",
      "Epoch: 3, iteration: 131, loss: 0.6599534224103303\n",
      "Epoch: 3, iteration: 132, loss: 0.7249489970298317\n",
      "Epoch: 3, iteration: 133, loss: 0.7276045449987508\n",
      "Epoch: 3, iteration: 134, loss: 0.6298668895115977\n",
      "Epoch: 3, iteration: 135, loss: 0.964898268215319\n",
      "Epoch: 3, iteration: 136, loss: 0.855383109322702\n",
      "Epoch: 3, iteration: 137, loss: 0.5900407988000813\n",
      "Epoch: 3, iteration: 138, loss: 0.7148341357407975\n",
      "Epoch: 3, iteration: 139, loss: 0.5751643688730437\n",
      "Epoch: 3, iteration: 140, loss: 0.6847055082986\n",
      "Epoch: 3, iteration: 141, loss: 0.6759661511358511\n",
      "Epoch: 3, iteration: 142, loss: 0.652702156386551\n",
      "Epoch: 3, iteration: 143, loss: 1.0272447421190345\n",
      "Epoch: 3, iteration: 144, loss: 0.7014541720525439\n",
      "Epoch: 3, iteration: 145, loss: 0.6673212750166024\n",
      "Epoch: 3, iteration: 146, loss: 0.5946686339552137\n",
      "Epoch: 3, iteration: 147, loss: 0.9650249731086182\n",
      "Epoch: 3, iteration: 148, loss: 0.9154581142796504\n",
      "Epoch: 3, iteration: 149, loss: 0.7630173559791009\n",
      "Epoch: 3, iteration: 150, loss: 0.6773365881857446\n",
      "Epoch: 3, iteration: 151, loss: 0.6009766805057695\n",
      "Epoch: 3, iteration: 152, loss: 0.8064848968234799\n",
      "Epoch: 3, iteration: 153, loss: 0.8125669795111762\n",
      "Epoch: 3, iteration: 154, loss: 0.6317191410562601\n",
      "Epoch: 3, iteration: 155, loss: 0.6938682756554567\n",
      "Epoch: 3, iteration: 156, loss: 0.8089679772450109\n",
      "Epoch: 3, iteration: 157, loss: 0.9425512234176836\n",
      "Epoch: 3, iteration: 158, loss: 0.5761640219209282\n",
      "Epoch: 3, iteration: 159, loss: 0.8114774835836712\n",
      "Epoch: 3, iteration: 160, loss: 0.7081561184432908\n",
      "Epoch: 3, iteration: 161, loss: 0.6216346165499618\n",
      "Epoch: 3, iteration: 162, loss: 0.813901672334095\n",
      "Epoch: 3, iteration: 163, loss: 0.6658697772880974\n",
      "Epoch: 3, iteration: 164, loss: 0.7327919362706762\n",
      "Epoch: 3, iteration: 165, loss: 0.7009020268587027\n",
      "Epoch: 3, iteration: 166, loss: 0.7767493592644772\n",
      "Epoch: 3, iteration: 167, loss: 0.5900446061465697\n",
      "Epoch: 3, iteration: 168, loss: 0.584030805321491\n",
      "Epoch: 3, iteration: 169, loss: 0.5042525427485391\n",
      "Epoch: 3, iteration: 170, loss: 0.7385579416271578\n",
      "Epoch: 3, iteration: 171, loss: 0.7269143940019692\n",
      "Epoch: 3, iteration: 172, loss: 0.8034942017463368\n",
      "Epoch: 3, iteration: 173, loss: 0.5665960740559471\n",
      "Epoch: 3, iteration: 174, loss: 0.590543449126664\n",
      "Epoch: 3, iteration: 175, loss: 0.6875692098313378\n",
      "Epoch: 3, iteration: 176, loss: 0.6173372677005451\n",
      "Epoch: 3, iteration: 177, loss: 0.6626930486858195\n",
      "Epoch: 3, iteration: 178, loss: 0.4906571988083192\n",
      "Epoch: 3, iteration: 179, loss: 0.6771668416186591\n",
      "Epoch: 3, iteration: 180, loss: 0.6938569029987915\n",
      "Epoch: 3, iteration: 181, loss: 0.5702359990463862\n",
      "Epoch: 3, iteration: 182, loss: 0.59037523728075\n",
      "Epoch: 3, iteration: 183, loss: 0.4848085856426851\n",
      "Epoch: 3, iteration: 184, loss: 0.5642520491469244\n",
      "Epoch: 3, iteration: 185, loss: 0.8036886983198028\n",
      "Epoch: 3, iteration: 186, loss: 0.5793566230301513\n",
      "Epoch: 3, iteration: 187, loss: 0.6687367311966498\n",
      "Epoch: 3, iteration: 188, loss: 0.8409732917794874\n",
      "2     0.6343551594 0.6955514917 0.4962810 0.5391433\n",
      "Epoch: 4, iteration: 1, loss: 0.6491117922971409\n",
      "Epoch: 4, iteration: 2, loss: 0.661681398311696\n",
      "Epoch: 4, iteration: 3, loss: 0.5263351012730721\n",
      "Epoch: 4, iteration: 4, loss: 0.631185689780466\n",
      "Epoch: 4, iteration: 5, loss: 0.637625905759305\n",
      "Epoch: 4, iteration: 6, loss: 0.7148364545796124\n",
      "Epoch: 4, iteration: 7, loss: 0.8942661422439898\n",
      "Epoch: 4, iteration: 8, loss: 0.5203955155531668\n",
      "Epoch: 4, iteration: 9, loss: 0.6107030643680142\n",
      "Epoch: 4, iteration: 10, loss: 0.6381031585461183\n",
      "Epoch: 4, iteration: 11, loss: 0.7391377882783862\n",
      "Epoch: 4, iteration: 12, loss: 0.6876829097512781\n",
      "Epoch: 4, iteration: 13, loss: 0.71255974964321\n",
      "Epoch: 4, iteration: 14, loss: 0.708440695383183\n",
      "Epoch: 4, iteration: 15, loss: 0.7529551247251464\n",
      "Epoch: 4, iteration: 16, loss: 0.6498723659394389\n",
      "Epoch: 4, iteration: 17, loss: 0.6438801943362167\n",
      "Epoch: 4, iteration: 18, loss: 0.733241802337785\n",
      "Epoch: 4, iteration: 19, loss: 0.688521128314233\n",
      "Epoch: 4, iteration: 20, loss: 0.6090105542580887\n",
      "Epoch: 4, iteration: 21, loss: 0.5278589406960582\n",
      "Epoch: 4, iteration: 22, loss: 0.4892763591448978\n",
      "Epoch: 4, iteration: 23, loss: 0.6297085879602534\n",
      "Epoch: 4, iteration: 24, loss: 0.6920621821557561\n",
      "Epoch: 4, iteration: 25, loss: 0.6759043294986843\n",
      "Epoch: 4, iteration: 26, loss: 0.577600684730768\n",
      "Epoch: 4, iteration: 27, loss: 0.6257097554798902\n",
      "Epoch: 4, iteration: 28, loss: 0.5717482309895952\n",
      "Epoch: 4, iteration: 29, loss: 0.4955452530851052\n",
      "Epoch: 4, iteration: 30, loss: 0.7749848559923604\n",
      "Epoch: 4, iteration: 31, loss: 0.7560937118665103\n",
      "Epoch: 4, iteration: 32, loss: 0.6956765307736097\n",
      "Epoch: 4, iteration: 33, loss: 0.5796855843051367\n",
      "Epoch: 4, iteration: 34, loss: 0.7017139557883848\n",
      "Epoch: 4, iteration: 35, loss: 0.5909362356826762\n",
      "Epoch: 4, iteration: 36, loss: 0.5804378042264549\n",
      "Epoch: 4, iteration: 37, loss: 0.848052443258937\n",
      "Epoch: 4, iteration: 38, loss: 0.5598596341009879\n",
      "Epoch: 4, iteration: 39, loss: 0.6937814481538623\n",
      "Epoch: 4, iteration: 40, loss: 0.4956861808248721\n",
      "Epoch: 4, iteration: 41, loss: 0.6038439047999348\n",
      "Epoch: 4, iteration: 42, loss: 0.5986386914308406\n",
      "Epoch: 4, iteration: 43, loss: 0.5586005026165537\n",
      "Epoch: 4, iteration: 44, loss: 0.8586790406527431\n",
      "Epoch: 4, iteration: 45, loss: 0.606954384093839\n",
      "Epoch: 4, iteration: 46, loss: 0.676908526537045\n",
      "Epoch: 4, iteration: 47, loss: 0.8404197769549628\n",
      "Epoch: 4, iteration: 48, loss: 0.8279144512035338\n",
      "Epoch: 4, iteration: 49, loss: 0.6729986430536775\n",
      "Epoch: 4, iteration: 50, loss: 0.7061937609141852\n",
      "Epoch: 4, iteration: 51, loss: 0.633535833794415\n",
      "Epoch: 4, iteration: 52, loss: 0.5143217579123363\n",
      "Epoch: 4, iteration: 53, loss: 0.474023392441724\n",
      "Epoch: 4, iteration: 54, loss: 0.5510336105782325\n",
      "Epoch: 4, iteration: 55, loss: 0.5201170493102232\n",
      "Epoch: 4, iteration: 56, loss: 0.6914270119457129\n",
      "Epoch: 4, iteration: 57, loss: 0.6518161416677515\n",
      "Epoch: 4, iteration: 58, loss: 0.7034634725286907\n",
      "Epoch: 4, iteration: 59, loss: 0.6473508820026614\n",
      "Epoch: 4, iteration: 60, loss: 0.6666818214131438\n",
      "Epoch: 4, iteration: 61, loss: 0.6346543544193436\n",
      "Epoch: 4, iteration: 62, loss: 0.5501867088103756\n",
      "Epoch: 4, iteration: 63, loss: 0.7268863266194848\n",
      "Epoch: 4, iteration: 64, loss: 0.716008546415359\n",
      "Epoch: 4, iteration: 65, loss: 0.5340415982937183\n",
      "Epoch: 4, iteration: 66, loss: 0.5146484768054953\n",
      "Epoch: 4, iteration: 67, loss: 0.6422822401968661\n",
      "Epoch: 4, iteration: 68, loss: 0.6915109307023586\n",
      "Epoch: 4, iteration: 69, loss: 0.6488044720164743\n",
      "Epoch: 4, iteration: 70, loss: 0.6096407072429832\n",
      "Epoch: 4, iteration: 71, loss: 0.6840011758062012\n",
      "Epoch: 4, iteration: 72, loss: 0.7109806090356471\n",
      "Epoch: 4, iteration: 73, loss: 0.6182065145484695\n",
      "Epoch: 4, iteration: 74, loss: 0.6941946184638295\n",
      "Epoch: 4, iteration: 75, loss: 0.6562194142722176\n",
      "Epoch: 4, iteration: 76, loss: 0.5417486207803484\n",
      "Epoch: 4, iteration: 77, loss: 0.6484974292673361\n",
      "Epoch: 4, iteration: 78, loss: 0.5468667115028408\n",
      "Epoch: 4, iteration: 79, loss: 0.6656553760924108\n",
      "Epoch: 4, iteration: 80, loss: 0.6516913576009262\n",
      "Epoch: 4, iteration: 81, loss: 0.5547668714132292\n",
      "Epoch: 4, iteration: 82, loss: 0.5204643446849563\n",
      "Epoch: 4, iteration: 83, loss: 0.5145701297787707\n",
      "Epoch: 4, iteration: 84, loss: 0.7490327774763713\n",
      "Epoch: 4, iteration: 85, loss: 0.8437434617624359\n",
      "Epoch: 4, iteration: 86, loss: 0.5537857045127097\n",
      "Epoch: 4, iteration: 87, loss: 0.5732684662318035\n",
      "Epoch: 4, iteration: 88, loss: 0.6339192593330314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, iteration: 89, loss: 0.5896212477546653\n",
      "Epoch: 4, iteration: 90, loss: 0.6702012988954887\n",
      "Epoch: 4, iteration: 91, loss: 1.1159890027933652\n",
      "Epoch: 4, iteration: 92, loss: 0.7426147341321448\n",
      "Epoch: 4, iteration: 93, loss: 0.6052481667338272\n",
      "Epoch: 4, iteration: 94, loss: 0.542508395107798\n",
      "Epoch: 4, iteration: 95, loss: 0.5804809698872203\n",
      "Epoch: 4, iteration: 96, loss: 0.7549133339392217\n",
      "Epoch: 4, iteration: 97, loss: 0.5990464493254589\n",
      "Epoch: 4, iteration: 98, loss: 0.6040594646384376\n",
      "Epoch: 4, iteration: 99, loss: 0.5951373934236982\n",
      "Epoch: 4, iteration: 100, loss: 0.609391525429277\n",
      "Epoch: 4, iteration: 101, loss: 0.7228131778459886\n",
      "Epoch: 4, iteration: 102, loss: 0.7657952354171366\n",
      "Epoch: 4, iteration: 103, loss: 0.7946066972078332\n",
      "Epoch: 4, iteration: 104, loss: 0.5414203296497349\n",
      "Epoch: 4, iteration: 105, loss: 0.6024954210127319\n",
      "Epoch: 4, iteration: 106, loss: 0.6593118655169973\n",
      "Epoch: 4, iteration: 107, loss: 0.943259881424324\n",
      "Epoch: 4, iteration: 108, loss: 0.7814698649351457\n",
      "Epoch: 4, iteration: 109, loss: 0.6966847823950969\n",
      "Epoch: 4, iteration: 110, loss: 0.6854187147889069\n",
      "Epoch: 4, iteration: 111, loss: 0.7363610284145736\n",
      "Epoch: 4, iteration: 112, loss: 0.6637464165050756\n",
      "Epoch: 4, iteration: 113, loss: 1.0790971090847086\n",
      "Epoch: 4, iteration: 114, loss: 0.5723668256454947\n",
      "Epoch: 4, iteration: 115, loss: 0.706980184706316\n",
      "Epoch: 4, iteration: 116, loss: 0.8554447070756039\n",
      "Epoch: 4, iteration: 117, loss: 0.6044034561612613\n",
      "Epoch: 4, iteration: 118, loss: 0.616353751244733\n",
      "Epoch: 4, iteration: 119, loss: 0.4848287352533487\n",
      "Epoch: 4, iteration: 120, loss: 0.5753606825653362\n",
      "Epoch: 4, iteration: 121, loss: 0.6490394511310416\n",
      "Epoch: 4, iteration: 122, loss: 0.6880580763791849\n",
      "Epoch: 4, iteration: 123, loss: 0.7358008183995696\n",
      "Epoch: 4, iteration: 124, loss: 0.648608767344762\n",
      "Epoch: 4, iteration: 125, loss: 0.6188373798223437\n",
      "Epoch: 4, iteration: 126, loss: 0.7212238965448595\n",
      "Epoch: 4, iteration: 127, loss: 0.9095453604082032\n",
      "Epoch: 4, iteration: 128, loss: 0.5106853267953254\n",
      "Epoch: 4, iteration: 129, loss: 0.8806840600017067\n",
      "Epoch: 4, iteration: 130, loss: 0.8558406709580888\n",
      "Epoch: 4, iteration: 131, loss: 0.6225256333745064\n",
      "Epoch: 4, iteration: 132, loss: 0.5992032506599645\n",
      "Epoch: 4, iteration: 133, loss: 0.5575087726361659\n",
      "Epoch: 4, iteration: 134, loss: 0.6723178764382938\n",
      "Epoch: 4, iteration: 135, loss: 0.602241204248438\n",
      "Epoch: 4, iteration: 136, loss: 0.7880822631796972\n",
      "Epoch: 4, iteration: 137, loss: 0.5715333588767647\n",
      "Epoch: 4, iteration: 138, loss: 0.6147226673006214\n",
      "Epoch: 4, iteration: 139, loss: 0.5429542190648166\n",
      "Epoch: 4, iteration: 140, loss: 0.6002637489085568\n",
      "Epoch: 4, iteration: 141, loss: 0.561826036270724\n",
      "Epoch: 4, iteration: 142, loss: 0.7190900910054906\n",
      "Epoch: 4, iteration: 143, loss: 0.6584018055959281\n",
      "Epoch: 4, iteration: 144, loss: 0.6032703938841334\n",
      "Epoch: 4, iteration: 145, loss: 0.7107122493189798\n",
      "Epoch: 4, iteration: 146, loss: 0.6367651482398002\n",
      "Epoch: 4, iteration: 147, loss: 0.6666816269122331\n",
      "Epoch: 4, iteration: 148, loss: 0.6495010497665222\n",
      "Epoch: 4, iteration: 149, loss: 0.7219194884845077\n",
      "Epoch: 4, iteration: 150, loss: 0.7290441313470286\n",
      "Epoch: 4, iteration: 151, loss: 0.6479516024560203\n",
      "Epoch: 4, iteration: 152, loss: 0.5329744220618599\n",
      "Epoch: 4, iteration: 153, loss: 0.6075607059550399\n",
      "Epoch: 4, iteration: 154, loss: 0.6252579277346114\n",
      "Epoch: 4, iteration: 155, loss: 0.5494258048271543\n",
      "Epoch: 4, iteration: 156, loss: 0.6215082991738277\n",
      "Epoch: 4, iteration: 157, loss: 0.620723646131737\n",
      "Epoch: 4, iteration: 158, loss: 0.5066011953766307\n",
      "Epoch: 4, iteration: 159, loss: 0.780484541019751\n",
      "Epoch: 4, iteration: 160, loss: 0.896836867211871\n",
      "Epoch: 4, iteration: 161, loss: 0.43752647897986174\n",
      "Epoch: 4, iteration: 162, loss: 0.5040024599784634\n",
      "Epoch: 4, iteration: 163, loss: 0.5532936200611938\n",
      "Epoch: 4, iteration: 164, loss: 0.5484722712483892\n",
      "Epoch: 4, iteration: 165, loss: 0.6248388485528423\n",
      "Epoch: 4, iteration: 166, loss: 0.5787985609786422\n",
      "Epoch: 4, iteration: 167, loss: 0.4658037974397311\n",
      "Epoch: 4, iteration: 168, loss: 0.6396186373219885\n",
      "Epoch: 4, iteration: 169, loss: 0.5747112281136469\n",
      "Epoch: 4, iteration: 170, loss: 0.7534741730246437\n",
      "Epoch: 4, iteration: 171, loss: 0.5466879138980795\n",
      "Epoch: 4, iteration: 172, loss: 0.6564692747878003\n",
      "Epoch: 4, iteration: 173, loss: 1.0700717361551397\n",
      "Epoch: 4, iteration: 174, loss: 0.531345772522802\n",
      "Epoch: 4, iteration: 175, loss: 0.6218581749147997\n",
      "Epoch: 4, iteration: 176, loss: 0.5969661592088373\n",
      "Epoch: 4, iteration: 177, loss: 0.5535556578842474\n",
      "Epoch: 4, iteration: 178, loss: 0.5927503267589213\n",
      "Epoch: 4, iteration: 179, loss: 0.6724347073964095\n",
      "Epoch: 4, iteration: 180, loss: 0.8360806044759203\n",
      "Epoch: 4, iteration: 181, loss: 0.7298342640996638\n",
      "Epoch: 4, iteration: 182, loss: 0.6002188866643768\n",
      "Epoch: 4, iteration: 183, loss: 0.7708840142429143\n",
      "Epoch: 4, iteration: 184, loss: 0.5984321263757003\n",
      "Epoch: 4, iteration: 185, loss: 0.49704022486461713\n",
      "Epoch: 4, iteration: 186, loss: 0.9901165727616499\n",
      "Epoch: 4, iteration: 187, loss: 0.5480556615303022\n",
      "Epoch: 4, iteration: 188, loss: 0.5410044037829645\n",
      "3     0.6045177808 0.7493167505 0.5196516 0.5364359\n",
      "Epoch: 5, iteration: 1, loss: 0.5529506989854801\n",
      "Epoch: 5, iteration: 2, loss: 0.5615394289222314\n",
      "Epoch: 5, iteration: 3, loss: 0.7424374258217659\n",
      "Epoch: 5, iteration: 4, loss: 0.5954871914181622\n",
      "Epoch: 5, iteration: 5, loss: 0.48819980462392415\n",
      "Epoch: 5, iteration: 6, loss: 0.6273356703795923\n",
      "Epoch: 5, iteration: 7, loss: 0.6411817878975583\n",
      "Epoch: 5, iteration: 8, loss: 0.6101562476080696\n",
      "Epoch: 5, iteration: 9, loss: 0.536894965728959\n",
      "Epoch: 5, iteration: 10, loss: 0.6925586255929091\n",
      "Epoch: 5, iteration: 11, loss: 0.5068100542864441\n",
      "Epoch: 5, iteration: 12, loss: 0.5528298923222243\n",
      "Epoch: 5, iteration: 13, loss: 0.5926251206000921\n",
      "Epoch: 5, iteration: 14, loss: 0.5615198096143856\n",
      "Epoch: 5, iteration: 15, loss: 0.9349810596508557\n",
      "Epoch: 5, iteration: 16, loss: 0.6388038545974497\n",
      "Epoch: 5, iteration: 17, loss: 0.5676042460830882\n",
      "Epoch: 5, iteration: 18, loss: 0.5855246867815431\n",
      "Epoch: 5, iteration: 19, loss: 0.5219314473738023\n",
      "Epoch: 5, iteration: 20, loss: 0.5461280271397525\n",
      "Epoch: 5, iteration: 21, loss: 0.5257466478226175\n",
      "Epoch: 5, iteration: 22, loss: 0.44043313687596103\n",
      "Epoch: 5, iteration: 23, loss: 0.5399832014129942\n",
      "Epoch: 5, iteration: 24, loss: 0.5730573685715075\n",
      "Epoch: 5, iteration: 25, loss: 0.532665309590307\n",
      "Epoch: 5, iteration: 26, loss: 0.48756032405916483\n",
      "Epoch: 5, iteration: 27, loss: 0.4946187050033188\n",
      "Epoch: 5, iteration: 28, loss: 0.48470657702525344\n",
      "Epoch: 5, iteration: 29, loss: 0.6217963448907865\n",
      "Epoch: 5, iteration: 30, loss: 0.49480075548000374\n",
      "Epoch: 5, iteration: 31, loss: 0.5790212776268256\n",
      "Epoch: 5, iteration: 32, loss: 0.6653918838827614\n",
      "Epoch: 5, iteration: 33, loss: 0.669141781228561\n",
      "Epoch: 5, iteration: 34, loss: 0.5982907550942355\n",
      "Epoch: 5, iteration: 35, loss: 0.4158709053228073\n",
      "Epoch: 5, iteration: 36, loss: 0.6562604351494838\n",
      "Epoch: 5, iteration: 37, loss: 0.4453421273873408\n",
      "Epoch: 5, iteration: 38, loss: 0.49725463358476196\n",
      "Epoch: 5, iteration: 39, loss: 0.5759524931066043\n",
      "Epoch: 5, iteration: 40, loss: 0.5919535041462595\n",
      "Epoch: 5, iteration: 41, loss: 0.618293938221347\n",
      "Epoch: 5, iteration: 42, loss: 0.5950518707783562\n",
      "Epoch: 5, iteration: 43, loss: 0.5577689291509567\n",
      "Epoch: 5, iteration: 44, loss: 0.5514465670401498\n",
      "Epoch: 5, iteration: 45, loss: 0.7929439609078734\n",
      "Epoch: 5, iteration: 46, loss: 0.4914907468595399\n",
      "Epoch: 5, iteration: 47, loss: 0.46748439656871765\n",
      "Epoch: 5, iteration: 48, loss: 0.70779931390512\n",
      "Epoch: 5, iteration: 49, loss: 0.5845885705727398\n",
      "Epoch: 5, iteration: 50, loss: 0.4227308972018805\n",
      "Epoch: 5, iteration: 51, loss: 0.5509122194842421\n",
      "Epoch: 5, iteration: 52, loss: 0.531561267850231\n",
      "Epoch: 5, iteration: 53, loss: 0.48153292283623156\n",
      "Epoch: 5, iteration: 54, loss: 0.5848898639507277\n",
      "Epoch: 5, iteration: 55, loss: 0.5131705991572164\n",
      "Epoch: 5, iteration: 56, loss: 0.4152064961970609\n",
      "Epoch: 5, iteration: 57, loss: 0.5030886463754087\n",
      "Epoch: 5, iteration: 58, loss: 0.5146734922778546\n",
      "Epoch: 5, iteration: 59, loss: 0.47459713550724664\n",
      "Epoch: 5, iteration: 60, loss: 0.5917807597760127\n",
      "Epoch: 5, iteration: 61, loss: 0.6258873863334158\n",
      "Epoch: 5, iteration: 62, loss: 0.49209249037540637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, iteration: 63, loss: 0.49750453657371424\n",
      "Epoch: 5, iteration: 64, loss: 0.6434869839697513\n",
      "Epoch: 5, iteration: 65, loss: 0.49107072875504887\n",
      "Epoch: 5, iteration: 66, loss: 0.50737154505866\n",
      "Epoch: 5, iteration: 67, loss: 0.4505584738514699\n",
      "Epoch: 5, iteration: 68, loss: 0.48709200266742725\n",
      "Epoch: 5, iteration: 69, loss: 0.4837890643134233\n",
      "Epoch: 5, iteration: 70, loss: 0.4903826783704982\n",
      "Epoch: 5, iteration: 71, loss: 0.7235780757071031\n",
      "Epoch: 5, iteration: 72, loss: 0.516533898116847\n",
      "Epoch: 5, iteration: 73, loss: 0.5878673244288309\n",
      "Epoch: 5, iteration: 74, loss: 0.49283216506253696\n",
      "Epoch: 5, iteration: 75, loss: 0.4326770623949226\n",
      "Epoch: 5, iteration: 76, loss: 0.5158918023575947\n",
      "Epoch: 5, iteration: 77, loss: 0.7085097744321526\n",
      "Epoch: 5, iteration: 78, loss: 0.49495649719806417\n",
      "Epoch: 5, iteration: 79, loss: 0.48268645714462494\n",
      "Epoch: 5, iteration: 80, loss: 0.5174454069944215\n",
      "Epoch: 5, iteration: 81, loss: 0.7879895487020233\n",
      "Epoch: 5, iteration: 82, loss: 0.702821018462743\n",
      "Epoch: 5, iteration: 83, loss: 0.5236026125183367\n",
      "Epoch: 5, iteration: 84, loss: 0.4773851242340532\n",
      "Epoch: 5, iteration: 85, loss: 0.4980205152416617\n",
      "Epoch: 5, iteration: 86, loss: 0.6821694774265551\n",
      "Epoch: 5, iteration: 87, loss: 0.7527991903420517\n",
      "Epoch: 5, iteration: 88, loss: 0.73096794842148\n",
      "Epoch: 5, iteration: 89, loss: 0.7295353551365781\n",
      "Epoch: 5, iteration: 90, loss: 1.1441893360775224\n",
      "Epoch: 5, iteration: 91, loss: 0.5812272797716056\n",
      "Epoch: 5, iteration: 92, loss: 0.5822418350667458\n",
      "Epoch: 5, iteration: 93, loss: 0.6426471996757059\n",
      "Epoch: 5, iteration: 94, loss: 0.7513562028682519\n",
      "Epoch: 5, iteration: 95, loss: 0.6109000704250708\n",
      "Epoch: 5, iteration: 96, loss: 0.6798509805710217\n",
      "Epoch: 5, iteration: 97, loss: 1.0846308487072713\n",
      "Epoch: 5, iteration: 98, loss: 0.6201950151128695\n",
      "Epoch: 5, iteration: 99, loss: 0.621925315033398\n",
      "Epoch: 5, iteration: 100, loss: 0.7795539859187176\n",
      "Epoch: 5, iteration: 101, loss: 0.9656989415373587\n",
      "Epoch: 5, iteration: 102, loss: 0.5980075132255158\n",
      "Epoch: 5, iteration: 103, loss: 0.5993723791126032\n",
      "Epoch: 5, iteration: 104, loss: 0.4753823752033419\n",
      "Epoch: 5, iteration: 105, loss: 0.5892071195058343\n",
      "Epoch: 5, iteration: 106, loss: 1.0647121937928357\n",
      "Epoch: 5, iteration: 107, loss: 0.6279834922546088\n",
      "Epoch: 5, iteration: 108, loss: 0.7308413423476089\n",
      "Epoch: 5, iteration: 109, loss: 0.7830686966364468\n",
      "Epoch: 5, iteration: 110, loss: 0.5763787137301903\n",
      "Epoch: 5, iteration: 111, loss: 0.5436907555068137\n",
      "Epoch: 5, iteration: 112, loss: 0.5524108247378837\n",
      "Epoch: 5, iteration: 113, loss: 0.5605818337727823\n",
      "Epoch: 5, iteration: 114, loss: 0.5874723566209197\n",
      "Epoch: 5, iteration: 115, loss: 0.5805084086752446\n",
      "Epoch: 5, iteration: 116, loss: 0.5643773826360874\n",
      "Epoch: 5, iteration: 117, loss: 0.706421563030245\n",
      "Epoch: 5, iteration: 118, loss: 0.822195210136206\n",
      "Epoch: 5, iteration: 119, loss: 0.7992580438318863\n",
      "Epoch: 5, iteration: 120, loss: 0.511967426615852\n",
      "Epoch: 5, iteration: 121, loss: 0.591597385539779\n",
      "Epoch: 5, iteration: 122, loss: 0.6836761467132354\n",
      "Epoch: 5, iteration: 123, loss: 0.49488104752512474\n",
      "Epoch: 5, iteration: 124, loss: 0.487240998936696\n",
      "Epoch: 5, iteration: 125, loss: 0.6467278006877554\n",
      "Epoch: 5, iteration: 126, loss: 0.7717156867640106\n",
      "Epoch: 5, iteration: 127, loss: 0.7856213400751322\n",
      "Epoch: 5, iteration: 128, loss: 0.7106133352678097\n",
      "Epoch: 5, iteration: 129, loss: 0.4606558595853016\n",
      "Epoch: 5, iteration: 130, loss: 0.6140508508993026\n",
      "Epoch: 5, iteration: 131, loss: 0.49745733104709994\n",
      "Epoch: 5, iteration: 132, loss: 0.5659874511773029\n",
      "Epoch: 5, iteration: 133, loss: 0.6488556813697974\n",
      "Epoch: 5, iteration: 134, loss: 0.5765648945919645\n",
      "Epoch: 5, iteration: 135, loss: 0.6891455822842847\n",
      "Epoch: 5, iteration: 136, loss: 0.5354550287079906\n",
      "Epoch: 5, iteration: 137, loss: 0.8349260330985787\n",
      "Epoch: 5, iteration: 138, loss: 0.8500455241679459\n",
      "Epoch: 5, iteration: 139, loss: 0.5674543322459735\n",
      "Epoch: 5, iteration: 140, loss: 0.5866725403479215\n",
      "Epoch: 5, iteration: 141, loss: 0.5838063782633024\n",
      "Epoch: 5, iteration: 142, loss: 0.473689321865966\n",
      "Epoch: 5, iteration: 143, loss: 0.5459438831893759\n",
      "Epoch: 5, iteration: 144, loss: 0.6006458096027997\n",
      "Epoch: 5, iteration: 145, loss: 0.6006202612671748\n",
      "Epoch: 5, iteration: 146, loss: 0.5148336286413923\n",
      "Epoch: 5, iteration: 147, loss: 0.5514335929973608\n",
      "Epoch: 5, iteration: 148, loss: 0.6766104123103692\n",
      "Epoch: 5, iteration: 149, loss: 0.6555108557070551\n",
      "Epoch: 5, iteration: 150, loss: 0.6055327624579693\n",
      "Epoch: 5, iteration: 151, loss: 0.5384557395133264\n",
      "Epoch: 5, iteration: 152, loss: 0.5587669497195079\n",
      "Epoch: 5, iteration: 153, loss: 0.6209479810720092\n",
      "Epoch: 5, iteration: 154, loss: 0.47781666598842776\n",
      "Epoch: 5, iteration: 155, loss: 0.4529828405831356\n",
      "Epoch: 5, iteration: 156, loss: 0.7992338608847844\n",
      "Epoch: 5, iteration: 157, loss: 0.43732989476808015\n",
      "Epoch: 5, iteration: 158, loss: 0.4321659784082423\n",
      "Epoch: 5, iteration: 159, loss: 0.4911776158112018\n",
      "Epoch: 5, iteration: 160, loss: 0.7280374362789055\n",
      "Epoch: 5, iteration: 161, loss: 0.6619638015562561\n",
      "Epoch: 5, iteration: 162, loss: 0.47670883543112624\n",
      "Epoch: 5, iteration: 163, loss: 0.484883035367626\n",
      "Epoch: 5, iteration: 164, loss: 0.5012348905015848\n",
      "Epoch: 5, iteration: 165, loss: 0.4309550999931422\n",
      "Epoch: 5, iteration: 166, loss: 0.4887512190507929\n",
      "Epoch: 5, iteration: 167, loss: 0.46537815370685287\n",
      "Epoch: 5, iteration: 168, loss: 0.874748012388442\n",
      "Epoch: 5, iteration: 169, loss: 0.47671775640281916\n",
      "Epoch: 5, iteration: 170, loss: 0.5522407937239613\n",
      "Epoch: 5, iteration: 171, loss: 0.4604386941754953\n",
      "Epoch: 5, iteration: 172, loss: 0.6510643893384198\n",
      "Epoch: 5, iteration: 173, loss: 0.6641068634190855\n",
      "Epoch: 5, iteration: 174, loss: 0.6041121588230342\n",
      "Epoch: 5, iteration: 175, loss: 0.7722652785740971\n",
      "Epoch: 5, iteration: 176, loss: 0.45678011552940706\n",
      "Epoch: 5, iteration: 177, loss: 0.45999495020726383\n",
      "Epoch: 5, iteration: 178, loss: 0.5762567304852931\n",
      "Epoch: 5, iteration: 179, loss: 0.6581400958954855\n",
      "Epoch: 5, iteration: 180, loss: 0.6238702006501115\n",
      "Epoch: 5, iteration: 181, loss: 0.6435910565006034\n",
      "Epoch: 5, iteration: 182, loss: 0.41611596910763066\n",
      "Epoch: 5, iteration: 183, loss: 0.5670558956776811\n",
      "Epoch: 5, iteration: 184, loss: 0.7187814007830615\n",
      "Epoch: 5, iteration: 185, loss: 0.5822192093657776\n",
      "Epoch: 5, iteration: 186, loss: 0.5598918999018445\n",
      "Epoch: 5, iteration: 187, loss: 0.5631366525954603\n",
      "Epoch: 5, iteration: 188, loss: 0.47887610557569005\n",
      "4     0.5172222235 0.7854005115 0.5593391 0.5740930\n",
      "Epoch: 6, iteration: 1, loss: 0.573419966981665\n",
      "Epoch: 6, iteration: 2, loss: 0.5088423757986567\n",
      "Epoch: 6, iteration: 3, loss: 0.51460231929905\n",
      "Epoch: 6, iteration: 4, loss: 0.45878903778597285\n",
      "Epoch: 6, iteration: 5, loss: 0.4528323014511693\n",
      "Epoch: 6, iteration: 6, loss: 0.5518771162455562\n",
      "Epoch: 6, iteration: 7, loss: 0.5463768857265062\n",
      "Epoch: 6, iteration: 8, loss: 0.7872326656532918\n",
      "Epoch: 6, iteration: 9, loss: 0.4795855691560229\n",
      "Epoch: 6, iteration: 10, loss: 0.6446467616846283\n",
      "Epoch: 6, iteration: 11, loss: 0.5234840762249257\n",
      "Epoch: 6, iteration: 12, loss: 0.5076690734980849\n",
      "Epoch: 6, iteration: 13, loss: 0.4763080066998449\n",
      "Epoch: 6, iteration: 14, loss: 1.1123554471819346\n",
      "Epoch: 6, iteration: 15, loss: 0.6007374749252284\n",
      "Epoch: 6, iteration: 16, loss: 0.48395005566863947\n",
      "Epoch: 6, iteration: 17, loss: 0.524727566933545\n",
      "Epoch: 6, iteration: 18, loss: 1.3984631735021893\n",
      "Epoch: 6, iteration: 19, loss: 0.7955579054822545\n",
      "Epoch: 6, iteration: 20, loss: 0.5394349530357914\n",
      "Epoch: 6, iteration: 21, loss: 0.5106131082251085\n",
      "Epoch: 6, iteration: 22, loss: 0.6667311657644586\n",
      "Epoch: 6, iteration: 23, loss: 0.8841416860968628\n",
      "Epoch: 6, iteration: 24, loss: 0.7213574134816425\n",
      "Epoch: 6, iteration: 25, loss: 0.5750127861384681\n",
      "Epoch: 6, iteration: 26, loss: 0.6298980615864513\n",
      "Epoch: 6, iteration: 27, loss: 0.6520261048490249\n",
      "Epoch: 6, iteration: 28, loss: 0.5588723347118069\n",
      "Epoch: 6, iteration: 29, loss: 0.7217888880790897\n",
      "Epoch: 6, iteration: 30, loss: 0.597157453677358\n",
      "Epoch: 6, iteration: 31, loss: 0.5726828198415652\n",
      "Epoch: 6, iteration: 32, loss: 0.5684467078501446\n",
      "Epoch: 6, iteration: 33, loss: 0.7182120392756193\n",
      "Epoch: 6, iteration: 34, loss: 0.6256944249746063\n",
      "Epoch: 6, iteration: 35, loss: 0.7172242133582647\n",
      "Epoch: 6, iteration: 36, loss: 0.6703000829672083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, iteration: 37, loss: 0.6779076845987448\n",
      "Epoch: 6, iteration: 38, loss: 0.6814908068834233\n",
      "Epoch: 6, iteration: 39, loss: 0.5215174024044267\n",
      "Epoch: 6, iteration: 40, loss: 0.5154773912391152\n",
      "Epoch: 6, iteration: 41, loss: 0.637303850241451\n",
      "Epoch: 6, iteration: 42, loss: 0.5265767602510394\n",
      "Epoch: 6, iteration: 43, loss: 0.4871514696357766\n",
      "Epoch: 6, iteration: 44, loss: 0.5058791914914303\n",
      "Epoch: 6, iteration: 45, loss: 0.4269416314353955\n",
      "Epoch: 6, iteration: 46, loss: 0.6389571966797378\n",
      "Epoch: 6, iteration: 47, loss: 0.45195459877797917\n",
      "Epoch: 6, iteration: 48, loss: 0.60176156068293\n",
      "Epoch: 6, iteration: 49, loss: 0.5786630670972606\n",
      "Epoch: 6, iteration: 50, loss: 0.6117055827926013\n",
      "Epoch: 6, iteration: 51, loss: 0.4451970976686408\n",
      "Epoch: 6, iteration: 52, loss: 0.5473221552614641\n",
      "Epoch: 6, iteration: 53, loss: 0.446041139710984\n",
      "Epoch: 6, iteration: 54, loss: 0.5263281613097425\n",
      "Epoch: 6, iteration: 55, loss: 0.8003531495464996\n",
      "Epoch: 6, iteration: 56, loss: 0.49732447310678923\n",
      "Epoch: 6, iteration: 57, loss: 0.4211093087337776\n",
      "Epoch: 6, iteration: 58, loss: 0.7021864602873003\n",
      "Epoch: 6, iteration: 59, loss: 0.8158325312538441\n",
      "Epoch: 6, iteration: 60, loss: 0.4668426321614357\n",
      "Epoch: 6, iteration: 61, loss: 0.528941365909338\n",
      "Epoch: 6, iteration: 62, loss: 0.6516016406391559\n",
      "Epoch: 6, iteration: 63, loss: 0.49251264055296734\n",
      "Epoch: 6, iteration: 64, loss: 0.5341914146445869\n",
      "Epoch: 6, iteration: 65, loss: 0.5510932157488929\n",
      "Epoch: 6, iteration: 66, loss: 0.6048224889931887\n",
      "Epoch: 6, iteration: 67, loss: 0.47953813482075824\n",
      "Epoch: 6, iteration: 68, loss: 0.5450840828369086\n",
      "Epoch: 6, iteration: 69, loss: 0.580273850760185\n",
      "Epoch: 6, iteration: 70, loss: 0.46246556967421565\n",
      "Epoch: 6, iteration: 71, loss: 0.6497570078134242\n",
      "Epoch: 6, iteration: 72, loss: 0.5455612756863812\n",
      "Epoch: 6, iteration: 73, loss: 0.40251409590516146\n",
      "Epoch: 6, iteration: 74, loss: 0.5325084706041219\n",
      "Epoch: 6, iteration: 75, loss: 0.6588958998029085\n",
      "Epoch: 6, iteration: 76, loss: 0.4825985131178188\n",
      "Epoch: 6, iteration: 77, loss: 0.8172096923270072\n",
      "Epoch: 6, iteration: 78, loss: 0.515217964215564\n",
      "Epoch: 6, iteration: 79, loss: 0.5169388667654178\n",
      "Epoch: 6, iteration: 80, loss: 0.7100856382953574\n",
      "Epoch: 6, iteration: 81, loss: 0.531032896673368\n",
      "Epoch: 6, iteration: 82, loss: 0.4984362326654107\n",
      "Epoch: 6, iteration: 83, loss: 0.4600019062564095\n",
      "Epoch: 6, iteration: 84, loss: 0.4397094422959982\n",
      "Epoch: 6, iteration: 85, loss: 0.5904966105455824\n",
      "Epoch: 6, iteration: 86, loss: 0.4666905668356403\n",
      "Epoch: 6, iteration: 87, loss: 0.8206986013044281\n",
      "Epoch: 6, iteration: 88, loss: 0.5466749434908814\n",
      "Epoch: 6, iteration: 89, loss: 0.42267295502904234\n",
      "Epoch: 6, iteration: 90, loss: 0.6308205396985965\n",
      "Epoch: 6, iteration: 91, loss: 0.4707537886437295\n",
      "Epoch: 6, iteration: 92, loss: 0.5870696511612701\n",
      "Epoch: 6, iteration: 93, loss: 0.4552778492489283\n",
      "Epoch: 6, iteration: 94, loss: 0.4591003261083803\n",
      "Epoch: 6, iteration: 95, loss: 0.6350255579628594\n",
      "Epoch: 6, iteration: 96, loss: 0.9011971600741094\n",
      "Epoch: 6, iteration: 97, loss: 0.5386055791738933\n",
      "Epoch: 6, iteration: 98, loss: 0.46176466911117287\n",
      "Epoch: 6, iteration: 99, loss: 0.7481965930378951\n",
      "Epoch: 6, iteration: 100, loss: 0.46822359765769833\n",
      "Epoch: 6, iteration: 101, loss: 0.5867682308625206\n",
      "Epoch: 6, iteration: 102, loss: 0.5823231399771096\n",
      "Epoch: 6, iteration: 103, loss: 0.7106806672280288\n",
      "Epoch: 6, iteration: 104, loss: 0.6032169077436106\n",
      "Epoch: 6, iteration: 105, loss: 0.6179084927260302\n",
      "Epoch: 6, iteration: 106, loss: 0.5870238134873487\n",
      "Epoch: 6, iteration: 107, loss: 0.5270368892156493\n",
      "Epoch: 6, iteration: 108, loss: 0.6616016269147779\n",
      "Epoch: 6, iteration: 109, loss: 0.5775051270062713\n",
      "Epoch: 6, iteration: 110, loss: 0.5657481829325023\n",
      "Epoch: 6, iteration: 111, loss: 0.6970069662345706\n",
      "Epoch: 6, iteration: 112, loss: 1.068676377437802\n",
      "Epoch: 6, iteration: 113, loss: 0.5809248024906445\n",
      "Epoch: 6, iteration: 114, loss: 0.4708408741897259\n",
      "Epoch: 6, iteration: 115, loss: 0.4874734459042069\n",
      "Epoch: 6, iteration: 116, loss: 0.5486054440184714\n",
      "Epoch: 6, iteration: 117, loss: 0.49456173477501475\n",
      "Epoch: 6, iteration: 118, loss: 0.6610513709881101\n",
      "Epoch: 6, iteration: 119, loss: 0.6364690372955204\n",
      "Epoch: 6, iteration: 120, loss: 0.4283230777636\n",
      "Epoch: 6, iteration: 121, loss: 0.512394040474402\n",
      "Epoch: 6, iteration: 122, loss: 0.5679973913010494\n",
      "Epoch: 6, iteration: 123, loss: 0.6218687064059577\n",
      "Epoch: 6, iteration: 124, loss: 0.4732083383237482\n",
      "Epoch: 6, iteration: 125, loss: 0.45976500339746246\n",
      "Epoch: 6, iteration: 126, loss: 0.5769987095580442\n",
      "Epoch: 6, iteration: 127, loss: 0.8454932697307808\n",
      "Epoch: 6, iteration: 128, loss: 0.625379623154034\n",
      "Epoch: 6, iteration: 129, loss: 0.5156699305446899\n",
      "Epoch: 6, iteration: 130, loss: 0.6366898815241395\n",
      "Epoch: 6, iteration: 131, loss: 0.6405680189831292\n",
      "Epoch: 6, iteration: 132, loss: 0.5447985179474963\n",
      "Epoch: 6, iteration: 133, loss: 0.5017231289031999\n",
      "Epoch: 6, iteration: 134, loss: 0.47246234712040985\n",
      "Epoch: 6, iteration: 135, loss: 0.6757298819427345\n",
      "Epoch: 6, iteration: 136, loss: 0.6690280487467232\n",
      "Epoch: 6, iteration: 137, loss: 0.7302509125376125\n",
      "Epoch: 6, iteration: 138, loss: 0.5436783623281927\n",
      "Epoch: 6, iteration: 139, loss: 0.5440387356664268\n",
      "Epoch: 6, iteration: 140, loss: 0.5094651006225543\n",
      "Epoch: 6, iteration: 141, loss: 0.5092440548258246\n",
      "Epoch: 6, iteration: 142, loss: 0.5502933832787144\n",
      "Epoch: 6, iteration: 143, loss: 0.4939695672167192\n",
      "Epoch: 6, iteration: 144, loss: 0.580086361949308\n",
      "Epoch: 6, iteration: 145, loss: 0.6145988139553334\n",
      "Epoch: 6, iteration: 146, loss: 0.5769234190005569\n",
      "Epoch: 6, iteration: 147, loss: 0.5165239152820388\n",
      "Epoch: 6, iteration: 148, loss: 0.4333164322970992\n",
      "Epoch: 6, iteration: 149, loss: 0.4214542183360676\n",
      "Epoch: 6, iteration: 150, loss: 0.4922962246958108\n",
      "Epoch: 6, iteration: 151, loss: 0.5427936913850205\n",
      "Epoch: 6, iteration: 152, loss: 0.4283126683889739\n",
      "Epoch: 6, iteration: 153, loss: 0.4804149313169155\n",
      "Epoch: 6, iteration: 154, loss: 0.4355949066558337\n",
      "Epoch: 6, iteration: 155, loss: 0.45998207977141603\n",
      "Epoch: 6, iteration: 156, loss: 0.7800736221642545\n",
      "Epoch: 6, iteration: 157, loss: 0.5812267991701948\n",
      "Epoch: 6, iteration: 158, loss: 0.54680718063494\n",
      "Epoch: 6, iteration: 159, loss: 0.43142777079005906\n",
      "Epoch: 6, iteration: 160, loss: 0.5024153720490696\n",
      "Epoch: 6, iteration: 161, loss: 0.5114319555729437\n",
      "Epoch: 6, iteration: 162, loss: 0.8517204128208732\n",
      "Epoch: 6, iteration: 163, loss: 0.5979194859985483\n",
      "Epoch: 6, iteration: 164, loss: 0.5521879790823624\n",
      "Epoch: 6, iteration: 165, loss: 0.48327861602236855\n",
      "Epoch: 6, iteration: 166, loss: 0.5751939463460531\n",
      "Epoch: 6, iteration: 167, loss: 0.6546946808178145\n",
      "Epoch: 6, iteration: 168, loss: 0.49226722905355436\n",
      "Epoch: 6, iteration: 169, loss: 0.4919193605918854\n",
      "Epoch: 6, iteration: 170, loss: 0.5318686865783911\n",
      "Epoch: 6, iteration: 171, loss: 0.5601381295039739\n",
      "Epoch: 6, iteration: 172, loss: 0.39073692028644885\n",
      "Epoch: 6, iteration: 173, loss: 0.48579389856182087\n",
      "Epoch: 6, iteration: 174, loss: 0.5710156792944941\n",
      "Epoch: 6, iteration: 175, loss: 0.5967551525497528\n",
      "Epoch: 6, iteration: 176, loss: 0.7167019686928955\n",
      "Epoch: 6, iteration: 177, loss: 0.5203649112343629\n",
      "Epoch: 6, iteration: 178, loss: 0.5083057383463475\n",
      "Epoch: 6, iteration: 179, loss: 0.5058301414257892\n",
      "Epoch: 6, iteration: 180, loss: 0.5894011206337146\n",
      "Epoch: 6, iteration: 181, loss: 0.42908326650965567\n",
      "Epoch: 6, iteration: 182, loss: 0.5691978857946524\n",
      "Epoch: 6, iteration: 183, loss: 0.6854604433009228\n",
      "Epoch: 6, iteration: 184, loss: 0.5532888083824881\n",
      "Epoch: 6, iteration: 185, loss: 0.5041568793735873\n",
      "Epoch: 6, iteration: 186, loss: 0.5087036226029711\n",
      "Epoch: 6, iteration: 187, loss: 0.5615600677173165\n",
      "Epoch: 6, iteration: 188, loss: 0.5719543662221285\n",
      "5     0.4955145785 0.7213867832 0.5724113 0.5863417\n",
      "Epoch: 7, iteration: 1, loss: 0.4528213472296136\n",
      "Epoch: 7, iteration: 2, loss: 0.41090415100398214\n",
      "Epoch: 7, iteration: 3, loss: 0.5722346345684173\n",
      "Epoch: 7, iteration: 4, loss: 0.5233858392688081\n",
      "Epoch: 7, iteration: 5, loss: 0.4604865845681769\n",
      "Epoch: 7, iteration: 6, loss: 0.4370387130864454\n",
      "Epoch: 7, iteration: 7, loss: 0.4446554067963952\n",
      "Epoch: 7, iteration: 8, loss: 0.4632247306583428\n",
      "Epoch: 7, iteration: 9, loss: 0.4618849959299507\n",
      "Epoch: 7, iteration: 10, loss: 0.4347529324265041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, iteration: 11, loss: 0.4986613292729313\n",
      "Epoch: 7, iteration: 12, loss: 0.5208348188584397\n",
      "Epoch: 7, iteration: 13, loss: 0.47933857340701636\n",
      "Epoch: 7, iteration: 14, loss: 0.47794240684265993\n",
      "Epoch: 7, iteration: 15, loss: 0.5089461534915893\n",
      "Epoch: 7, iteration: 16, loss: 0.4864184156733258\n",
      "Epoch: 7, iteration: 17, loss: 0.41407401932349847\n",
      "Epoch: 7, iteration: 18, loss: 0.5560965479645984\n",
      "Epoch: 7, iteration: 19, loss: 0.45658893665153966\n",
      "Epoch: 7, iteration: 20, loss: 0.41607472468559764\n",
      "Epoch: 7, iteration: 21, loss: 0.7646666202775767\n",
      "Epoch: 7, iteration: 22, loss: 0.4218218076260419\n",
      "Epoch: 7, iteration: 23, loss: 0.5827527108700359\n",
      "Epoch: 7, iteration: 24, loss: 0.5290346342583229\n",
      "Epoch: 7, iteration: 25, loss: 0.5447067745404413\n",
      "Epoch: 7, iteration: 26, loss: 0.8898965934006347\n",
      "Epoch: 7, iteration: 27, loss: 0.48746218833134575\n",
      "Epoch: 7, iteration: 28, loss: 0.41418480809648256\n",
      "Epoch: 7, iteration: 29, loss: 0.5127221550707113\n",
      "Epoch: 7, iteration: 30, loss: 0.5537824227480345\n",
      "Epoch: 7, iteration: 31, loss: 0.5130840887016388\n",
      "Epoch: 7, iteration: 32, loss: 0.5988709902766688\n",
      "Epoch: 7, iteration: 33, loss: 0.5178823344206006\n",
      "Epoch: 7, iteration: 34, loss: 0.4227384489783879\n",
      "Epoch: 7, iteration: 35, loss: 0.5928005386502264\n",
      "Epoch: 7, iteration: 36, loss: 0.6099264779210858\n",
      "Epoch: 7, iteration: 37, loss: 0.5792904402349295\n",
      "Epoch: 7, iteration: 38, loss: 0.5767154780466978\n",
      "Epoch: 7, iteration: 39, loss: 0.6662050536202381\n",
      "Epoch: 7, iteration: 40, loss: 0.5430529939569214\n",
      "Epoch: 7, iteration: 41, loss: 0.4620299240295252\n",
      "Epoch: 7, iteration: 42, loss: 0.4032256231533372\n",
      "Epoch: 7, iteration: 43, loss: 0.4639089672998136\n",
      "Epoch: 7, iteration: 44, loss: 0.559616172817455\n",
      "Epoch: 7, iteration: 45, loss: 0.7134131197716935\n",
      "Epoch: 7, iteration: 46, loss: 0.39699064555904845\n",
      "Epoch: 7, iteration: 47, loss: 0.48151571219721206\n",
      "Epoch: 7, iteration: 48, loss: 0.4409281693697742\n",
      "Epoch: 7, iteration: 49, loss: 0.8056282345307925\n",
      "Epoch: 7, iteration: 50, loss: 0.45597028452037247\n",
      "Epoch: 7, iteration: 51, loss: 0.5214163989563062\n",
      "Epoch: 7, iteration: 52, loss: 0.457276842398108\n",
      "Epoch: 7, iteration: 53, loss: 0.5751736534356126\n",
      "Epoch: 7, iteration: 54, loss: 0.5346971688446976\n",
      "Epoch: 7, iteration: 55, loss: 0.4611963745648729\n",
      "Epoch: 7, iteration: 56, loss: 0.5278276930271054\n",
      "Epoch: 7, iteration: 57, loss: 0.6096512812045293\n",
      "Epoch: 7, iteration: 58, loss: 0.560134276726097\n",
      "Epoch: 7, iteration: 59, loss: 0.45177219305797556\n",
      "Epoch: 7, iteration: 60, loss: 0.49430158977503535\n",
      "Epoch: 7, iteration: 61, loss: 0.4632867227474053\n",
      "Epoch: 7, iteration: 62, loss: 0.5388421481792218\n",
      "Epoch: 7, iteration: 63, loss: 0.47870884311610906\n",
      "Epoch: 7, iteration: 64, loss: 0.4665886878869749\n",
      "Epoch: 7, iteration: 65, loss: 0.4291062369437164\n",
      "Epoch: 7, iteration: 66, loss: 0.3994568854311895\n",
      "Epoch: 7, iteration: 67, loss: 0.43360266787033125\n",
      "Epoch: 7, iteration: 68, loss: 0.36716748014902706\n",
      "Epoch: 7, iteration: 69, loss: 0.3853674577954844\n",
      "Epoch: 7, iteration: 70, loss: 0.39513357811752003\n",
      "Epoch: 7, iteration: 71, loss: 0.5172045648375271\n",
      "Epoch: 7, iteration: 72, loss: 0.45667873991072266\n",
      "Epoch: 7, iteration: 73, loss: 0.3881767269824418\n",
      "Epoch: 7, iteration: 74, loss: 0.7676089801888946\n",
      "Epoch: 7, iteration: 75, loss: 0.5095389241062648\n",
      "Epoch: 7, iteration: 76, loss: 0.5764381035228802\n",
      "Epoch: 7, iteration: 77, loss: 1.0581351031901056\n",
      "Epoch: 7, iteration: 78, loss: 0.5066510260198717\n",
      "Epoch: 7, iteration: 79, loss: 0.5925659250211528\n",
      "Epoch: 7, iteration: 80, loss: 1.156431228736791\n",
      "Epoch: 7, iteration: 81, loss: 0.4723167259384852\n",
      "Epoch: 7, iteration: 82, loss: 1.0245256116415258\n",
      "Epoch: 7, iteration: 83, loss: 0.7798861888102873\n",
      "Epoch: 7, iteration: 84, loss: 0.6155096549500498\n",
      "Epoch: 7, iteration: 85, loss: 0.6831366671334504\n",
      "Epoch: 7, iteration: 86, loss: 0.7217584202813828\n",
      "Epoch: 7, iteration: 87, loss: 0.6325189029354922\n",
      "Epoch: 7, iteration: 88, loss: 0.5176184862595161\n",
      "Epoch: 7, iteration: 89, loss: 0.5015234417111414\n",
      "Epoch: 7, iteration: 90, loss: 0.4553644202450983\n",
      "Epoch: 7, iteration: 91, loss: 0.628086986847051\n",
      "Epoch: 7, iteration: 92, loss: 0.5537717397883586\n",
      "Epoch: 7, iteration: 93, loss: 0.4353388536115666\n",
      "Epoch: 7, iteration: 94, loss: 0.532923841619781\n",
      "Epoch: 7, iteration: 95, loss: 0.4301392607635593\n",
      "Epoch: 7, iteration: 96, loss: 0.5434827766530448\n",
      "Epoch: 7, iteration: 97, loss: 0.8951322594477469\n",
      "Epoch: 7, iteration: 98, loss: 0.47477001850520417\n",
      "Epoch: 7, iteration: 99, loss: 0.43780187756869743\n",
      "Epoch: 7, iteration: 100, loss: 0.4972656108797206\n",
      "Epoch: 7, iteration: 101, loss: 0.5221431656779214\n",
      "Epoch: 7, iteration: 102, loss: 0.5153856702307931\n",
      "Epoch: 7, iteration: 103, loss: 0.4026510656048921\n",
      "Epoch: 7, iteration: 104, loss: 0.7247897752127701\n",
      "Epoch: 7, iteration: 105, loss: 0.582224755975032\n",
      "Epoch: 7, iteration: 106, loss: 0.6013079017918771\n",
      "Epoch: 7, iteration: 107, loss: 0.6280388963845612\n",
      "Epoch: 7, iteration: 108, loss: 0.5310089840715997\n",
      "Epoch: 7, iteration: 109, loss: 0.4813490004233452\n",
      "Epoch: 7, iteration: 110, loss: 0.42060705616144217\n",
      "Epoch: 7, iteration: 111, loss: 0.5838010596480445\n",
      "Epoch: 7, iteration: 112, loss: 0.6192438727343751\n",
      "Epoch: 7, iteration: 113, loss: 0.5033169655610746\n",
      "Epoch: 7, iteration: 114, loss: 0.4991354703480716\n",
      "Epoch: 7, iteration: 115, loss: 0.5579498778733657\n",
      "Epoch: 7, iteration: 116, loss: 0.5870842543251834\n",
      "Epoch: 7, iteration: 117, loss: 0.6514209270947657\n",
      "Epoch: 7, iteration: 118, loss: 0.5057093009431035\n",
      "Epoch: 7, iteration: 119, loss: 0.5095903125535411\n",
      "Epoch: 7, iteration: 120, loss: 0.5369267385750188\n",
      "Epoch: 7, iteration: 121, loss: 0.6015936517184693\n",
      "Epoch: 7, iteration: 122, loss: 0.401845601354845\n",
      "Epoch: 7, iteration: 123, loss: 0.5560223070821125\n",
      "Epoch: 7, iteration: 124, loss: 0.49010014663125084\n",
      "Epoch: 7, iteration: 125, loss: 0.6203606596834307\n",
      "Epoch: 7, iteration: 126, loss: 0.7577577530720453\n",
      "Epoch: 7, iteration: 127, loss: 0.4897935938769219\n",
      "Epoch: 7, iteration: 128, loss: 0.6173164503442062\n",
      "Epoch: 7, iteration: 129, loss: 0.47757574836224354\n",
      "Epoch: 7, iteration: 130, loss: 0.5465861600550486\n",
      "Epoch: 7, iteration: 131, loss: 0.5422552063816986\n",
      "Epoch: 7, iteration: 132, loss: 0.7674429224528361\n",
      "Epoch: 7, iteration: 133, loss: 0.5178319310527075\n",
      "Epoch: 7, iteration: 134, loss: 0.4806609450490464\n",
      "Epoch: 7, iteration: 135, loss: 0.5454368658320448\n",
      "Epoch: 7, iteration: 136, loss: 0.5643269108837742\n",
      "Epoch: 7, iteration: 137, loss: 0.5263119350045502\n",
      "Epoch: 7, iteration: 138, loss: 0.5722612251154195\n",
      "Epoch: 7, iteration: 139, loss: 0.6202072042648421\n",
      "Epoch: 7, iteration: 140, loss: 0.4585062122220179\n",
      "Epoch: 7, iteration: 141, loss: 0.5230495814196963\n",
      "Epoch: 7, iteration: 142, loss: 0.48540618457050055\n",
      "Epoch: 7, iteration: 143, loss: 0.5544272906630943\n",
      "Epoch: 7, iteration: 144, loss: 0.46320056138830507\n",
      "Epoch: 7, iteration: 145, loss: 0.4528458018622056\n",
      "Epoch: 7, iteration: 146, loss: 0.5821469486413431\n",
      "Epoch: 7, iteration: 147, loss: 0.9545659382734877\n",
      "Epoch: 7, iteration: 148, loss: 0.5709515303473699\n",
      "Epoch: 7, iteration: 149, loss: 0.4379299179231233\n",
      "Epoch: 7, iteration: 150, loss: 0.43300827096015493\n",
      "Epoch: 7, iteration: 151, loss: 0.520779849233373\n",
      "Epoch: 7, iteration: 152, loss: 0.4958518768577272\n",
      "Epoch: 7, iteration: 153, loss: 0.5452336606380528\n",
      "Epoch: 7, iteration: 154, loss: 0.6430897423659242\n",
      "Epoch: 7, iteration: 155, loss: 0.40359728604927003\n",
      "Epoch: 7, iteration: 156, loss: 0.5218192969274555\n",
      "Epoch: 7, iteration: 157, loss: 0.3629933567637099\n",
      "Epoch: 7, iteration: 158, loss: 0.5434555737976213\n",
      "Epoch: 7, iteration: 159, loss: 0.5514753673500827\n",
      "Epoch: 7, iteration: 160, loss: 0.6702259719955793\n",
      "Epoch: 7, iteration: 161, loss: 0.4852065900404957\n",
      "Epoch: 7, iteration: 162, loss: 0.5049399946888184\n",
      "Epoch: 7, iteration: 163, loss: 0.8984344864075514\n",
      "Epoch: 7, iteration: 164, loss: 0.6792911209229933\n",
      "Epoch: 7, iteration: 165, loss: 0.7286479906832165\n",
      "Epoch: 7, iteration: 166, loss: 0.5311092133054562\n",
      "Epoch: 7, iteration: 167, loss: 0.6311362403795897\n",
      "Epoch: 7, iteration: 168, loss: 0.43962403615384776\n",
      "Epoch: 7, iteration: 169, loss: 0.5118377703371593\n",
      "Epoch: 7, iteration: 170, loss: 0.5732267667691124\n",
      "Epoch: 7, iteration: 171, loss: 0.49204107569052646\n",
      "Epoch: 7, iteration: 172, loss: 0.5073310965589458\n",
      "Epoch: 7, iteration: 173, loss: 0.5099324613240921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, iteration: 174, loss: 0.5351560288595812\n",
      "Epoch: 7, iteration: 175, loss: 0.7450569867026064\n",
      "Epoch: 7, iteration: 176, loss: 0.4092973911999732\n",
      "Epoch: 7, iteration: 177, loss: 0.6215763692788436\n",
      "Epoch: 7, iteration: 178, loss: 0.4391506878401634\n",
      "Epoch: 7, iteration: 179, loss: 0.42244305133372395\n",
      "Epoch: 7, iteration: 180, loss: 0.6044911892740195\n",
      "Epoch: 7, iteration: 181, loss: 0.5049962126855475\n",
      "Epoch: 7, iteration: 182, loss: 0.4737195047895154\n",
      "Epoch: 7, iteration: 183, loss: 0.6914322327209175\n",
      "Epoch: 7, iteration: 184, loss: 0.6647816340570439\n",
      "Epoch: 7, iteration: 185, loss: 0.4646527841975526\n",
      "Epoch: 7, iteration: 186, loss: 0.5476492527042052\n",
      "Epoch: 7, iteration: 187, loss: 0.45504077030821105\n",
      "Epoch: 7, iteration: 188, loss: 0.8028680521423716\n",
      "6     0.5192239959 0.7885950211 0.5411207 0.5740912\n",
      "Epoch: 8, iteration: 1, loss: 0.4475812198917814\n",
      "Epoch: 8, iteration: 2, loss: 0.48479223462466564\n",
      "Epoch: 8, iteration: 3, loss: 0.5317713673606826\n",
      "Epoch: 8, iteration: 4, loss: 0.4632591822033243\n",
      "Epoch: 8, iteration: 5, loss: 0.5554436043521968\n",
      "Epoch: 8, iteration: 6, loss: 0.45755815061111227\n",
      "Epoch: 8, iteration: 7, loss: 0.4669755443113187\n",
      "Epoch: 8, iteration: 8, loss: 0.649405669980598\n",
      "Epoch: 8, iteration: 9, loss: 0.7568886168410144\n",
      "Epoch: 8, iteration: 10, loss: 0.47776736880725984\n",
      "Epoch: 8, iteration: 11, loss: 0.426325029397056\n",
      "Epoch: 8, iteration: 12, loss: 0.42055444454630236\n",
      "Epoch: 8, iteration: 13, loss: 0.4617335268686659\n",
      "Epoch: 8, iteration: 14, loss: 0.4664259114164888\n",
      "Epoch: 8, iteration: 15, loss: 0.42467313911491933\n",
      "Epoch: 8, iteration: 16, loss: 0.49473265440219166\n",
      "Epoch: 8, iteration: 17, loss: 0.5177099723145672\n",
      "Epoch: 8, iteration: 18, loss: 0.4673211720129082\n",
      "Epoch: 8, iteration: 19, loss: 0.31671382594304714\n",
      "Epoch: 8, iteration: 20, loss: 0.38503292650005\n",
      "Epoch: 8, iteration: 21, loss: 0.4718096595747182\n",
      "Epoch: 8, iteration: 22, loss: 0.4262810831614527\n",
      "Epoch: 8, iteration: 23, loss: 0.6086639014963281\n",
      "Epoch: 8, iteration: 24, loss: 0.7946218092425303\n",
      "Epoch: 8, iteration: 25, loss: 0.34925065301609415\n",
      "Epoch: 8, iteration: 26, loss: 0.370084993306382\n",
      "Epoch: 8, iteration: 27, loss: 0.3769237610334827\n",
      "Epoch: 8, iteration: 28, loss: 0.5980459192584038\n",
      "Epoch: 8, iteration: 29, loss: 0.5335650171884814\n",
      "Epoch: 8, iteration: 30, loss: 0.5458824656100691\n",
      "Epoch: 8, iteration: 31, loss: 0.3901900943143814\n",
      "Epoch: 8, iteration: 32, loss: 0.4873356620170997\n",
      "Epoch: 8, iteration: 33, loss: 0.6148040730071695\n",
      "Epoch: 8, iteration: 34, loss: 0.41818028494838616\n",
      "Epoch: 8, iteration: 35, loss: 0.4740717853372228\n",
      "Epoch: 8, iteration: 36, loss: 0.4538251104908848\n",
      "Epoch: 8, iteration: 37, loss: 0.5000576177084995\n",
      "Epoch: 8, iteration: 38, loss: 0.9281131697608835\n",
      "Epoch: 8, iteration: 39, loss: 0.5211029556060167\n",
      "Epoch: 8, iteration: 40, loss: 0.3838951124850574\n",
      "Epoch: 8, iteration: 41, loss: 0.4446184561304355\n",
      "Epoch: 8, iteration: 42, loss: 0.41789446842057965\n",
      "Epoch: 8, iteration: 43, loss: 0.6015374742208615\n",
      "Epoch: 8, iteration: 44, loss: 0.415927058045513\n",
      "Epoch: 8, iteration: 45, loss: 0.41064805140300614\n",
      "Epoch: 8, iteration: 46, loss: 0.552489291179271\n",
      "Epoch: 8, iteration: 47, loss: 1.1038700950909626\n",
      "Epoch: 8, iteration: 48, loss: 0.45596976482522705\n",
      "Epoch: 8, iteration: 49, loss: 0.46995766857834365\n",
      "Epoch: 8, iteration: 50, loss: 0.3673271474000703\n",
      "Epoch: 8, iteration: 51, loss: 0.44802280594156246\n",
      "Epoch: 8, iteration: 52, loss: 0.528217377729253\n",
      "Epoch: 8, iteration: 53, loss: 0.6372376647518331\n",
      "Epoch: 8, iteration: 54, loss: 0.5196605845936196\n",
      "Epoch: 8, iteration: 55, loss: 0.39044406853625596\n",
      "Epoch: 8, iteration: 56, loss: 0.4154352455338512\n",
      "Epoch: 8, iteration: 57, loss: 0.43732491608758656\n",
      "Epoch: 8, iteration: 58, loss: 0.3819552318917382\n",
      "Epoch: 8, iteration: 59, loss: 0.45813262058973736\n",
      "Epoch: 8, iteration: 60, loss: 0.49374849501610163\n",
      "Epoch: 8, iteration: 61, loss: 0.4171899626227003\n",
      "Epoch: 8, iteration: 62, loss: 0.5198121751957842\n",
      "Epoch: 8, iteration: 63, loss: 0.42446182087293055\n",
      "Epoch: 8, iteration: 64, loss: 0.41563798207047037\n",
      "Epoch: 8, iteration: 65, loss: 0.4051403284532046\n",
      "Epoch: 8, iteration: 66, loss: 0.6066285813317512\n",
      "Epoch: 8, iteration: 67, loss: 0.4670949329048404\n",
      "Epoch: 8, iteration: 68, loss: 0.43683511459568014\n",
      "Epoch: 8, iteration: 69, loss: 0.7106581290341673\n",
      "Epoch: 8, iteration: 70, loss: 0.550741190003324\n",
      "Epoch: 8, iteration: 71, loss: 0.7288885918134859\n",
      "Epoch: 8, iteration: 72, loss: 0.398054431735507\n",
      "Epoch: 8, iteration: 73, loss: 0.5259387681222482\n",
      "Epoch: 8, iteration: 74, loss: 0.5114465625726988\n",
      "Epoch: 8, iteration: 75, loss: 0.4018635212629295\n",
      "Epoch: 8, iteration: 76, loss: 0.5434381503025345\n",
      "Epoch: 8, iteration: 77, loss: 0.5862575469425528\n",
      "Epoch: 8, iteration: 78, loss: 0.5382344097578232\n",
      "Epoch: 8, iteration: 79, loss: 0.6912361314300643\n",
      "Epoch: 8, iteration: 80, loss: 0.7099668921750021\n",
      "Epoch: 8, iteration: 81, loss: 0.5229583026715674\n",
      "Epoch: 8, iteration: 82, loss: 0.5067097437798447\n",
      "Epoch: 8, iteration: 83, loss: 0.4835198871369552\n",
      "Epoch: 8, iteration: 84, loss: 0.5263788341865594\n",
      "Epoch: 8, iteration: 85, loss: 0.43621974412298303\n",
      "Epoch: 8, iteration: 86, loss: 0.7886631642831137\n",
      "Epoch: 8, iteration: 87, loss: 0.5814717071436566\n",
      "Epoch: 8, iteration: 88, loss: 0.5577848166749062\n",
      "Epoch: 8, iteration: 89, loss: 0.5564008658630363\n",
      "Epoch: 8, iteration: 90, loss: 0.6583640067251234\n",
      "Epoch: 8, iteration: 91, loss: 0.5499307023632123\n",
      "Epoch: 8, iteration: 92, loss: 1.0143847331786668\n",
      "Epoch: 8, iteration: 93, loss: 0.4350408264560628\n",
      "Epoch: 8, iteration: 94, loss: 0.511800894897378\n",
      "Epoch: 8, iteration: 95, loss: 0.5502730923349892\n",
      "Epoch: 8, iteration: 96, loss: 0.501323870927289\n",
      "Epoch: 8, iteration: 97, loss: 0.4970040216879369\n",
      "Epoch: 8, iteration: 98, loss: 0.4986590781563417\n",
      "Epoch: 8, iteration: 99, loss: 0.5462119258959375\n",
      "Epoch: 8, iteration: 100, loss: 0.7943887449939419\n",
      "Epoch: 8, iteration: 101, loss: 0.4696107796869118\n",
      "Epoch: 8, iteration: 102, loss: 0.5835545381160127\n",
      "Epoch: 8, iteration: 103, loss: 0.5297686798058527\n",
      "Epoch: 8, iteration: 104, loss: 0.5465499286493144\n",
      "Epoch: 8, iteration: 105, loss: 0.494030919117982\n",
      "Epoch: 8, iteration: 106, loss: 0.4173344755018926\n",
      "Epoch: 8, iteration: 107, loss: 0.6165127952542813\n",
      "Epoch: 8, iteration: 108, loss: 0.41995992680081407\n",
      "Epoch: 8, iteration: 109, loss: 0.6261543726430258\n",
      "Epoch: 8, iteration: 110, loss: 0.4983002717707718\n",
      "Epoch: 8, iteration: 111, loss: 0.5591100877192222\n",
      "Epoch: 8, iteration: 112, loss: 0.4628775373779465\n",
      "Epoch: 8, iteration: 113, loss: 0.5279285179469163\n",
      "Epoch: 8, iteration: 114, loss: 0.4549918538853298\n",
      "Epoch: 8, iteration: 115, loss: 0.5320476456116721\n",
      "Epoch: 8, iteration: 116, loss: 0.502571385144293\n",
      "Epoch: 8, iteration: 117, loss: 0.8730652364052393\n",
      "Epoch: 8, iteration: 118, loss: 0.48024451213468994\n",
      "Epoch: 8, iteration: 119, loss: 0.5208781527363566\n",
      "Epoch: 8, iteration: 120, loss: 0.6118540463700282\n",
      "Epoch: 8, iteration: 121, loss: 0.587958936206101\n",
      "Epoch: 8, iteration: 122, loss: 0.5419391683726854\n",
      "Epoch: 8, iteration: 123, loss: 0.6057132368847478\n",
      "Epoch: 8, iteration: 124, loss: 0.374050689139827\n",
      "Epoch: 8, iteration: 125, loss: 0.4009681552757902\n",
      "Epoch: 8, iteration: 126, loss: 0.3969235105846813\n",
      "Epoch: 8, iteration: 127, loss: 0.43687247339462554\n",
      "Epoch: 8, iteration: 128, loss: 0.4291864974619141\n",
      "Epoch: 8, iteration: 129, loss: 0.44992348967527235\n",
      "Epoch: 8, iteration: 130, loss: 0.4572194741664933\n",
      "Epoch: 8, iteration: 131, loss: 0.5955337459849864\n",
      "Epoch: 8, iteration: 132, loss: 0.48857052099049436\n",
      "Epoch: 8, iteration: 133, loss: 0.42316712309662313\n",
      "Epoch: 8, iteration: 134, loss: 0.5002004736074491\n",
      "Epoch: 8, iteration: 135, loss: 0.44454678123993074\n",
      "Epoch: 8, iteration: 136, loss: 0.4176785918801002\n",
      "Epoch: 8, iteration: 137, loss: 0.5762700257227568\n",
      "Epoch: 8, iteration: 138, loss: 0.4914295966500211\n",
      "Epoch: 8, iteration: 139, loss: 0.4460144370653227\n",
      "Epoch: 8, iteration: 140, loss: 0.6454406627178033\n",
      "Epoch: 8, iteration: 141, loss: 0.3878947244012713\n",
      "Epoch: 8, iteration: 142, loss: 0.5262139771389245\n",
      "Epoch: 8, iteration: 143, loss: 0.42746533049400576\n",
      "Epoch: 8, iteration: 144, loss: 0.41703398279948817\n",
      "Epoch: 8, iteration: 145, loss: 0.585407876139048\n",
      "Epoch: 8, iteration: 146, loss: 0.5780806490902536\n",
      "Epoch: 8, iteration: 147, loss: 0.4206998127863258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, iteration: 148, loss: 0.46192110989812535\n",
      "Epoch: 8, iteration: 149, loss: 0.45768612896627775\n",
      "Epoch: 8, iteration: 150, loss: 0.4603024811304586\n",
      "Epoch: 8, iteration: 151, loss: 0.4890223933345283\n",
      "Epoch: 8, iteration: 152, loss: 0.46785545872071305\n",
      "Epoch: 8, iteration: 153, loss: 0.5143539436546349\n",
      "Epoch: 8, iteration: 154, loss: 0.3844987011426876\n",
      "Epoch: 8, iteration: 155, loss: 0.5268120180883304\n",
      "Epoch: 8, iteration: 156, loss: 0.44107733703232166\n",
      "Epoch: 8, iteration: 157, loss: 0.4113772847319699\n",
      "Epoch: 8, iteration: 158, loss: 0.7616812769424011\n",
      "Epoch: 8, iteration: 159, loss: 0.3749434878361752\n",
      "Epoch: 8, iteration: 160, loss: 0.5675851886357907\n",
      "Epoch: 8, iteration: 161, loss: 0.4102415702925585\n",
      "Epoch: 8, iteration: 162, loss: 0.42774084155324116\n",
      "Epoch: 8, iteration: 163, loss: 0.6666795711204465\n",
      "Epoch: 8, iteration: 164, loss: 0.5131878603312665\n",
      "Epoch: 8, iteration: 165, loss: 0.4267896127755406\n",
      "Epoch: 8, iteration: 166, loss: 0.514734154670415\n",
      "Epoch: 8, iteration: 167, loss: 0.5093805001168854\n",
      "Epoch: 8, iteration: 168, loss: 0.541892030273253\n",
      "Epoch: 8, iteration: 169, loss: 0.3917454948211825\n",
      "Epoch: 8, iteration: 170, loss: 0.46407954300719506\n",
      "Epoch: 8, iteration: 171, loss: 0.5133923115774347\n",
      "Epoch: 8, iteration: 172, loss: 0.5047756164933683\n",
      "Epoch: 8, iteration: 173, loss: 0.48051386937191254\n",
      "Epoch: 8, iteration: 174, loss: 0.4180780756665679\n",
      "Epoch: 8, iteration: 175, loss: 0.6825250238410695\n",
      "Epoch: 8, iteration: 176, loss: 0.4192465595792209\n",
      "Epoch: 8, iteration: 177, loss: 0.664473556330941\n",
      "Epoch: 8, iteration: 178, loss: 0.6642377717532865\n",
      "Epoch: 8, iteration: 179, loss: 0.5268910398012671\n",
      "Epoch: 8, iteration: 180, loss: 0.5071724643895813\n",
      "Epoch: 8, iteration: 181, loss: 0.5346941534326295\n",
      "Epoch: 8, iteration: 182, loss: 0.5133265660803448\n",
      "Epoch: 8, iteration: 183, loss: 0.5552459321014644\n",
      "Epoch: 8, iteration: 184, loss: 0.6136308893692756\n",
      "Epoch: 8, iteration: 185, loss: 0.7502646939387686\n",
      "Epoch: 8, iteration: 186, loss: 0.6480876110863288\n",
      "Epoch: 8, iteration: 187, loss: 0.392187357842247\n",
      "Epoch: 8, iteration: 188, loss: 0.5220768956834891\n",
      "7     0.5228045555 0.8488884629 0.5573261 0.5924461\n",
      "Epoch: 9, iteration: 1, loss: 0.8655721282287769\n",
      "Epoch: 9, iteration: 2, loss: 0.43972778460211537\n",
      "Epoch: 9, iteration: 3, loss: 0.5496877043650273\n",
      "Epoch: 9, iteration: 4, loss: 0.5737337038509661\n",
      "Epoch: 9, iteration: 5, loss: 0.8425862954179111\n",
      "Epoch: 9, iteration: 6, loss: 0.6335005339133959\n",
      "Epoch: 9, iteration: 7, loss: 0.392290272408266\n",
      "Epoch: 9, iteration: 8, loss: 0.5056416041162805\n",
      "Epoch: 9, iteration: 9, loss: 0.5262302603649912\n",
      "Epoch: 9, iteration: 10, loss: 0.5239103648134793\n",
      "Epoch: 9, iteration: 11, loss: 0.42678208213244073\n",
      "Epoch: 9, iteration: 12, loss: 0.5056178596239536\n",
      "Epoch: 9, iteration: 13, loss: 0.46683030475912757\n",
      "Epoch: 9, iteration: 14, loss: 0.5078842522646656\n",
      "Epoch: 9, iteration: 15, loss: 0.4166102366022533\n",
      "Epoch: 9, iteration: 16, loss: 0.45796306527538544\n",
      "Epoch: 9, iteration: 17, loss: 0.5172945033673606\n",
      "Epoch: 9, iteration: 18, loss: 0.41677793188794454\n",
      "Epoch: 9, iteration: 19, loss: 0.5747512830587701\n",
      "Epoch: 9, iteration: 20, loss: 0.4350411619058416\n",
      "Epoch: 9, iteration: 21, loss: 0.41566350004584535\n",
      "Epoch: 9, iteration: 22, loss: 0.6261519839645634\n",
      "Epoch: 9, iteration: 23, loss: 0.39860056801633287\n",
      "Epoch: 9, iteration: 24, loss: 0.4075432367991491\n",
      "Epoch: 9, iteration: 25, loss: 0.429141124177764\n",
      "Epoch: 9, iteration: 26, loss: 0.45372453143411784\n",
      "Epoch: 9, iteration: 27, loss: 0.38858434788235596\n",
      "Epoch: 9, iteration: 28, loss: 0.5465434608928071\n",
      "Epoch: 9, iteration: 29, loss: 0.5145820125706903\n",
      "Epoch: 9, iteration: 30, loss: 0.38305730667990284\n",
      "Epoch: 9, iteration: 31, loss: 0.6045524429296112\n",
      "Epoch: 9, iteration: 32, loss: 0.4768783669021944\n",
      "Epoch: 9, iteration: 33, loss: 0.4483340719300864\n",
      "Epoch: 9, iteration: 34, loss: 0.6116786275434286\n",
      "Epoch: 9, iteration: 35, loss: 0.5186356140821464\n",
      "Epoch: 9, iteration: 36, loss: 0.3851466638590918\n",
      "Epoch: 9, iteration: 37, loss: 0.5395246725151626\n",
      "Epoch: 9, iteration: 38, loss: 0.5072650709376457\n",
      "Epoch: 9, iteration: 39, loss: 0.5002029330676723\n",
      "Epoch: 9, iteration: 40, loss: 0.48558489813569466\n",
      "Epoch: 9, iteration: 41, loss: 0.6576639401662967\n",
      "Epoch: 9, iteration: 42, loss: 0.7525724762536873\n",
      "Epoch: 9, iteration: 43, loss: 0.47732260518592057\n",
      "Epoch: 9, iteration: 44, loss: 0.5223931465540619\n",
      "Epoch: 9, iteration: 45, loss: 0.4266522998378025\n",
      "Epoch: 9, iteration: 46, loss: 0.5247320299065794\n",
      "Epoch: 9, iteration: 47, loss: 0.5979732766104104\n",
      "Epoch: 9, iteration: 48, loss: 0.5021476625042899\n",
      "Epoch: 9, iteration: 49, loss: 0.4319466573995615\n",
      "Epoch: 9, iteration: 50, loss: 0.4717995927106398\n",
      "Epoch: 9, iteration: 51, loss: 0.4773383762443941\n",
      "Epoch: 9, iteration: 52, loss: 0.42224417766266453\n",
      "Epoch: 9, iteration: 53, loss: 0.4493313604200873\n",
      "Epoch: 9, iteration: 54, loss: 0.48139596772997517\n",
      "Epoch: 9, iteration: 55, loss: 0.40512636102819966\n",
      "Epoch: 9, iteration: 56, loss: 0.5621195798163517\n",
      "Epoch: 9, iteration: 57, loss: 0.4792050411570601\n",
      "Epoch: 9, iteration: 58, loss: 0.973315019107784\n",
      "Epoch: 9, iteration: 59, loss: 0.36923136193092815\n",
      "Epoch: 9, iteration: 60, loss: 0.9741654736317261\n",
      "Epoch: 9, iteration: 61, loss: 0.4443799175317799\n",
      "Epoch: 9, iteration: 62, loss: 0.4550539140551594\n",
      "Epoch: 9, iteration: 63, loss: 0.42357208663899176\n",
      "Epoch: 9, iteration: 64, loss: 0.512121051026075\n",
      "Epoch: 9, iteration: 65, loss: 0.44179126864410617\n",
      "Epoch: 9, iteration: 66, loss: 0.5130371200152664\n",
      "Epoch: 9, iteration: 67, loss: 0.4960593334202406\n",
      "Epoch: 9, iteration: 68, loss: 0.555233774019613\n",
      "Epoch: 9, iteration: 69, loss: 0.39150258928518844\n",
      "Epoch: 9, iteration: 70, loss: 0.43294242139427236\n",
      "Epoch: 9, iteration: 71, loss: 0.3705935401430783\n",
      "Epoch: 9, iteration: 72, loss: 0.5532995599836744\n",
      "Epoch: 9, iteration: 73, loss: 0.5706878641824147\n",
      "Epoch: 9, iteration: 74, loss: 0.4908689282399122\n",
      "Epoch: 9, iteration: 75, loss: 0.4459569214131594\n",
      "Epoch: 9, iteration: 76, loss: 0.5074162577918696\n",
      "Epoch: 9, iteration: 77, loss: 0.4981071671111561\n",
      "Epoch: 9, iteration: 78, loss: 0.48141421528833683\n",
      "Epoch: 9, iteration: 79, loss: 0.5359308426397907\n",
      "Epoch: 9, iteration: 80, loss: 0.8067499639705629\n",
      "Epoch: 9, iteration: 81, loss: 0.5663296703132142\n",
      "Epoch: 9, iteration: 82, loss: 0.559823977401623\n",
      "Epoch: 9, iteration: 83, loss: 0.43190354923261387\n",
      "Epoch: 9, iteration: 84, loss: 0.507414926881332\n",
      "Epoch: 9, iteration: 85, loss: 0.5993211636029311\n",
      "Epoch: 9, iteration: 86, loss: 0.7194616112840077\n",
      "Epoch: 9, iteration: 87, loss: 0.4254241534353305\n",
      "Epoch: 9, iteration: 88, loss: 0.5277229521843912\n",
      "Epoch: 9, iteration: 89, loss: 0.5644678401257808\n",
      "Epoch: 9, iteration: 90, loss: 0.6581844829244523\n",
      "Epoch: 9, iteration: 91, loss: 0.6203401680657388\n",
      "Epoch: 9, iteration: 92, loss: 0.4419145402576558\n",
      "Epoch: 9, iteration: 93, loss: 0.3971017500765223\n",
      "Epoch: 9, iteration: 94, loss: 0.4800076762383988\n",
      "Epoch: 9, iteration: 95, loss: 0.6691162348379887\n",
      "Epoch: 9, iteration: 96, loss: 0.5665752210997231\n",
      "Epoch: 9, iteration: 97, loss: 0.38740504285887334\n",
      "Epoch: 9, iteration: 98, loss: 0.6284972911460605\n",
      "Epoch: 9, iteration: 99, loss: 0.4987604715719337\n",
      "Epoch: 9, iteration: 100, loss: 0.41996545648863703\n",
      "Epoch: 9, iteration: 101, loss: 0.6207575334970863\n",
      "Epoch: 9, iteration: 102, loss: 0.44413039451802805\n",
      "Epoch: 9, iteration: 103, loss: 0.4585107944134522\n",
      "Epoch: 9, iteration: 104, loss: 0.49482006095409686\n",
      "Epoch: 9, iteration: 105, loss: 0.4357969717009847\n",
      "Epoch: 9, iteration: 106, loss: 0.4622928710426197\n",
      "Epoch: 9, iteration: 107, loss: 0.45808804971313893\n",
      "Epoch: 9, iteration: 108, loss: 0.4363702959472987\n",
      "Epoch: 9, iteration: 109, loss: 0.4336274887965952\n",
      "Epoch: 9, iteration: 110, loss: 0.3217187789357302\n",
      "Epoch: 9, iteration: 111, loss: 0.582540586601977\n",
      "Epoch: 9, iteration: 112, loss: 0.4830067431080769\n",
      "Epoch: 9, iteration: 113, loss: 0.3412035831102433\n",
      "Epoch: 9, iteration: 114, loss: 0.36907267691573453\n",
      "Epoch: 9, iteration: 115, loss: 0.6333516336654867\n",
      "Epoch: 9, iteration: 116, loss: 0.545945224479601\n",
      "Epoch: 9, iteration: 117, loss: 0.4089080526092509\n",
      "Epoch: 9, iteration: 118, loss: 0.4911018363229521\n",
      "Epoch: 9, iteration: 119, loss: 0.36558321918863546\n",
      "Epoch: 9, iteration: 120, loss: 0.39790167540365057\n",
      "Epoch: 9, iteration: 121, loss: 0.45149418888022735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, iteration: 122, loss: 0.47111054308356726\n",
      "Epoch: 9, iteration: 123, loss: 0.42407564241415496\n",
      "Epoch: 9, iteration: 124, loss: 0.42196378737398366\n",
      "Epoch: 9, iteration: 125, loss: 0.5186157606885492\n",
      "Epoch: 9, iteration: 126, loss: 0.474093338152226\n",
      "Epoch: 9, iteration: 127, loss: 0.4253848913074156\n",
      "Epoch: 9, iteration: 128, loss: 0.4891185123558151\n",
      "Epoch: 9, iteration: 129, loss: 0.5338315661274324\n",
      "Epoch: 9, iteration: 130, loss: 0.5159149615596453\n",
      "Epoch: 9, iteration: 131, loss: 0.4376268107985358\n",
      "Epoch: 9, iteration: 132, loss: 0.5017032505344869\n",
      "Epoch: 9, iteration: 133, loss: 0.472957597207263\n",
      "Epoch: 9, iteration: 134, loss: 0.42954748799721\n",
      "Epoch: 9, iteration: 135, loss: 0.4156225756696374\n",
      "Epoch: 9, iteration: 136, loss: 0.5366255639975442\n",
      "Epoch: 9, iteration: 137, loss: 0.47356121623699676\n",
      "Epoch: 9, iteration: 138, loss: 0.41453123382481843\n",
      "Epoch: 9, iteration: 139, loss: 0.4602502801254708\n",
      "Epoch: 9, iteration: 140, loss: 0.3881818287950515\n",
      "Epoch: 9, iteration: 141, loss: 0.38788477738572846\n",
      "Epoch: 9, iteration: 142, loss: 0.5451394164967537\n",
      "Epoch: 9, iteration: 143, loss: 0.4997044453455183\n",
      "Epoch: 9, iteration: 144, loss: 0.5964840038593909\n",
      "Epoch: 9, iteration: 145, loss: 0.492618378834935\n",
      "Epoch: 9, iteration: 146, loss: 0.3875444155269617\n",
      "Epoch: 9, iteration: 147, loss: 0.371873858427194\n",
      "Epoch: 9, iteration: 148, loss: 0.45166224881140443\n",
      "Epoch: 9, iteration: 149, loss: 0.4581462391256389\n",
      "Epoch: 9, iteration: 150, loss: 0.39485599578407\n",
      "Epoch: 9, iteration: 151, loss: 0.48466508025399835\n",
      "Epoch: 9, iteration: 152, loss: 0.3901690781368596\n",
      "Epoch: 9, iteration: 153, loss: 0.40275640049600236\n",
      "Epoch: 9, iteration: 154, loss: 0.4396256461577125\n",
      "Epoch: 9, iteration: 155, loss: 0.5591119671016364\n",
      "Epoch: 9, iteration: 156, loss: 0.4401770076858108\n",
      "Epoch: 9, iteration: 157, loss: 0.4712932829585955\n",
      "Epoch: 9, iteration: 158, loss: 0.4242435692539222\n",
      "Epoch: 9, iteration: 159, loss: 0.39141752557942827\n",
      "Epoch: 9, iteration: 160, loss: 0.43914352734836476\n",
      "Epoch: 9, iteration: 161, loss: 0.39684490946476997\n",
      "Epoch: 9, iteration: 162, loss: 0.448500561590618\n",
      "Epoch: 9, iteration: 163, loss: 0.4682285971071606\n",
      "Epoch: 9, iteration: 164, loss: 0.35062373241267974\n",
      "Epoch: 9, iteration: 165, loss: 0.4267856071177533\n",
      "Epoch: 9, iteration: 166, loss: 0.4671638180905958\n",
      "Epoch: 9, iteration: 167, loss: 0.36633928618510464\n",
      "Epoch: 9, iteration: 168, loss: 0.3974840165364018\n",
      "Epoch: 9, iteration: 169, loss: 0.4281339014080131\n",
      "Epoch: 9, iteration: 170, loss: 0.44778006000280174\n",
      "Epoch: 9, iteration: 171, loss: 0.44378870110253527\n",
      "Epoch: 9, iteration: 172, loss: 0.3837616119139028\n",
      "Epoch: 9, iteration: 173, loss: 0.4282029063389828\n",
      "Epoch: 9, iteration: 174, loss: 0.47648466833649006\n",
      "Epoch: 9, iteration: 175, loss: 0.4260538360199614\n",
      "Epoch: 9, iteration: 176, loss: 0.40385160627794703\n",
      "Epoch: 9, iteration: 177, loss: 0.447049446805783\n",
      "Epoch: 9, iteration: 178, loss: 0.5239886389161528\n",
      "Epoch: 9, iteration: 179, loss: 0.5647328259206695\n",
      "Epoch: 9, iteration: 180, loss: 0.390478600127496\n",
      "Epoch: 9, iteration: 181, loss: 0.4680473326565249\n",
      "Epoch: 9, iteration: 182, loss: 0.42393173107950066\n",
      "Epoch: 9, iteration: 183, loss: 0.4858805879065637\n",
      "Epoch: 9, iteration: 184, loss: 0.5326893685093282\n",
      "Epoch: 9, iteration: 185, loss: 0.42179761409262834\n",
      "Epoch: 9, iteration: 186, loss: 0.40804395145735844\n",
      "Epoch: 9, iteration: 187, loss: 0.40084657664178286\n",
      "Epoch: 9, iteration: 188, loss: 0.43956809759383986\n",
      "8     0.4131473249 0.8358899788 0.5986080 0.6180879\n",
      "Epoch: 10, iteration: 1, loss: 0.4900234621408486\n",
      "Epoch: 10, iteration: 2, loss: 0.2955077051607053\n",
      "Epoch: 10, iteration: 3, loss: 0.39928591785098805\n",
      "Epoch: 10, iteration: 4, loss: 0.38531474316325326\n",
      "Epoch: 10, iteration: 5, loss: 0.492616379490289\n",
      "Epoch: 10, iteration: 6, loss: 0.36476342044297605\n",
      "Epoch: 10, iteration: 7, loss: 0.4012065667182794\n",
      "Epoch: 10, iteration: 8, loss: 0.35075318257972654\n",
      "Epoch: 10, iteration: 9, loss: 0.39526411661190497\n",
      "Epoch: 10, iteration: 10, loss: 0.44223531363740604\n",
      "Epoch: 10, iteration: 11, loss: 0.35876158202346986\n",
      "Epoch: 10, iteration: 12, loss: 0.45343610012020785\n",
      "Epoch: 10, iteration: 13, loss: 0.7092422496592907\n",
      "Epoch: 10, iteration: 14, loss: 0.44868012274921465\n",
      "Epoch: 10, iteration: 15, loss: 0.44465157591574495\n",
      "Epoch: 10, iteration: 16, loss: 0.6287367141659502\n",
      "Epoch: 10, iteration: 17, loss: 0.37424793699372116\n",
      "Epoch: 10, iteration: 18, loss: 0.47996880731916947\n",
      "Epoch: 10, iteration: 19, loss: 0.39846763549724507\n",
      "Epoch: 10, iteration: 20, loss: 0.415509288801588\n",
      "Epoch: 10, iteration: 21, loss: 0.40536003333591164\n",
      "Epoch: 10, iteration: 22, loss: 0.7905080324815859\n",
      "Epoch: 10, iteration: 23, loss: 0.5742637897188413\n",
      "Epoch: 10, iteration: 24, loss: 0.5222561183020586\n",
      "Epoch: 10, iteration: 25, loss: 0.4356458909673706\n",
      "Epoch: 10, iteration: 26, loss: 0.3402779718939338\n",
      "Epoch: 10, iteration: 27, loss: 0.4799105320941112\n",
      "Epoch: 10, iteration: 28, loss: 0.5453480020071786\n",
      "Epoch: 10, iteration: 29, loss: 0.4472918352674593\n",
      "Epoch: 10, iteration: 30, loss: 0.5182526137289178\n",
      "Epoch: 10, iteration: 31, loss: 0.46122952242117604\n",
      "Epoch: 10, iteration: 32, loss: 0.4622202937697434\n",
      "Epoch: 10, iteration: 33, loss: 0.4781165231964274\n",
      "Epoch: 10, iteration: 34, loss: 0.41579703281138175\n",
      "Epoch: 10, iteration: 35, loss: 0.6431878080213814\n",
      "Epoch: 10, iteration: 36, loss: 0.4722341195273679\n",
      "Epoch: 10, iteration: 37, loss: 0.6832915614043132\n",
      "Epoch: 10, iteration: 38, loss: 0.5292555248811276\n",
      "Epoch: 10, iteration: 39, loss: 0.41419335406659585\n",
      "Epoch: 10, iteration: 40, loss: 0.47475845787372506\n",
      "Epoch: 10, iteration: 41, loss: 0.5047615584442658\n",
      "Epoch: 10, iteration: 42, loss: 0.43025749294169263\n",
      "Epoch: 10, iteration: 43, loss: 0.6453751514980646\n",
      "Epoch: 10, iteration: 44, loss: 0.5985065794771448\n",
      "Epoch: 10, iteration: 45, loss: 0.3661585634537214\n",
      "Epoch: 10, iteration: 46, loss: 0.4010153025792448\n",
      "Epoch: 10, iteration: 47, loss: 0.4285934888611832\n",
      "Epoch: 10, iteration: 48, loss: 0.7075485194354867\n",
      "Epoch: 10, iteration: 49, loss: 0.4771525656240946\n",
      "Epoch: 10, iteration: 50, loss: 0.4569049529172627\n",
      "Epoch: 10, iteration: 51, loss: 0.41537560728801554\n",
      "Epoch: 10, iteration: 52, loss: 0.37879711452041304\n",
      "Epoch: 10, iteration: 53, loss: 0.44828597274369114\n",
      "Epoch: 10, iteration: 54, loss: 0.49377014917600354\n",
      "Epoch: 10, iteration: 55, loss: 0.30797753368024705\n",
      "Epoch: 10, iteration: 56, loss: 0.422401716572218\n",
      "Epoch: 10, iteration: 57, loss: 0.3508396756947249\n",
      "Epoch: 10, iteration: 58, loss: 0.6108006583050682\n",
      "Epoch: 10, iteration: 59, loss: 0.5670933888667751\n",
      "Epoch: 10, iteration: 60, loss: 0.32667597444171814\n",
      "Epoch: 10, iteration: 61, loss: 0.4111347191723953\n",
      "Epoch: 10, iteration: 62, loss: 0.4774608093043901\n",
      "Epoch: 10, iteration: 63, loss: 0.6082438689202785\n",
      "Epoch: 10, iteration: 64, loss: 0.43882192678241316\n",
      "Epoch: 10, iteration: 65, loss: 0.555029642811379\n",
      "Epoch: 10, iteration: 66, loss: 0.3735812717029891\n",
      "Epoch: 10, iteration: 67, loss: 0.3163985595162398\n",
      "Epoch: 10, iteration: 68, loss: 0.41610200369962114\n",
      "Epoch: 10, iteration: 69, loss: 0.35255087331192575\n",
      "Epoch: 10, iteration: 70, loss: 0.46981836127104215\n",
      "Epoch: 10, iteration: 71, loss: 0.5497071055699886\n",
      "Epoch: 10, iteration: 72, loss: 0.5914874881944768\n",
      "Epoch: 10, iteration: 73, loss: 0.4208124708915368\n",
      "Epoch: 10, iteration: 74, loss: 0.4223791550183083\n",
      "Epoch: 10, iteration: 75, loss: 0.36473316793748883\n",
      "Epoch: 10, iteration: 76, loss: 0.40268571691073485\n",
      "Epoch: 10, iteration: 77, loss: 0.7102014011548846\n",
      "Epoch: 10, iteration: 78, loss: 0.49083504762813696\n",
      "Epoch: 10, iteration: 79, loss: 0.47718615913564516\n",
      "Epoch: 10, iteration: 80, loss: 0.48152638311160956\n",
      "Epoch: 10, iteration: 81, loss: 0.36243359913562884\n",
      "Epoch: 10, iteration: 82, loss: 0.675590215269185\n",
      "Epoch: 10, iteration: 83, loss: 0.35024358621478285\n",
      "Epoch: 10, iteration: 84, loss: 0.36788163757256764\n",
      "Epoch: 10, iteration: 85, loss: 0.46667377175747915\n",
      "Epoch: 10, iteration: 86, loss: 0.41430326971474\n",
      "Epoch: 10, iteration: 87, loss: 0.5064152328259585\n",
      "Epoch: 10, iteration: 88, loss: 0.43790001782571525\n",
      "Epoch: 10, iteration: 89, loss: 0.413939757201409\n",
      "Epoch: 10, iteration: 90, loss: 0.3833447895200774\n",
      "Epoch: 10, iteration: 91, loss: 0.5052379698885352\n",
      "Epoch: 10, iteration: 92, loss: 0.4315943208039882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, iteration: 93, loss: 0.5270504934791018\n",
      "Epoch: 10, iteration: 94, loss: 0.5031026273764053\n",
      "Epoch: 10, iteration: 95, loss: 0.35849778162838\n",
      "Epoch: 10, iteration: 96, loss: 0.37118085176833754\n",
      "Epoch: 10, iteration: 97, loss: 0.5963851067352671\n",
      "Epoch: 10, iteration: 98, loss: 0.35065987880010857\n",
      "Epoch: 10, iteration: 99, loss: 0.38238194844220724\n",
      "Epoch: 10, iteration: 100, loss: 0.5646205029810261\n",
      "Epoch: 10, iteration: 101, loss: 0.3402454305072163\n",
      "Epoch: 10, iteration: 102, loss: 0.4020664223131729\n",
      "Epoch: 10, iteration: 103, loss: 0.38414151638856736\n",
      "Epoch: 10, iteration: 104, loss: 0.5010704923739349\n",
      "Epoch: 10, iteration: 105, loss: 0.4114386836609968\n",
      "Epoch: 10, iteration: 106, loss: 0.406437244452797\n",
      "Epoch: 10, iteration: 107, loss: 0.3879489187477521\n",
      "Epoch: 10, iteration: 108, loss: 0.39984381494968846\n",
      "Epoch: 10, iteration: 109, loss: 0.3743705843968669\n",
      "Epoch: 10, iteration: 110, loss: 0.3554823501885932\n",
      "Epoch: 10, iteration: 111, loss: 0.3029320860464304\n",
      "Epoch: 10, iteration: 112, loss: 0.7211014364110083\n",
      "Epoch: 10, iteration: 113, loss: 0.3068113438800953\n",
      "Epoch: 10, iteration: 114, loss: 0.45193306002690947\n",
      "Epoch: 10, iteration: 115, loss: 0.39303264005543875\n",
      "Epoch: 10, iteration: 116, loss: 0.45026803957947803\n",
      "Epoch: 10, iteration: 117, loss: 0.39541006715450916\n",
      "Epoch: 10, iteration: 118, loss: 0.42065619094302525\n",
      "Epoch: 10, iteration: 119, loss: 0.38606872935277176\n",
      "Epoch: 10, iteration: 120, loss: 0.3901382850243329\n",
      "Epoch: 10, iteration: 121, loss: 0.47984939002169047\n",
      "Epoch: 10, iteration: 122, loss: 0.3755231086116408\n",
      "Epoch: 10, iteration: 123, loss: 0.7108828518747511\n",
      "Epoch: 10, iteration: 124, loss: 0.4517939032058065\n",
      "Epoch: 10, iteration: 125, loss: 0.32719315358224577\n",
      "Epoch: 10, iteration: 126, loss: 0.5833808008153532\n",
      "Epoch: 10, iteration: 127, loss: 0.3479990309101013\n",
      "Epoch: 10, iteration: 128, loss: 0.39280238558238434\n",
      "Epoch: 10, iteration: 129, loss: 0.4323224546762616\n",
      "Epoch: 10, iteration: 130, loss: 0.3187513601444656\n",
      "Epoch: 10, iteration: 131, loss: 0.45470519708399953\n",
      "Epoch: 10, iteration: 132, loss: 0.5444945671358444\n",
      "Epoch: 10, iteration: 133, loss: 0.31989812969075726\n",
      "Epoch: 10, iteration: 134, loss: 0.4108765391027439\n",
      "Epoch: 10, iteration: 135, loss: 0.4755454530934399\n",
      "Epoch: 10, iteration: 136, loss: 0.8013803526707253\n",
      "Epoch: 10, iteration: 137, loss: 0.43981364529465833\n",
      "Epoch: 10, iteration: 138, loss: 0.43893480627064846\n",
      "Epoch: 10, iteration: 139, loss: 0.4468926458538671\n",
      "Epoch: 10, iteration: 140, loss: 0.3699202162387844\n",
      "Epoch: 10, iteration: 141, loss: 0.4397093507026441\n",
      "Epoch: 10, iteration: 142, loss: 0.3857016178052132\n",
      "Epoch: 10, iteration: 143, loss: 0.4033121751837645\n",
      "Epoch: 10, iteration: 144, loss: 0.4518814807411775\n",
      "Epoch: 10, iteration: 145, loss: 0.8438202190743124\n",
      "Epoch: 10, iteration: 146, loss: 0.34371786499349094\n",
      "Epoch: 10, iteration: 147, loss: 0.4293050605772347\n",
      "Epoch: 10, iteration: 148, loss: 0.4149267671668068\n",
      "Epoch: 10, iteration: 149, loss: 0.49392557723304553\n",
      "Epoch: 10, iteration: 150, loss: 0.5099214681228934\n",
      "Epoch: 10, iteration: 151, loss: 0.4408079480709126\n",
      "Epoch: 10, iteration: 152, loss: 0.40099236379344394\n",
      "Epoch: 10, iteration: 153, loss: 0.4321085949620254\n",
      "Epoch: 10, iteration: 154, loss: 0.3548512338111209\n",
      "Epoch: 10, iteration: 155, loss: 0.4802788455359003\n",
      "Epoch: 10, iteration: 156, loss: 0.4062400741923396\n",
      "Epoch: 10, iteration: 157, loss: 0.5620092946171287\n",
      "Epoch: 10, iteration: 158, loss: 0.7432738152509178\n",
      "Epoch: 10, iteration: 159, loss: 0.4167135882356488\n",
      "Epoch: 10, iteration: 160, loss: 0.44562140771999087\n",
      "Epoch: 10, iteration: 161, loss: 0.43117952139996374\n",
      "Epoch: 10, iteration: 162, loss: 0.3932224232634362\n",
      "Epoch: 10, iteration: 163, loss: 0.5902642849812754\n",
      "Epoch: 10, iteration: 164, loss: 0.37339445052692943\n",
      "Epoch: 10, iteration: 165, loss: 0.3918314004061354\n",
      "Epoch: 10, iteration: 166, loss: 0.38695808258802145\n",
      "Epoch: 10, iteration: 167, loss: 0.44579484751511195\n",
      "Epoch: 10, iteration: 168, loss: 0.4936104606260185\n",
      "Epoch: 10, iteration: 169, loss: 0.4237483927613288\n",
      "Epoch: 10, iteration: 170, loss: 0.4754545567160387\n",
      "Epoch: 10, iteration: 171, loss: 0.5269353911798298\n",
      "Epoch: 10, iteration: 172, loss: 0.38334874479462344\n",
      "Epoch: 10, iteration: 173, loss: 0.403603097912381\n",
      "Epoch: 10, iteration: 174, loss: 0.53632963567437\n",
      "Epoch: 10, iteration: 175, loss: 0.4039883175720868\n",
      "Epoch: 10, iteration: 176, loss: 0.35983934437315773\n",
      "Epoch: 10, iteration: 177, loss: 0.40965277301705266\n",
      "Epoch: 10, iteration: 178, loss: 0.3843375704595747\n",
      "Epoch: 10, iteration: 179, loss: 0.49105770764772\n",
      "Epoch: 10, iteration: 180, loss: 0.3828341415752682\n",
      "Epoch: 10, iteration: 181, loss: 0.3939076167768631\n",
      "Epoch: 10, iteration: 182, loss: 0.3187301189452301\n",
      "Epoch: 10, iteration: 183, loss: 0.4858890622817493\n",
      "Epoch: 10, iteration: 184, loss: 0.39737467857399367\n",
      "Epoch: 10, iteration: 185, loss: 0.5111739118190118\n",
      "Epoch: 10, iteration: 186, loss: 0.5869639222809391\n",
      "Epoch: 10, iteration: 187, loss: 0.4419588199327918\n",
      "Epoch: 10, iteration: 188, loss: 0.546985616459515\n",
      "9     0.4137115762 0.8725628822 0.6112796 0.6175230\n",
      "Epoch: 11, iteration: 1, loss: 0.35043083269738073\n",
      "Epoch: 11, iteration: 2, loss: 0.3704374621963593\n",
      "Epoch: 11, iteration: 3, loss: 0.40725511823239036\n",
      "Epoch: 11, iteration: 4, loss: 0.3807889376426045\n",
      "Epoch: 11, iteration: 5, loss: 0.3400934460884972\n",
      "Epoch: 11, iteration: 6, loss: 0.33128383027623\n",
      "Epoch: 11, iteration: 7, loss: 0.4511542926504679\n",
      "Epoch: 11, iteration: 8, loss: 0.40206379857652885\n",
      "Epoch: 11, iteration: 9, loss: 0.41185014372279555\n",
      "Epoch: 11, iteration: 10, loss: 0.3428910921269611\n",
      "Epoch: 11, iteration: 11, loss: 0.30102493542041486\n",
      "Epoch: 11, iteration: 12, loss: 0.3799993897847709\n",
      "Epoch: 11, iteration: 13, loss: 0.3553920690802349\n",
      "Epoch: 11, iteration: 14, loss: 0.43063203899868524\n",
      "Epoch: 11, iteration: 15, loss: 0.44736206414332275\n",
      "Epoch: 11, iteration: 16, loss: 0.5358554978018744\n",
      "Epoch: 11, iteration: 17, loss: 0.3965182561619207\n",
      "Epoch: 11, iteration: 18, loss: 0.3873325653939953\n",
      "Epoch: 11, iteration: 19, loss: 0.5024154038980365\n",
      "Epoch: 11, iteration: 20, loss: 0.4105601222903625\n",
      "Epoch: 11, iteration: 21, loss: 0.444630203252227\n",
      "Epoch: 11, iteration: 22, loss: 0.39946471360413927\n",
      "Epoch: 11, iteration: 23, loss: 0.5297249037611202\n",
      "Epoch: 11, iteration: 24, loss: 0.3282727568899802\n",
      "Epoch: 11, iteration: 25, loss: 0.3787357391436914\n",
      "Epoch: 11, iteration: 26, loss: 0.4074131712020444\n",
      "Epoch: 11, iteration: 27, loss: 0.3611908877396804\n",
      "Epoch: 11, iteration: 28, loss: 0.3839298577494511\n",
      "Epoch: 11, iteration: 29, loss: 0.3629468398166091\n",
      "Epoch: 11, iteration: 30, loss: 0.35062368768822194\n",
      "Epoch: 11, iteration: 31, loss: 0.318654853531703\n",
      "Epoch: 11, iteration: 32, loss: 0.566754177847756\n",
      "Epoch: 11, iteration: 33, loss: 0.33536797017984543\n",
      "Epoch: 11, iteration: 34, loss: 0.3245607946836911\n",
      "Epoch: 11, iteration: 35, loss: 0.3583104262873787\n",
      "Epoch: 11, iteration: 36, loss: 0.3227891466651375\n",
      "Epoch: 11, iteration: 37, loss: 0.30403221805505576\n",
      "Epoch: 11, iteration: 38, loss: 0.3624747131326655\n",
      "Epoch: 11, iteration: 39, loss: 0.30987281127228056\n",
      "Epoch: 11, iteration: 40, loss: 0.35290755612508407\n",
      "Epoch: 11, iteration: 41, loss: 0.4126004092010235\n",
      "Epoch: 11, iteration: 42, loss: 0.7930813401604496\n",
      "Epoch: 11, iteration: 43, loss: 0.48281673414533227\n",
      "Epoch: 11, iteration: 44, loss: 0.38837942186705926\n",
      "Epoch: 11, iteration: 45, loss: 0.3459183701962137\n",
      "Epoch: 11, iteration: 46, loss: 0.39886357565416275\n",
      "Epoch: 11, iteration: 47, loss: 0.49173395939465997\n",
      "Epoch: 11, iteration: 48, loss: 0.4300655944045189\n",
      "Epoch: 11, iteration: 49, loss: 0.5961022119143355\n",
      "Epoch: 11, iteration: 50, loss: 0.3950044023550577\n",
      "Epoch: 11, iteration: 51, loss: 0.4145745311162049\n",
      "Epoch: 11, iteration: 52, loss: 0.4579728900701367\n",
      "Epoch: 11, iteration: 53, loss: 0.5144640096923807\n",
      "Epoch: 11, iteration: 54, loss: 0.3609701743117076\n",
      "Epoch: 11, iteration: 55, loss: 0.4214814859383175\n",
      "Epoch: 11, iteration: 56, loss: 0.4217586933422551\n",
      "Epoch: 11, iteration: 57, loss: 0.6077149745782693\n",
      "Epoch: 11, iteration: 58, loss: 0.542838114421944\n",
      "Epoch: 11, iteration: 59, loss: 0.37994428405550196\n",
      "Epoch: 11, iteration: 60, loss: 0.34671407266838006\n",
      "Epoch: 11, iteration: 61, loss: 0.515682656017915\n",
      "Epoch: 11, iteration: 62, loss: 0.5790909984365227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, iteration: 63, loss: 0.4827612217861664\n",
      "Epoch: 11, iteration: 64, loss: 0.3816790456322395\n",
      "Epoch: 11, iteration: 65, loss: 0.3865169274482141\n",
      "Epoch: 11, iteration: 66, loss: 0.362991348922292\n",
      "Epoch: 11, iteration: 67, loss: 0.40683332119952953\n",
      "Epoch: 11, iteration: 68, loss: 0.3820373730276395\n",
      "Epoch: 11, iteration: 69, loss: 0.41562569220410667\n",
      "Epoch: 11, iteration: 70, loss: 0.3754015170162292\n",
      "Epoch: 11, iteration: 71, loss: 0.40867556357142626\n",
      "Epoch: 11, iteration: 72, loss: 0.47880078581011565\n",
      "Epoch: 11, iteration: 73, loss: 0.31969254255385193\n",
      "Epoch: 11, iteration: 74, loss: 0.3610156279294381\n",
      "Epoch: 11, iteration: 75, loss: 0.4016128714234519\n",
      "Epoch: 11, iteration: 76, loss: 0.3938368391791922\n",
      "Epoch: 11, iteration: 77, loss: 0.3320007494988235\n",
      "Epoch: 11, iteration: 78, loss: 0.5516679388578497\n",
      "Epoch: 11, iteration: 79, loss: 0.4703366223646764\n",
      "Epoch: 11, iteration: 80, loss: 0.6143731524646642\n",
      "Epoch: 11, iteration: 81, loss: 0.35297462326830303\n",
      "Epoch: 11, iteration: 82, loss: 0.5021563099646152\n",
      "Epoch: 11, iteration: 83, loss: 0.4105887082021414\n",
      "Epoch: 11, iteration: 84, loss: 0.4578133684030884\n",
      "Epoch: 11, iteration: 85, loss: 0.40241070233193843\n",
      "Epoch: 11, iteration: 86, loss: 0.3271575293529973\n",
      "Epoch: 11, iteration: 87, loss: 0.5957792135935368\n",
      "Epoch: 11, iteration: 88, loss: 0.3648987118457697\n",
      "Epoch: 11, iteration: 89, loss: 0.40632337582089767\n",
      "Epoch: 11, iteration: 90, loss: 1.2531091065806004\n",
      "Epoch: 11, iteration: 91, loss: 0.41330030201335544\n",
      "Epoch: 11, iteration: 92, loss: 0.37209809275729594\n",
      "Epoch: 11, iteration: 93, loss: 0.4647681573049169\n",
      "Epoch: 11, iteration: 94, loss: 0.45877437480570377\n",
      "Epoch: 11, iteration: 95, loss: 0.45322470612093957\n",
      "Epoch: 11, iteration: 96, loss: 0.4124869136929869\n",
      "Epoch: 11, iteration: 97, loss: 0.5404604715740322\n",
      "Epoch: 11, iteration: 98, loss: 0.39647974316216994\n",
      "Epoch: 11, iteration: 99, loss: 0.4078301379263977\n",
      "Epoch: 11, iteration: 100, loss: 0.37564509174102767\n",
      "Epoch: 11, iteration: 101, loss: 0.32420697610293264\n",
      "Epoch: 11, iteration: 102, loss: 0.3873820190763068\n",
      "Epoch: 11, iteration: 103, loss: 0.5611701847077307\n",
      "Epoch: 11, iteration: 104, loss: 0.26788513480605136\n",
      "Epoch: 11, iteration: 105, loss: 0.4199045477961074\n",
      "Epoch: 11, iteration: 106, loss: 0.35429708317365216\n",
      "Epoch: 11, iteration: 107, loss: 0.3945891550075331\n",
      "Epoch: 11, iteration: 108, loss: 0.4637112876658221\n",
      "Epoch: 11, iteration: 109, loss: 0.5757056456309352\n",
      "Epoch: 11, iteration: 110, loss: 0.3587792879601599\n",
      "Epoch: 11, iteration: 111, loss: 0.43590502739263454\n",
      "Epoch: 11, iteration: 112, loss: 0.35820364671965677\n",
      "Epoch: 11, iteration: 113, loss: 0.4331773271874211\n",
      "Epoch: 11, iteration: 114, loss: 0.5058560015126919\n",
      "Epoch: 11, iteration: 115, loss: 0.3425105164401209\n",
      "Epoch: 11, iteration: 116, loss: 0.3552841805227986\n",
      "Epoch: 11, iteration: 117, loss: 0.42835917432154147\n",
      "Epoch: 11, iteration: 118, loss: 0.33400074593857815\n",
      "Epoch: 11, iteration: 119, loss: 0.585036903231364\n",
      "Epoch: 11, iteration: 120, loss: 0.36093382796264856\n",
      "Epoch: 11, iteration: 121, loss: 0.3417410654431853\n",
      "Epoch: 11, iteration: 122, loss: 0.3967060203096896\n",
      "Epoch: 11, iteration: 123, loss: 0.32543820509973975\n",
      "Epoch: 11, iteration: 124, loss: 0.5539156614159115\n",
      "Epoch: 11, iteration: 125, loss: 0.30012005211424714\n",
      "Epoch: 11, iteration: 126, loss: 0.3930033288532466\n",
      "Epoch: 11, iteration: 127, loss: 0.7701281017520271\n",
      "Epoch: 11, iteration: 128, loss: 0.40836492376601663\n",
      "Epoch: 11, iteration: 129, loss: 0.31669517993543905\n",
      "Epoch: 11, iteration: 130, loss: 0.3883117494326293\n",
      "Epoch: 11, iteration: 131, loss: 0.7806794987070291\n",
      "Epoch: 11, iteration: 132, loss: 0.46280313986033716\n",
      "Epoch: 11, iteration: 133, loss: 0.36598500237904985\n",
      "Epoch: 11, iteration: 134, loss: 1.0212569789532295\n",
      "Epoch: 11, iteration: 135, loss: 0.467169851693926\n",
      "Epoch: 11, iteration: 136, loss: 0.3472203700585836\n",
      "Epoch: 11, iteration: 137, loss: 0.36476866709461464\n",
      "Epoch: 11, iteration: 138, loss: 0.43040822291434483\n",
      "Epoch: 11, iteration: 139, loss: 0.5104178467198162\n",
      "Epoch: 11, iteration: 140, loss: 0.4175234909702919\n",
      "Epoch: 11, iteration: 141, loss: 0.6304044080580029\n",
      "Epoch: 11, iteration: 142, loss: 0.40727063044247686\n",
      "Epoch: 11, iteration: 143, loss: 0.3684623405056303\n",
      "Epoch: 11, iteration: 144, loss: 0.48769049142036364\n",
      "Epoch: 11, iteration: 145, loss: 0.5208892927989915\n",
      "Epoch: 11, iteration: 146, loss: 0.44233896425474767\n",
      "Epoch: 11, iteration: 147, loss: 0.40828651695091217\n",
      "Epoch: 11, iteration: 148, loss: 0.7159801481010084\n",
      "Epoch: 11, iteration: 149, loss: 0.6099034995308593\n",
      "Epoch: 11, iteration: 150, loss: 0.6136249559564421\n",
      "Epoch: 11, iteration: 151, loss: 0.3948069157199091\n",
      "Epoch: 11, iteration: 152, loss: 0.35559029397982966\n",
      "Epoch: 11, iteration: 153, loss: 0.39845218042527597\n",
      "Epoch: 11, iteration: 154, loss: 0.5377108976343884\n",
      "Epoch: 11, iteration: 155, loss: 0.6138386058946261\n",
      "Epoch: 11, iteration: 156, loss: 0.42070295628276533\n",
      "Epoch: 11, iteration: 157, loss: 0.45480827790352624\n",
      "Epoch: 11, iteration: 158, loss: 0.49344252943808964\n",
      "Epoch: 11, iteration: 159, loss: 0.4903428785190831\n",
      "Epoch: 11, iteration: 160, loss: 0.3735376617244498\n",
      "Epoch: 11, iteration: 161, loss: 0.3666979219560522\n",
      "Epoch: 11, iteration: 162, loss: 0.37630789351094307\n",
      "Epoch: 11, iteration: 163, loss: 0.5545558444336388\n",
      "Epoch: 11, iteration: 164, loss: 0.39082229516141775\n",
      "Epoch: 11, iteration: 165, loss: 0.5300303527019489\n",
      "Epoch: 11, iteration: 166, loss: 0.4484137674420244\n",
      "Epoch: 11, iteration: 167, loss: 0.5508853378777303\n",
      "Epoch: 11, iteration: 168, loss: 0.5453302076353196\n",
      "Epoch: 11, iteration: 169, loss: 0.4253773662772791\n",
      "Epoch: 11, iteration: 170, loss: 0.5533095692567866\n",
      "Epoch: 11, iteration: 171, loss: 0.44638842253338423\n",
      "Epoch: 11, iteration: 172, loss: 0.509215598362735\n",
      "Epoch: 11, iteration: 173, loss: 0.5246893693345344\n",
      "Epoch: 11, iteration: 174, loss: 0.4666841768057375\n",
      "Epoch: 11, iteration: 175, loss: 0.4314467546040907\n",
      "Epoch: 11, iteration: 176, loss: 0.33241253419926187\n",
      "Epoch: 11, iteration: 177, loss: 0.48842632217185433\n",
      "Epoch: 11, iteration: 178, loss: 0.3714008932503315\n",
      "Epoch: 11, iteration: 179, loss: 0.8223956011155992\n",
      "Epoch: 11, iteration: 180, loss: 0.5377597601474984\n",
      "Epoch: 11, iteration: 181, loss: 0.4633257160901602\n",
      "Epoch: 11, iteration: 182, loss: 0.3899012894903571\n",
      "Epoch: 11, iteration: 183, loss: 0.4639358627797775\n",
      "Epoch: 11, iteration: 184, loss: 0.5520424268020689\n",
      "Epoch: 11, iteration: 185, loss: 0.7389461233122919\n",
      "Epoch: 11, iteration: 186, loss: 0.9528831325082707\n",
      "Epoch: 11, iteration: 187, loss: 0.5973507693011358\n",
      "Epoch: 11, iteration: 188, loss: 0.5935423664786201\n",
      "10    0.5085828188 1.0305239960 0.5800377 0.6143779\n",
      "Epoch: 12, iteration: 1, loss: 0.6289257763435283\n",
      "Epoch: 12, iteration: 2, loss: 0.4455832889114595\n",
      "Epoch: 12, iteration: 3, loss: 0.5190736092084143\n",
      "Epoch: 12, iteration: 4, loss: 0.3336815236246953\n",
      "Epoch: 12, iteration: 5, loss: 0.46267001138289277\n",
      "Epoch: 12, iteration: 6, loss: 0.4730707211034772\n",
      "Epoch: 12, iteration: 7, loss: 0.5541306035009583\n",
      "Epoch: 12, iteration: 8, loss: 0.4474364560819645\n",
      "Epoch: 12, iteration: 9, loss: 0.3805672934628228\n",
      "Epoch: 12, iteration: 10, loss: 0.3609462249282126\n",
      "Epoch: 12, iteration: 11, loss: 0.5003822995041036\n",
      "Epoch: 12, iteration: 12, loss: 0.5130685570649045\n",
      "Epoch: 12, iteration: 13, loss: 0.45414594932607616\n",
      "Epoch: 12, iteration: 14, loss: 0.43342692877353217\n",
      "Epoch: 12, iteration: 15, loss: 0.47673810453143706\n",
      "Epoch: 12, iteration: 16, loss: 0.4166920145652291\n",
      "Epoch: 12, iteration: 17, loss: 0.3914505937555509\n",
      "Epoch: 12, iteration: 18, loss: 0.474980539468607\n",
      "Epoch: 12, iteration: 19, loss: 0.38237520797215974\n",
      "Epoch: 12, iteration: 20, loss: 0.5214278257515355\n",
      "Epoch: 12, iteration: 21, loss: 0.319670676885767\n",
      "Epoch: 12, iteration: 22, loss: 0.3245922772530471\n",
      "Epoch: 12, iteration: 23, loss: 0.40566632916934225\n",
      "Epoch: 12, iteration: 24, loss: 0.3700865120159154\n",
      "Epoch: 12, iteration: 25, loss: 0.31224236618945395\n",
      "Epoch: 12, iteration: 26, loss: 0.6013322982410607\n",
      "Epoch: 12, iteration: 27, loss: 0.4142809849633936\n",
      "Epoch: 12, iteration: 28, loss: 0.34640737960980955\n",
      "Epoch: 12, iteration: 29, loss: 0.3784263671006109\n",
      "Epoch: 12, iteration: 30, loss: 0.48896390273779683\n",
      "Epoch: 12, iteration: 31, loss: 0.4558441732080097\n",
      "Epoch: 12, iteration: 32, loss: 0.6338490443411597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, iteration: 33, loss: 0.3461187087584045\n",
      "Epoch: 12, iteration: 34, loss: 0.48654326610283827\n",
      "Epoch: 12, iteration: 35, loss: 0.5685338981214566\n",
      "Epoch: 12, iteration: 36, loss: 0.31864766645735965\n",
      "Epoch: 12, iteration: 37, loss: 0.38068983790264727\n",
      "Epoch: 12, iteration: 38, loss: 0.6357297536053437\n",
      "Epoch: 12, iteration: 39, loss: 0.4057234250867113\n",
      "Epoch: 12, iteration: 40, loss: 0.4103788271573215\n",
      "Epoch: 12, iteration: 41, loss: 0.36453984389796135\n",
      "Epoch: 12, iteration: 42, loss: 0.37384546074827685\n",
      "Epoch: 12, iteration: 43, loss: 0.48427040090906154\n",
      "Epoch: 12, iteration: 44, loss: 0.38444624640914643\n",
      "Epoch: 12, iteration: 45, loss: 0.4146958488676472\n",
      "Epoch: 12, iteration: 46, loss: 0.8355030189062527\n",
      "Epoch: 12, iteration: 47, loss: 0.3400289634656345\n",
      "Epoch: 12, iteration: 48, loss: 0.3710851095943803\n",
      "Epoch: 12, iteration: 49, loss: 0.3947735597620727\n",
      "Epoch: 12, iteration: 50, loss: 0.3699872157293392\n",
      "Epoch: 12, iteration: 51, loss: 0.44395785687883393\n",
      "Epoch: 12, iteration: 52, loss: 0.5131045836905057\n",
      "Epoch: 12, iteration: 53, loss: 0.37008296656785733\n",
      "Epoch: 12, iteration: 54, loss: 0.36647464116030715\n",
      "Epoch: 12, iteration: 55, loss: 0.5315667443695361\n",
      "Epoch: 12, iteration: 56, loss: 0.43496024118248644\n",
      "Epoch: 12, iteration: 57, loss: 0.5038402293299282\n",
      "Epoch: 12, iteration: 58, loss: 0.34370704884415965\n",
      "Epoch: 12, iteration: 59, loss: 0.5011122043306495\n",
      "Epoch: 12, iteration: 60, loss: 0.38332672089872066\n",
      "Epoch: 12, iteration: 61, loss: 0.3245147371851045\n",
      "Epoch: 12, iteration: 62, loss: 0.4977300381180038\n",
      "Epoch: 12, iteration: 63, loss: 0.38779944411264144\n",
      "Epoch: 12, iteration: 64, loss: 0.3497949089473324\n",
      "Epoch: 12, iteration: 65, loss: 0.4213105770640455\n",
      "Epoch: 12, iteration: 66, loss: 0.3362418590105944\n",
      "Epoch: 12, iteration: 67, loss: 0.5017232532539562\n",
      "Epoch: 12, iteration: 68, loss: 0.413614357389275\n",
      "Epoch: 12, iteration: 69, loss: 0.4121022403077834\n",
      "Epoch: 12, iteration: 70, loss: 0.39696330769855065\n",
      "Epoch: 12, iteration: 71, loss: 0.40066783957357777\n",
      "Epoch: 12, iteration: 72, loss: 0.4731371630179786\n",
      "Epoch: 12, iteration: 73, loss: 0.3714771151291555\n",
      "Epoch: 12, iteration: 74, loss: 0.3470044654622398\n",
      "Epoch: 12, iteration: 75, loss: 0.40062945189588595\n",
      "Epoch: 12, iteration: 76, loss: 0.3267339574527188\n",
      "Epoch: 12, iteration: 77, loss: 0.3857165149333468\n",
      "Epoch: 12, iteration: 78, loss: 0.35068389490769736\n",
      "Epoch: 12, iteration: 79, loss: 0.49755084905678876\n",
      "Epoch: 12, iteration: 80, loss: 0.36162170055396653\n",
      "Epoch: 12, iteration: 81, loss: 0.2706701901128148\n",
      "Epoch: 12, iteration: 82, loss: 0.3314695819874847\n",
      "Epoch: 12, iteration: 83, loss: 0.4082772282491936\n",
      "Epoch: 12, iteration: 84, loss: 0.37716170249601194\n",
      "Epoch: 12, iteration: 85, loss: 0.4765022542313785\n",
      "Epoch: 12, iteration: 86, loss: 0.37275249146458755\n",
      "Epoch: 12, iteration: 87, loss: 0.3416751574176624\n",
      "Epoch: 12, iteration: 88, loss: 0.4803889952864426\n",
      "Epoch: 12, iteration: 89, loss: 0.48256283902028835\n",
      "Epoch: 12, iteration: 90, loss: 0.49543114502585417\n",
      "Epoch: 12, iteration: 91, loss: 0.41578264619209787\n",
      "Epoch: 12, iteration: 92, loss: 0.3287494869200828\n",
      "Epoch: 12, iteration: 93, loss: 0.4563216783655605\n",
      "Epoch: 12, iteration: 94, loss: 0.4012556999550693\n",
      "Epoch: 12, iteration: 95, loss: 0.3775688713167885\n",
      "Epoch: 12, iteration: 96, loss: 0.4683769513468711\n",
      "Epoch: 12, iteration: 97, loss: 0.3397221038140716\n",
      "Epoch: 12, iteration: 98, loss: 0.3238957898767234\n",
      "Epoch: 12, iteration: 99, loss: 0.44208704387109654\n",
      "Epoch: 12, iteration: 100, loss: 0.44604401229791263\n",
      "Epoch: 12, iteration: 101, loss: 0.3447659404526425\n",
      "Epoch: 12, iteration: 102, loss: 0.5962932353561995\n",
      "Epoch: 12, iteration: 103, loss: 0.404095067182106\n",
      "Epoch: 12, iteration: 104, loss: 0.38055056190731845\n",
      "Epoch: 12, iteration: 105, loss: 0.3829668767621227\n",
      "Epoch: 12, iteration: 106, loss: 0.38103889801835245\n",
      "Epoch: 12, iteration: 107, loss: 0.40027243455823036\n",
      "Epoch: 12, iteration: 108, loss: 0.3891484035414647\n",
      "Epoch: 12, iteration: 109, loss: 0.3348753322375372\n",
      "Epoch: 12, iteration: 110, loss: 0.3354716139213744\n",
      "Epoch: 12, iteration: 111, loss: 0.36411504520485444\n",
      "Epoch: 12, iteration: 112, loss: 0.43145046566308176\n",
      "Epoch: 12, iteration: 113, loss: 0.3015359648487995\n",
      "Epoch: 12, iteration: 114, loss: 0.36223844032676705\n",
      "Epoch: 12, iteration: 115, loss: 0.3076508825485347\n",
      "Epoch: 12, iteration: 116, loss: 0.42116187102803904\n",
      "Epoch: 12, iteration: 117, loss: 0.35684216116920664\n",
      "Epoch: 12, iteration: 118, loss: 0.2860347842044017\n",
      "Epoch: 12, iteration: 119, loss: 0.7259267491692908\n",
      "Epoch: 12, iteration: 120, loss: 0.3005174096560344\n",
      "Epoch: 12, iteration: 121, loss: 0.3584509725520043\n",
      "Epoch: 12, iteration: 122, loss: 0.2802840472185004\n",
      "Epoch: 12, iteration: 123, loss: 0.29799826195976503\n",
      "Epoch: 12, iteration: 124, loss: 0.3764805276103993\n",
      "Epoch: 12, iteration: 125, loss: 0.3605944019199521\n",
      "Epoch: 12, iteration: 126, loss: 0.5352484742592907\n",
      "Epoch: 12, iteration: 127, loss: 0.3625361551032908\n",
      "Epoch: 12, iteration: 128, loss: 0.30635645596820477\n",
      "Epoch: 12, iteration: 129, loss: 0.36986646127154355\n",
      "Epoch: 12, iteration: 130, loss: 0.36782342900490506\n",
      "Epoch: 12, iteration: 131, loss: 0.2849275134185047\n",
      "Epoch: 12, iteration: 132, loss: 0.3612541491899344\n",
      "Epoch: 12, iteration: 133, loss: 0.4333506999306315\n",
      "Epoch: 12, iteration: 134, loss: 0.4209628951684622\n",
      "Epoch: 12, iteration: 135, loss: 0.3783138423313286\n",
      "Epoch: 12, iteration: 136, loss: 0.5199182036750101\n",
      "Epoch: 12, iteration: 137, loss: 0.29613365074083225\n",
      "Epoch: 12, iteration: 138, loss: 0.5896414021606936\n",
      "Epoch: 12, iteration: 139, loss: 0.2960061412723797\n",
      "Epoch: 12, iteration: 140, loss: 0.41114712450731794\n",
      "Epoch: 12, iteration: 141, loss: 0.4216081035800346\n",
      "Epoch: 12, iteration: 142, loss: 0.30875773360181336\n",
      "Epoch: 12, iteration: 143, loss: 0.4632501810183806\n",
      "Epoch: 12, iteration: 144, loss: 0.4757374277028524\n",
      "Epoch: 12, iteration: 145, loss: 0.45424497601185987\n",
      "Epoch: 12, iteration: 146, loss: 0.4569067957186394\n",
      "Epoch: 12, iteration: 147, loss: 0.37591092512618834\n",
      "Epoch: 12, iteration: 148, loss: 0.5373247873072666\n",
      "Epoch: 12, iteration: 149, loss: 0.3764465932846175\n",
      "Epoch: 12, iteration: 150, loss: 0.36631444302936944\n",
      "Epoch: 12, iteration: 151, loss: 0.5187515332915685\n",
      "Epoch: 12, iteration: 152, loss: 0.34073963601865354\n",
      "Epoch: 12, iteration: 153, loss: 0.3767498204652419\n",
      "Epoch: 12, iteration: 154, loss: 0.5218846227147159\n",
      "Epoch: 12, iteration: 155, loss: 0.4795507563289558\n",
      "Epoch: 12, iteration: 156, loss: 0.3944470172854276\n",
      "Epoch: 12, iteration: 157, loss: 0.4111621557699519\n",
      "Epoch: 12, iteration: 158, loss: 0.40857827169696515\n",
      "Epoch: 12, iteration: 159, loss: 0.3761320108917681\n",
      "Epoch: 12, iteration: 160, loss: 0.477984251577552\n",
      "Epoch: 12, iteration: 161, loss: 0.3710876746015075\n",
      "Epoch: 12, iteration: 162, loss: 0.3532125244086877\n",
      "Epoch: 12, iteration: 163, loss: 0.4508185479423454\n",
      "Epoch: 12, iteration: 164, loss: 0.4009464681780182\n",
      "Epoch: 12, iteration: 165, loss: 0.3731514502848611\n",
      "Epoch: 12, iteration: 166, loss: 0.4404756063856005\n",
      "Epoch: 12, iteration: 167, loss: 0.36262343896173066\n",
      "Epoch: 12, iteration: 168, loss: 0.33787232057150857\n",
      "Epoch: 12, iteration: 169, loss: 0.34047379308738146\n",
      "Epoch: 12, iteration: 170, loss: 0.3096372890227568\n",
      "Epoch: 12, iteration: 171, loss: 0.3289170483663244\n",
      "Epoch: 12, iteration: 172, loss: 0.6982769647557375\n",
      "Epoch: 12, iteration: 173, loss: 0.32627453777996945\n",
      "Epoch: 12, iteration: 174, loss: 0.37617909365959273\n",
      "Epoch: 12, iteration: 175, loss: 0.41354959456241874\n",
      "Epoch: 12, iteration: 176, loss: 0.38450041540076063\n",
      "Epoch: 12, iteration: 177, loss: 0.46982947284537674\n",
      "Epoch: 12, iteration: 178, loss: 0.4101641185767101\n",
      "Epoch: 12, iteration: 179, loss: 0.3373021657226764\n",
      "Epoch: 12, iteration: 180, loss: 0.34039879702297055\n",
      "Epoch: 12, iteration: 181, loss: 0.6417390682535627\n",
      "Epoch: 12, iteration: 182, loss: 0.41355805584688005\n",
      "Epoch: 12, iteration: 183, loss: 0.3829860069259501\n",
      "Epoch: 12, iteration: 184, loss: 0.4500656831471965\n",
      "Epoch: 12, iteration: 185, loss: 0.42903106613924896\n",
      "Epoch: 12, iteration: 186, loss: 0.4227474122949354\n",
      "Epoch: 12, iteration: 187, loss: 0.44212483519615425\n",
      "Epoch: 12, iteration: 188, loss: 0.4514457419830728\n",
      "11    0.3925070466 1.2346450602 0.6403409 0.6494867\n",
      "Epoch: 13, iteration: 1, loss: 0.67972052145391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, iteration: 2, loss: 0.3411848878359181\n",
      "Epoch: 13, iteration: 3, loss: 0.4471518287171624\n",
      "Epoch: 13, iteration: 4, loss: 0.5260819666754226\n",
      "Epoch: 13, iteration: 5, loss: 0.3667303736408688\n",
      "Epoch: 13, iteration: 6, loss: 0.3644002890501109\n",
      "Epoch: 13, iteration: 7, loss: 0.3722851497900423\n",
      "Epoch: 13, iteration: 8, loss: 0.3931185595868661\n",
      "Epoch: 13, iteration: 9, loss: 0.33577987898074535\n",
      "Epoch: 13, iteration: 10, loss: 0.39633049852412633\n",
      "Epoch: 13, iteration: 11, loss: 0.4171963279364564\n",
      "Epoch: 13, iteration: 12, loss: 0.42781527179342443\n",
      "Epoch: 13, iteration: 13, loss: 0.41755530512210287\n",
      "Epoch: 13, iteration: 14, loss: 0.3290515571688311\n",
      "Epoch: 13, iteration: 15, loss: 0.4112774288174217\n",
      "Epoch: 13, iteration: 16, loss: 0.34214097840167956\n",
      "Epoch: 13, iteration: 17, loss: 0.37276983458396346\n",
      "Epoch: 13, iteration: 18, loss: 0.41131100589061637\n",
      "Epoch: 13, iteration: 19, loss: 0.36041965650292496\n",
      "Epoch: 13, iteration: 20, loss: 0.5697128098313716\n",
      "Epoch: 13, iteration: 21, loss: 0.37897386977991876\n",
      "Epoch: 13, iteration: 22, loss: 0.32858955558519537\n",
      "Epoch: 13, iteration: 23, loss: 0.29666550617115567\n",
      "Epoch: 13, iteration: 24, loss: 0.3942274981105373\n",
      "Epoch: 13, iteration: 25, loss: 0.432278589906111\n",
      "Epoch: 13, iteration: 26, loss: 0.32823218701829493\n",
      "Epoch: 13, iteration: 27, loss: 0.35818767613895447\n",
      "Epoch: 13, iteration: 28, loss: 0.3368945249960447\n",
      "Epoch: 13, iteration: 29, loss: 0.5148037373376179\n",
      "Epoch: 13, iteration: 30, loss: 0.6263322236432457\n",
      "Epoch: 13, iteration: 31, loss: 0.2857105148575764\n",
      "Epoch: 13, iteration: 32, loss: 0.30213902102190543\n",
      "Epoch: 13, iteration: 33, loss: 0.3381930128881683\n",
      "Epoch: 13, iteration: 34, loss: 0.4807639129423811\n",
      "Epoch: 13, iteration: 35, loss: 0.5803419791788994\n",
      "Epoch: 13, iteration: 36, loss: 0.36181883618862803\n",
      "Epoch: 13, iteration: 37, loss: 0.4730184927142513\n",
      "Epoch: 13, iteration: 38, loss: 0.32233625006966765\n",
      "Epoch: 13, iteration: 39, loss: 0.34188103819644705\n",
      "Epoch: 13, iteration: 40, loss: 0.4167217619967866\n",
      "Epoch: 13, iteration: 41, loss: 0.36393008166780594\n",
      "Epoch: 13, iteration: 42, loss: 0.47782507650136435\n",
      "Epoch: 13, iteration: 43, loss: 0.3691133642689442\n",
      "Epoch: 13, iteration: 44, loss: 0.3681098627107099\n",
      "Epoch: 13, iteration: 45, loss: 0.33486693210449675\n",
      "Epoch: 13, iteration: 46, loss: 0.3977948899228982\n",
      "Epoch: 13, iteration: 47, loss: 0.46674804299418904\n",
      "Epoch: 13, iteration: 48, loss: 0.41730180375704645\n",
      "Epoch: 13, iteration: 49, loss: 0.3794179006443366\n",
      "Epoch: 13, iteration: 50, loss: 0.36372506984047387\n",
      "Epoch: 13, iteration: 51, loss: 0.49092920555685415\n",
      "Epoch: 13, iteration: 52, loss: 0.6208929794481102\n",
      "Epoch: 13, iteration: 53, loss: 0.3470160289296831\n",
      "Epoch: 13, iteration: 54, loss: 0.3440206548702922\n",
      "Epoch: 13, iteration: 55, loss: 0.34554141488954737\n",
      "Epoch: 13, iteration: 56, loss: 0.48363475671281725\n",
      "Epoch: 13, iteration: 57, loss: 0.35323257586683127\n",
      "Epoch: 13, iteration: 58, loss: 0.37582866878495036\n",
      "Epoch: 13, iteration: 59, loss: 0.7512019834987802\n",
      "Epoch: 13, iteration: 60, loss: 0.4307620729037905\n",
      "Epoch: 13, iteration: 61, loss: 0.31454281043210575\n",
      "Epoch: 13, iteration: 62, loss: 0.5238058554174514\n",
      "Epoch: 13, iteration: 63, loss: 0.46345504913215496\n",
      "Epoch: 13, iteration: 64, loss: 0.47278215705759574\n",
      "Epoch: 13, iteration: 65, loss: 0.50472433898782\n",
      "Epoch: 13, iteration: 66, loss: 0.37287286342326115\n",
      "Epoch: 13, iteration: 67, loss: 0.3915836919293305\n",
      "Epoch: 13, iteration: 68, loss: 0.4246940238324915\n",
      "Epoch: 13, iteration: 69, loss: 0.4982015559518591\n",
      "Epoch: 13, iteration: 70, loss: 0.351123257282393\n",
      "Epoch: 13, iteration: 71, loss: 0.4043919281368136\n",
      "Epoch: 13, iteration: 72, loss: 0.789818084079983\n",
      "Epoch: 13, iteration: 73, loss: 0.38762968791007\n",
      "Epoch: 13, iteration: 74, loss: 0.41373013075605575\n",
      "Epoch: 13, iteration: 75, loss: 0.5966856488593607\n",
      "Epoch: 13, iteration: 76, loss: 0.41945597672522267\n",
      "Epoch: 13, iteration: 77, loss: 0.42290881768860694\n",
      "Epoch: 13, iteration: 78, loss: 0.42473643194537486\n",
      "Epoch: 13, iteration: 79, loss: 0.5097417345210835\n",
      "Epoch: 13, iteration: 80, loss: 0.5101884813804916\n",
      "Epoch: 13, iteration: 81, loss: 0.634674298852639\n",
      "Epoch: 13, iteration: 82, loss: 0.4579253314640248\n",
      "Epoch: 13, iteration: 83, loss: 0.4219606058327388\n",
      "Epoch: 13, iteration: 84, loss: 0.40828900142689734\n",
      "Epoch: 13, iteration: 85, loss: 0.3967499394737496\n",
      "Epoch: 13, iteration: 86, loss: 0.4390841591085429\n",
      "Epoch: 13, iteration: 87, loss: 0.3823791582814553\n",
      "Epoch: 13, iteration: 88, loss: 0.658517569158582\n",
      "Epoch: 13, iteration: 89, loss: 0.5928012399368278\n",
      "Epoch: 13, iteration: 90, loss: 0.5685525250438462\n",
      "Epoch: 13, iteration: 91, loss: 0.4434100318666589\n",
      "Epoch: 13, iteration: 92, loss: 0.34748084653345773\n",
      "Epoch: 13, iteration: 93, loss: 0.508481296549917\n",
      "Epoch: 13, iteration: 94, loss: 0.4773730511860316\n",
      "Epoch: 13, iteration: 95, loss: 0.3414277776893976\n",
      "Epoch: 13, iteration: 96, loss: 0.40418037828249537\n",
      "Epoch: 13, iteration: 97, loss: 0.44470352169902094\n",
      "Epoch: 13, iteration: 98, loss: 0.34825569202041695\n",
      "Epoch: 13, iteration: 99, loss: 0.3730166277430386\n",
      "Epoch: 13, iteration: 100, loss: 0.4480721788201127\n",
      "Epoch: 13, iteration: 101, loss: 0.345470624688543\n",
      "Epoch: 13, iteration: 102, loss: 0.6097340289082112\n",
      "Epoch: 13, iteration: 103, loss: 0.4450593038041251\n",
      "Epoch: 13, iteration: 104, loss: 0.6109768195255566\n",
      "Epoch: 13, iteration: 105, loss: 0.41059414588011345\n",
      "Epoch: 13, iteration: 106, loss: 0.44546123886775885\n",
      "Epoch: 13, iteration: 107, loss: 0.7295978087674115\n",
      "Epoch: 13, iteration: 108, loss: 0.42563556051890145\n",
      "Epoch: 13, iteration: 109, loss: 0.45323741125859823\n",
      "Epoch: 13, iteration: 110, loss: 0.49784159523346755\n",
      "Epoch: 13, iteration: 111, loss: 0.3852643438146705\n",
      "Epoch: 13, iteration: 112, loss: 0.5454342209754763\n",
      "Epoch: 13, iteration: 113, loss: 0.5510676199505862\n",
      "Epoch: 13, iteration: 114, loss: 0.43174406137596183\n",
      "Epoch: 13, iteration: 115, loss: 0.3275232933549415\n",
      "Epoch: 13, iteration: 116, loss: 0.5160642679346362\n",
      "Epoch: 13, iteration: 117, loss: 0.47713121686402554\n",
      "Epoch: 13, iteration: 118, loss: 0.6496451416261189\n",
      "Epoch: 13, iteration: 119, loss: 0.43179983569946256\n",
      "Epoch: 13, iteration: 120, loss: 0.49956881713835055\n",
      "Epoch: 13, iteration: 121, loss: 0.4425024357486307\n",
      "Epoch: 13, iteration: 122, loss: 0.44213925683758987\n",
      "Epoch: 13, iteration: 123, loss: 0.5393316077516015\n",
      "Epoch: 13, iteration: 124, loss: 0.4446895405551563\n",
      "Epoch: 13, iteration: 125, loss: 0.6118759094447648\n",
      "Epoch: 13, iteration: 126, loss: 0.3834111949725294\n",
      "Epoch: 13, iteration: 127, loss: 0.3811256433460211\n",
      "Epoch: 13, iteration: 128, loss: 0.3753123387361473\n",
      "Epoch: 13, iteration: 129, loss: 0.6069083481872924\n",
      "Epoch: 13, iteration: 130, loss: 0.46318524375270126\n",
      "Epoch: 13, iteration: 131, loss: 0.3989252321239275\n",
      "Epoch: 13, iteration: 132, loss: 0.49644188373977055\n",
      "Epoch: 13, iteration: 133, loss: 0.4435785514416811\n",
      "Epoch: 13, iteration: 134, loss: 0.5003709633974858\n",
      "Epoch: 13, iteration: 135, loss: 0.5491507472248639\n",
      "Epoch: 13, iteration: 136, loss: 0.5212681237267551\n",
      "Epoch: 13, iteration: 137, loss: 0.3953545740774912\n",
      "Epoch: 13, iteration: 138, loss: 0.39204430896666553\n",
      "Epoch: 13, iteration: 139, loss: 0.722504454077319\n",
      "Epoch: 13, iteration: 140, loss: 0.6586895580842168\n",
      "Epoch: 13, iteration: 141, loss: 0.34454512159769607\n",
      "Epoch: 13, iteration: 142, loss: 0.3404975251592567\n",
      "Epoch: 13, iteration: 143, loss: 0.5200216584973409\n",
      "Epoch: 13, iteration: 144, loss: 0.39434026529258975\n",
      "Epoch: 13, iteration: 145, loss: 0.39521363173157315\n",
      "Epoch: 13, iteration: 146, loss: 0.3547476771323775\n",
      "Epoch: 13, iteration: 147, loss: 0.9956054490156463\n",
      "Epoch: 13, iteration: 148, loss: 0.3971455853993922\n",
      "Epoch: 13, iteration: 149, loss: 0.3852505726224193\n",
      "Epoch: 13, iteration: 150, loss: 0.37610602713623126\n",
      "Epoch: 13, iteration: 151, loss: 0.3528253251408872\n",
      "Epoch: 13, iteration: 152, loss: 0.37047801897773\n",
      "Epoch: 13, iteration: 153, loss: 0.39633969674505953\n",
      "Epoch: 13, iteration: 154, loss: 0.3478980727735757\n",
      "Epoch: 13, iteration: 155, loss: 0.3629807550869893\n",
      "Epoch: 13, iteration: 156, loss: 0.5482255944467064\n",
      "Epoch: 13, iteration: 157, loss: 0.4855537056251553\n",
      "Epoch: 13, iteration: 158, loss: 0.40702410255847454\n",
      "Epoch: 13, iteration: 159, loss: 0.3811329616518844\n",
      "Epoch: 13, iteration: 160, loss: 0.43114770938751373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, iteration: 161, loss: 0.46704620363838517\n",
      "Epoch: 13, iteration: 162, loss: 0.5342163227605253\n",
      "Epoch: 13, iteration: 163, loss: 0.500316149047743\n",
      "Epoch: 13, iteration: 164, loss: 0.3592774468448972\n",
      "Epoch: 13, iteration: 165, loss: 0.48546827125666075\n",
      "Epoch: 13, iteration: 166, loss: 0.3477728196231283\n",
      "Epoch: 13, iteration: 167, loss: 0.4262372753384704\n",
      "Epoch: 13, iteration: 168, loss: 0.5170744324965674\n",
      "Epoch: 13, iteration: 169, loss: 0.3624190210415852\n",
      "Epoch: 13, iteration: 170, loss: 0.3790250675980418\n",
      "Epoch: 13, iteration: 171, loss: 0.5764370146266515\n",
      "Epoch: 13, iteration: 172, loss: 0.4198156879969038\n",
      "Epoch: 13, iteration: 173, loss: 0.472601741129574\n",
      "Epoch: 13, iteration: 174, loss: 0.3722801750357455\n",
      "Epoch: 13, iteration: 175, loss: 0.35783883106490505\n",
      "Epoch: 13, iteration: 176, loss: 0.4195697793419543\n",
      "Epoch: 13, iteration: 177, loss: 0.3047336275505469\n",
      "Epoch: 13, iteration: 178, loss: 0.3140048907617519\n",
      "Epoch: 13, iteration: 179, loss: 0.43698096806880776\n",
      "Epoch: 13, iteration: 180, loss: 0.44504820968840264\n",
      "Epoch: 13, iteration: 181, loss: 0.38816631402890933\n",
      "Epoch: 13, iteration: 182, loss: 0.3795544163144709\n",
      "Epoch: 13, iteration: 183, loss: 0.4577651013565455\n",
      "Epoch: 13, iteration: 184, loss: 0.27767542874883405\n",
      "Epoch: 13, iteration: 185, loss: 0.45618687045782136\n",
      "Epoch: 13, iteration: 186, loss: 0.3377922084168581\n",
      "Epoch: 13, iteration: 187, loss: 0.3250030128355153\n",
      "Epoch: 13, iteration: 188, loss: 0.48445875053591\n",
      "12    0.3795111710 0.8358402540 0.6119540 0.6254866\n",
      "Epoch: 14, iteration: 1, loss: 0.379895609192505\n",
      "Epoch: 14, iteration: 2, loss: 0.4749741866418809\n",
      "Epoch: 14, iteration: 3, loss: 0.35224879904008866\n",
      "Epoch: 14, iteration: 4, loss: 0.3954705774281249\n",
      "Epoch: 14, iteration: 5, loss: 0.6453307412632875\n",
      "Epoch: 14, iteration: 6, loss: 0.30807025387358555\n",
      "Epoch: 14, iteration: 7, loss: 0.3771600588473192\n",
      "Epoch: 14, iteration: 8, loss: 0.3124567375190531\n",
      "Epoch: 14, iteration: 9, loss: 0.38153085195918124\n",
      "Epoch: 14, iteration: 10, loss: 0.3777729740893815\n",
      "Epoch: 14, iteration: 11, loss: 0.38104273382521975\n",
      "Epoch: 14, iteration: 12, loss: 0.3354485093511594\n",
      "Epoch: 14, iteration: 13, loss: 0.35442182277770007\n",
      "Epoch: 14, iteration: 14, loss: 0.4288718669325181\n",
      "Epoch: 14, iteration: 15, loss: 0.4299881238519196\n",
      "Epoch: 14, iteration: 16, loss: 0.36560734027132263\n",
      "Epoch: 14, iteration: 17, loss: 0.3520087023548899\n",
      "Epoch: 14, iteration: 18, loss: 0.32343614582420377\n",
      "Epoch: 14, iteration: 19, loss: 0.42294357811564276\n",
      "Epoch: 14, iteration: 20, loss: 0.46235830333783673\n",
      "Epoch: 14, iteration: 21, loss: 0.8481619222465911\n",
      "Epoch: 14, iteration: 22, loss: 0.5177064915569375\n",
      "Epoch: 14, iteration: 23, loss: 0.5734727319432219\n",
      "Epoch: 14, iteration: 24, loss: 0.4248405721981985\n",
      "Epoch: 14, iteration: 25, loss: 0.3310749736810505\n",
      "Epoch: 14, iteration: 26, loss: 0.3971901390697149\n",
      "Epoch: 14, iteration: 27, loss: 0.5433097900077871\n",
      "Epoch: 14, iteration: 28, loss: 0.5782135792539679\n",
      "Epoch: 14, iteration: 29, loss: 0.47561633732942254\n",
      "Epoch: 14, iteration: 30, loss: 0.4929251458281178\n",
      "Epoch: 14, iteration: 31, loss: 1.0850738819179289\n",
      "Epoch: 14, iteration: 32, loss: 0.4031111789192661\n",
      "Epoch: 14, iteration: 33, loss: 0.48636246220139334\n",
      "Epoch: 14, iteration: 34, loss: 0.5329913310242854\n",
      "Epoch: 14, iteration: 35, loss: 0.35886161468846667\n",
      "Epoch: 14, iteration: 36, loss: 0.5285992519727486\n",
      "Epoch: 14, iteration: 37, loss: 0.5326211154499123\n",
      "Epoch: 14, iteration: 38, loss: 0.48526636088548386\n",
      "Epoch: 14, iteration: 39, loss: 0.5199265474328292\n",
      "Epoch: 14, iteration: 40, loss: 0.44873996611443845\n",
      "Epoch: 14, iteration: 41, loss: 0.43122572716473767\n",
      "Epoch: 14, iteration: 42, loss: 0.36936990764156646\n",
      "Epoch: 14, iteration: 43, loss: 0.4431053682518283\n",
      "Epoch: 14, iteration: 44, loss: 0.44747559948988425\n",
      "Epoch: 14, iteration: 45, loss: 0.3993349530016246\n",
      "Epoch: 14, iteration: 46, loss: 0.6573205484422645\n",
      "Epoch: 14, iteration: 47, loss: 0.4783372850668425\n",
      "Epoch: 14, iteration: 48, loss: 0.39984729072452374\n",
      "Epoch: 14, iteration: 49, loss: 0.49426553482250485\n",
      "Epoch: 14, iteration: 50, loss: 0.33164482473648027\n",
      "Epoch: 14, iteration: 51, loss: 0.33655533850077773\n",
      "Epoch: 14, iteration: 52, loss: 0.4557492107484676\n",
      "Epoch: 14, iteration: 53, loss: 0.402925398731531\n",
      "Epoch: 14, iteration: 54, loss: 0.39879522391175914\n",
      "Epoch: 14, iteration: 55, loss: 0.3862144340252319\n",
      "Epoch: 14, iteration: 56, loss: 0.40027065514046545\n",
      "Epoch: 14, iteration: 57, loss: 0.477806049055864\n",
      "Epoch: 14, iteration: 58, loss: 0.5840393467094176\n",
      "Epoch: 14, iteration: 59, loss: 0.39462844795118057\n",
      "Epoch: 14, iteration: 60, loss: 0.413055441367282\n",
      "Epoch: 14, iteration: 61, loss: 0.324204952438388\n",
      "Epoch: 14, iteration: 62, loss: 0.4002356839713486\n",
      "Epoch: 14, iteration: 63, loss: 0.3912936944738525\n",
      "Epoch: 14, iteration: 64, loss: 0.3256400100778141\n",
      "Epoch: 14, iteration: 65, loss: 0.3519788861883553\n",
      "Epoch: 14, iteration: 66, loss: 0.3809013780575925\n",
      "Epoch: 14, iteration: 67, loss: 0.5646821437586511\n",
      "Epoch: 14, iteration: 68, loss: 0.4966607079276302\n",
      "Epoch: 14, iteration: 69, loss: 0.3388192475141632\n",
      "Epoch: 14, iteration: 70, loss: 0.3439860026549837\n",
      "Epoch: 14, iteration: 71, loss: 0.31548936589369614\n",
      "Epoch: 14, iteration: 72, loss: 0.3895806327347119\n",
      "Epoch: 14, iteration: 73, loss: 0.3786558614262529\n",
      "Epoch: 14, iteration: 74, loss: 0.29867435430382944\n",
      "Epoch: 14, iteration: 75, loss: 0.4340760321614472\n",
      "Epoch: 14, iteration: 76, loss: 0.3515205743962654\n",
      "Epoch: 14, iteration: 77, loss: 0.3615427697224367\n",
      "Epoch: 14, iteration: 78, loss: 0.3955293669757756\n",
      "Epoch: 14, iteration: 79, loss: 0.26703138815326466\n",
      "Epoch: 14, iteration: 80, loss: 0.36156527705476016\n",
      "Epoch: 14, iteration: 81, loss: 0.4794442018464188\n",
      "Epoch: 14, iteration: 82, loss: 0.4259240474712095\n",
      "Epoch: 14, iteration: 83, loss: 0.34863931068669113\n",
      "Epoch: 14, iteration: 84, loss: 0.375059684284812\n",
      "Epoch: 14, iteration: 85, loss: 0.32552979919950215\n",
      "Epoch: 14, iteration: 86, loss: 0.37792498050247214\n",
      "Epoch: 14, iteration: 87, loss: 0.32288896627027175\n",
      "Epoch: 14, iteration: 88, loss: 0.2754061256056844\n",
      "Epoch: 14, iteration: 89, loss: 0.3464546974360334\n",
      "Epoch: 14, iteration: 90, loss: 0.33086870838283317\n",
      "Epoch: 14, iteration: 91, loss: 0.4052009934674516\n",
      "Epoch: 14, iteration: 92, loss: 0.4407297038357151\n",
      "Epoch: 14, iteration: 93, loss: 0.3321146278140147\n",
      "Epoch: 14, iteration: 94, loss: 0.36480864022009507\n",
      "Epoch: 14, iteration: 95, loss: 0.4852664415233012\n",
      "Epoch: 14, iteration: 96, loss: 0.4163196335243797\n",
      "Epoch: 14, iteration: 97, loss: 0.48736330510340786\n",
      "Epoch: 14, iteration: 98, loss: 0.3254834864444983\n",
      "Epoch: 14, iteration: 99, loss: 0.4124486297620089\n",
      "Epoch: 14, iteration: 100, loss: 0.40228185278424194\n",
      "Epoch: 14, iteration: 101, loss: 0.34451062344376865\n",
      "Epoch: 14, iteration: 102, loss: 0.4940493275571053\n",
      "Epoch: 14, iteration: 103, loss: 0.4782308723444277\n",
      "Epoch: 14, iteration: 104, loss: 0.3322744250180193\n",
      "Epoch: 14, iteration: 105, loss: 0.403928146534264\n",
      "Epoch: 14, iteration: 106, loss: 0.4175882652118909\n",
      "Epoch: 14, iteration: 107, loss: 0.4607430407669279\n",
      "Epoch: 14, iteration: 108, loss: 0.5256102092589067\n",
      "Epoch: 14, iteration: 109, loss: 0.2908163350287952\n",
      "Epoch: 14, iteration: 110, loss: 0.42314555920634483\n",
      "Epoch: 14, iteration: 111, loss: 0.6967993757443373\n",
      "Epoch: 14, iteration: 112, loss: 0.45757487327964697\n",
      "Epoch: 14, iteration: 113, loss: 0.35993132763114655\n",
      "Epoch: 14, iteration: 114, loss: 0.3350135697117315\n",
      "Epoch: 14, iteration: 115, loss: 0.5302666983524625\n",
      "Epoch: 14, iteration: 116, loss: 0.3659440884135765\n",
      "Epoch: 14, iteration: 117, loss: 0.47482824552215797\n",
      "Epoch: 14, iteration: 118, loss: 0.3970905932118646\n",
      "Epoch: 14, iteration: 119, loss: 0.30692823753766674\n",
      "Epoch: 14, iteration: 120, loss: 0.5424232362808271\n",
      "Epoch: 14, iteration: 121, loss: 0.35205622633990413\n",
      "Epoch: 14, iteration: 122, loss: 0.41422596628169456\n",
      "Epoch: 14, iteration: 123, loss: 0.42745945976428507\n",
      "Epoch: 14, iteration: 124, loss: 0.36606774225030947\n",
      "Epoch: 14, iteration: 125, loss: 0.4740022500089853\n",
      "Epoch: 14, iteration: 126, loss: 0.3609155427873951\n",
      "Epoch: 14, iteration: 127, loss: 0.5193742382288922\n",
      "Epoch: 14, iteration: 128, loss: 0.44042382999100116\n",
      "Epoch: 14, iteration: 129, loss: 0.4909258979254039\n",
      "Epoch: 14, iteration: 130, loss: 0.38266506221104324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, iteration: 131, loss: 0.3734836691334959\n",
      "Epoch: 14, iteration: 132, loss: 0.3648374325191062\n",
      "Epoch: 14, iteration: 133, loss: 0.30798569928137864\n",
      "Epoch: 14, iteration: 134, loss: 0.2892945574155244\n",
      "Epoch: 14, iteration: 135, loss: 0.3581669306007006\n",
      "Epoch: 14, iteration: 136, loss: 0.3721328318592509\n",
      "Epoch: 14, iteration: 137, loss: 0.32737645431310997\n",
      "Epoch: 14, iteration: 138, loss: 0.3355514261302839\n",
      "Epoch: 14, iteration: 139, loss: 0.34053367002206314\n",
      "Epoch: 14, iteration: 140, loss: 0.33578044085514863\n",
      "Epoch: 14, iteration: 141, loss: 0.41492909426558067\n",
      "Epoch: 14, iteration: 142, loss: 0.3102980231078768\n",
      "Epoch: 14, iteration: 143, loss: 0.2955898372349141\n",
      "Epoch: 14, iteration: 144, loss: 0.4760138473654194\n",
      "Epoch: 14, iteration: 145, loss: 0.3479932620879668\n",
      "Epoch: 14, iteration: 146, loss: 0.28291804921574104\n",
      "Epoch: 14, iteration: 147, loss: 0.31672002118975334\n",
      "Epoch: 14, iteration: 148, loss: 0.3626995738439159\n",
      "Epoch: 14, iteration: 149, loss: 0.41067366173871767\n",
      "Epoch: 14, iteration: 150, loss: 0.34871029696999173\n",
      "Epoch: 14, iteration: 151, loss: 0.3298642070558475\n",
      "Epoch: 14, iteration: 152, loss: 0.3903307224957926\n",
      "Epoch: 14, iteration: 153, loss: 0.33524029307157716\n",
      "Epoch: 14, iteration: 154, loss: 0.29452710501796764\n",
      "Epoch: 14, iteration: 155, loss: 0.3550899561016425\n",
      "Epoch: 14, iteration: 156, loss: 0.3539127476469199\n",
      "Epoch: 14, iteration: 157, loss: 0.3652371140909419\n",
      "Epoch: 14, iteration: 158, loss: 0.5461370379173219\n",
      "Epoch: 14, iteration: 159, loss: 0.28609295321453176\n",
      "Epoch: 14, iteration: 160, loss: 0.35891758350441966\n",
      "Epoch: 14, iteration: 161, loss: 0.30832408257752614\n",
      "Epoch: 14, iteration: 162, loss: 0.45398370726678644\n",
      "Epoch: 14, iteration: 163, loss: 0.268772598793142\n",
      "Epoch: 14, iteration: 164, loss: 0.3186208369465857\n",
      "Epoch: 14, iteration: 165, loss: 0.2819539859325833\n",
      "Epoch: 14, iteration: 166, loss: 0.36179385114093515\n",
      "Epoch: 14, iteration: 167, loss: 0.3222373220799997\n",
      "Epoch: 14, iteration: 168, loss: 0.8018361134546097\n",
      "Epoch: 14, iteration: 169, loss: 0.29083595933066647\n",
      "Epoch: 14, iteration: 170, loss: 0.4037304225715179\n",
      "Epoch: 14, iteration: 171, loss: 0.3952116642770285\n",
      "Epoch: 14, iteration: 172, loss: 0.4159540700541024\n",
      "Epoch: 14, iteration: 173, loss: 0.32867470772484386\n",
      "Epoch: 14, iteration: 174, loss: 0.3695929912146553\n",
      "Epoch: 14, iteration: 175, loss: 1.017690616492906\n",
      "Epoch: 14, iteration: 176, loss: 0.43307194088718626\n",
      "Epoch: 14, iteration: 177, loss: 0.344711613654351\n",
      "Epoch: 14, iteration: 178, loss: 0.31533813359935164\n",
      "Epoch: 14, iteration: 179, loss: 0.5398960464660348\n",
      "Epoch: 14, iteration: 180, loss: 0.45928502290376666\n",
      "Epoch: 14, iteration: 181, loss: 0.3203261111790701\n",
      "Epoch: 14, iteration: 182, loss: 0.4339709148740165\n",
      "Epoch: 14, iteration: 183, loss: 0.4792394745220674\n",
      "Epoch: 14, iteration: 184, loss: 0.3422228786662309\n",
      "Epoch: 14, iteration: 185, loss: 0.387949659478033\n",
      "Epoch: 14, iteration: 186, loss: 0.39459533067594116\n",
      "Epoch: 14, iteration: 187, loss: 0.4402172314703237\n",
      "Epoch: 14, iteration: 188, loss: 0.4285158607723485\n",
      "13    0.3662667683 0.9639103776 0.6318632 0.6282520\n",
      "Epoch: 15, iteration: 1, loss: 0.3000338923337954\n",
      "Epoch: 15, iteration: 2, loss: 0.34671725567416434\n",
      "Epoch: 15, iteration: 3, loss: 0.265489956381918\n",
      "Epoch: 15, iteration: 4, loss: 0.49295622103434766\n",
      "Epoch: 15, iteration: 5, loss: 0.44634195129493476\n",
      "Epoch: 15, iteration: 6, loss: 0.31250327821148527\n",
      "Epoch: 15, iteration: 7, loss: 0.3879670704180009\n",
      "Epoch: 15, iteration: 8, loss: 0.288323239433274\n",
      "Epoch: 15, iteration: 9, loss: 0.2825207269926207\n",
      "Epoch: 15, iteration: 10, loss: 0.42765530353125786\n",
      "Epoch: 15, iteration: 11, loss: 0.3499495884205198\n",
      "Epoch: 15, iteration: 12, loss: 0.3324955919523203\n",
      "Epoch: 15, iteration: 13, loss: 0.29695527225840945\n",
      "Epoch: 15, iteration: 14, loss: 0.9476969889404306\n",
      "Epoch: 15, iteration: 15, loss: 0.3158292378656025\n",
      "Epoch: 15, iteration: 16, loss: 0.3787869650065255\n",
      "Epoch: 15, iteration: 17, loss: 0.30432508971992944\n",
      "Epoch: 15, iteration: 18, loss: 0.3211648384025589\n",
      "Epoch: 15, iteration: 19, loss: 0.36289829201456675\n",
      "Epoch: 15, iteration: 20, loss: 0.39432027795666114\n",
      "Epoch: 15, iteration: 21, loss: 0.337048846322205\n",
      "Epoch: 15, iteration: 22, loss: 0.42037436861517335\n",
      "Epoch: 15, iteration: 23, loss: 0.3160424206118652\n",
      "Epoch: 15, iteration: 24, loss: 0.3087365704074704\n",
      "Epoch: 15, iteration: 25, loss: 0.36108276235706793\n",
      "Epoch: 15, iteration: 26, loss: 0.3347078925514346\n",
      "Epoch: 15, iteration: 27, loss: 0.30667401057688726\n",
      "Epoch: 15, iteration: 28, loss: 0.32715529330410964\n",
      "Epoch: 15, iteration: 29, loss: 0.3516917408533563\n",
      "Epoch: 15, iteration: 30, loss: 0.49886093937556003\n",
      "Epoch: 15, iteration: 31, loss: 0.30266315188831344\n",
      "Epoch: 15, iteration: 32, loss: 0.3019067684793649\n",
      "Epoch: 15, iteration: 33, loss: 0.43232939787528074\n",
      "Epoch: 15, iteration: 34, loss: 0.39989205897462454\n",
      "Epoch: 15, iteration: 35, loss: 0.3556694184977053\n",
      "Epoch: 15, iteration: 36, loss: 0.4725135091456991\n",
      "Epoch: 15, iteration: 37, loss: 0.4417234355716403\n",
      "Epoch: 15, iteration: 38, loss: 0.4018349746825533\n",
      "Epoch: 15, iteration: 39, loss: 0.30206037066805574\n",
      "Epoch: 15, iteration: 40, loss: 0.4103638220531203\n",
      "Epoch: 15, iteration: 41, loss: 0.42934111100622474\n",
      "Epoch: 15, iteration: 42, loss: 0.33932733308814417\n",
      "Epoch: 15, iteration: 43, loss: 0.4271280373258155\n",
      "Epoch: 15, iteration: 44, loss: 0.30220163493845265\n",
      "Epoch: 15, iteration: 45, loss: 0.6873236542474553\n",
      "Epoch: 15, iteration: 46, loss: 0.3593315614957041\n",
      "Epoch: 15, iteration: 47, loss: 0.2873243925337087\n",
      "Epoch: 15, iteration: 48, loss: 0.3878943121258675\n",
      "Epoch: 15, iteration: 49, loss: 0.4512246856465507\n",
      "Epoch: 15, iteration: 50, loss: 0.5080487452870383\n",
      "Epoch: 15, iteration: 51, loss: 0.39657397478170714\n",
      "Epoch: 15, iteration: 52, loss: 0.4393423462248192\n",
      "Epoch: 15, iteration: 53, loss: 0.3362380190713666\n",
      "Epoch: 15, iteration: 54, loss: 0.3268559521124344\n",
      "Epoch: 15, iteration: 55, loss: 0.52763953915327\n",
      "Epoch: 15, iteration: 56, loss: 0.3427297898323237\n",
      "Epoch: 15, iteration: 57, loss: 0.34840327996451614\n",
      "Epoch: 15, iteration: 58, loss: 0.43396958121840656\n",
      "Epoch: 15, iteration: 59, loss: 0.3531104418693793\n",
      "Epoch: 15, iteration: 60, loss: 0.3626228998748949\n",
      "Epoch: 15, iteration: 61, loss: 0.3340216442001797\n",
      "Epoch: 15, iteration: 62, loss: 0.4301110673106887\n",
      "Epoch: 15, iteration: 63, loss: 0.38553833786210634\n",
      "Epoch: 15, iteration: 64, loss: 0.7905573192856905\n",
      "Epoch: 15, iteration: 65, loss: 0.5297642390099762\n",
      "Epoch: 15, iteration: 66, loss: 0.41764866115424043\n",
      "Epoch: 15, iteration: 67, loss: 0.25653215364839826\n",
      "Epoch: 15, iteration: 68, loss: 1.1738208795917877\n",
      "Epoch: 15, iteration: 69, loss: 0.30565037968511233\n",
      "Epoch: 15, iteration: 70, loss: 0.35929617796806235\n",
      "Epoch: 15, iteration: 71, loss: 0.42345357632858793\n",
      "Epoch: 15, iteration: 72, loss: 0.6429552197994438\n",
      "Epoch: 15, iteration: 73, loss: 0.4081460452042687\n",
      "Epoch: 15, iteration: 74, loss: 0.5022754078525457\n",
      "Epoch: 15, iteration: 75, loss: 0.4107473813465455\n",
      "Epoch: 15, iteration: 76, loss: 0.812112908363744\n",
      "Epoch: 15, iteration: 77, loss: 0.4918229851233856\n",
      "Epoch: 15, iteration: 78, loss: 0.3406455290707228\n",
      "Epoch: 15, iteration: 79, loss: 0.34555982767207377\n",
      "Epoch: 15, iteration: 80, loss: 0.3806487973596535\n",
      "Epoch: 15, iteration: 81, loss: 0.42403549187308787\n",
      "Epoch: 15, iteration: 82, loss: 0.3547069261752058\n",
      "Epoch: 15, iteration: 83, loss: 0.425948790630155\n",
      "Epoch: 15, iteration: 84, loss: 0.3694498673392497\n",
      "Epoch: 15, iteration: 85, loss: 0.4048600733688485\n",
      "Epoch: 15, iteration: 86, loss: 0.6610420602892396\n",
      "Epoch: 15, iteration: 87, loss: 0.3665182846424926\n",
      "Epoch: 15, iteration: 88, loss: 0.3675151614825247\n",
      "Epoch: 15, iteration: 89, loss: 0.6505352062965022\n",
      "Epoch: 15, iteration: 90, loss: 0.39144693672421627\n",
      "Epoch: 15, iteration: 91, loss: 0.3489056727556637\n",
      "Epoch: 15, iteration: 92, loss: 0.3588195345859964\n",
      "Epoch: 15, iteration: 93, loss: 0.32506244351356267\n",
      "Epoch: 15, iteration: 94, loss: 0.48375043233101217\n",
      "Epoch: 15, iteration: 95, loss: 0.4186583222702609\n",
      "Epoch: 15, iteration: 96, loss: 0.3687545765403209\n",
      "Epoch: 15, iteration: 97, loss: 0.5796069089600279\n",
      "Epoch: 15, iteration: 98, loss: 0.34160767853071045\n",
      "Epoch: 15, iteration: 99, loss: 0.319808684114858\n",
      "Epoch: 15, iteration: 100, loss: 0.3498380406275324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, iteration: 101, loss: 0.4067599072425921\n",
      "Epoch: 15, iteration: 102, loss: 0.3218813284686763\n",
      "Epoch: 15, iteration: 103, loss: 0.3260876173982122\n",
      "Epoch: 15, iteration: 104, loss: 0.35999743811131635\n",
      "Epoch: 15, iteration: 105, loss: 0.42224691866459585\n",
      "Epoch: 15, iteration: 106, loss: 0.35965630712332614\n",
      "Epoch: 15, iteration: 107, loss: 0.3227296514634471\n",
      "Epoch: 15, iteration: 108, loss: 0.39350377906099954\n",
      "Epoch: 15, iteration: 109, loss: 0.3642442478088419\n",
      "Epoch: 15, iteration: 110, loss: 0.30513368587485323\n",
      "Epoch: 15, iteration: 111, loss: 0.39360928608899104\n",
      "Epoch: 15, iteration: 112, loss: 0.7392036775702328\n",
      "Epoch: 15, iteration: 113, loss: 0.3112213894868761\n",
      "Epoch: 15, iteration: 114, loss: 0.3839961427539263\n",
      "Epoch: 15, iteration: 115, loss: 0.3725672082522472\n",
      "Epoch: 15, iteration: 116, loss: 0.3555839306362508\n",
      "Epoch: 15, iteration: 117, loss: 0.3632025049769505\n",
      "Epoch: 15, iteration: 118, loss: 0.35675376657618346\n",
      "Epoch: 15, iteration: 119, loss: 0.5994489259400893\n",
      "Epoch: 15, iteration: 120, loss: 0.42160610059703635\n",
      "Epoch: 15, iteration: 121, loss: 0.39609896563824026\n",
      "Epoch: 15, iteration: 122, loss: 0.39385267171491634\n",
      "Epoch: 15, iteration: 123, loss: 0.4741920359557413\n",
      "Epoch: 15, iteration: 124, loss: 0.839878116299765\n",
      "Epoch: 15, iteration: 125, loss: 0.42483797224171155\n",
      "Epoch: 15, iteration: 126, loss: 0.39049371054028564\n",
      "Epoch: 15, iteration: 127, loss: 0.366540935079557\n",
      "Epoch: 15, iteration: 128, loss: 0.3521132238680056\n",
      "Epoch: 15, iteration: 129, loss: 0.3243464993663379\n",
      "Epoch: 15, iteration: 130, loss: 0.43717238193447494\n",
      "Epoch: 15, iteration: 131, loss: 0.3518600752383419\n",
      "Epoch: 15, iteration: 132, loss: 0.35939945401496776\n",
      "Epoch: 15, iteration: 133, loss: 0.36270167345069904\n",
      "Epoch: 15, iteration: 134, loss: 0.3873152707559772\n",
      "Epoch: 15, iteration: 135, loss: 0.34065217435693296\n",
      "Epoch: 15, iteration: 136, loss: 0.35478655133542475\n",
      "Epoch: 15, iteration: 137, loss: 0.39998987780654954\n",
      "Epoch: 15, iteration: 138, loss: 0.3933300795510793\n",
      "Epoch: 15, iteration: 139, loss: 0.33491191574607765\n",
      "Epoch: 15, iteration: 140, loss: 0.3809028516121234\n",
      "Epoch: 15, iteration: 141, loss: 0.39210004985139635\n",
      "Epoch: 15, iteration: 142, loss: 0.5261133096930988\n",
      "Epoch: 15, iteration: 143, loss: 0.3948374939797213\n",
      "Epoch: 15, iteration: 144, loss: 0.4071129639321854\n",
      "Epoch: 15, iteration: 145, loss: 0.43690303283122867\n",
      "Epoch: 15, iteration: 146, loss: 0.38288055138286625\n",
      "Epoch: 15, iteration: 147, loss: 0.44207077038730197\n",
      "Epoch: 15, iteration: 148, loss: 0.33123706033715256\n",
      "Epoch: 15, iteration: 149, loss: 0.49905104181217635\n",
      "Epoch: 15, iteration: 150, loss: 0.34885060430051823\n",
      "Epoch: 15, iteration: 151, loss: 0.3258140236811422\n",
      "Epoch: 15, iteration: 152, loss: 0.42274056400655446\n",
      "Epoch: 15, iteration: 153, loss: 0.47913784821443606\n",
      "Epoch: 15, iteration: 154, loss: 0.2896642639995734\n",
      "Epoch: 15, iteration: 155, loss: 0.4502636215302592\n",
      "Epoch: 15, iteration: 156, loss: 0.35339548797923426\n",
      "Epoch: 15, iteration: 157, loss: 0.44160158530180366\n",
      "Epoch: 15, iteration: 158, loss: 0.43808164459373244\n",
      "Epoch: 15, iteration: 159, loss: 0.522777990590144\n",
      "Epoch: 15, iteration: 160, loss: 0.3131156656030935\n",
      "Epoch: 15, iteration: 161, loss: 0.3391479104517953\n",
      "Epoch: 15, iteration: 162, loss: 0.3314183833493449\n",
      "Epoch: 15, iteration: 163, loss: 0.35790926780222215\n",
      "Epoch: 15, iteration: 164, loss: 0.9167987910541358\n",
      "Epoch: 15, iteration: 165, loss: 0.3622592806233707\n",
      "Epoch: 15, iteration: 166, loss: 0.30480872873744796\n",
      "Epoch: 15, iteration: 167, loss: 0.5096513449638129\n",
      "Epoch: 15, iteration: 168, loss: 0.35516809151138007\n",
      "Epoch: 15, iteration: 169, loss: 0.4511648386087226\n",
      "Epoch: 15, iteration: 170, loss: 0.33419884273314854\n",
      "Epoch: 15, iteration: 171, loss: 0.3799659524920533\n",
      "Epoch: 15, iteration: 172, loss: 0.39350900665564054\n",
      "Epoch: 15, iteration: 173, loss: 0.24708451971936737\n",
      "Epoch: 15, iteration: 174, loss: 0.6250963762386559\n",
      "Epoch: 15, iteration: 175, loss: 0.37195838062718356\n",
      "Epoch: 15, iteration: 176, loss: 0.34148846847331177\n",
      "Epoch: 15, iteration: 177, loss: 0.3241580706706031\n",
      "Epoch: 15, iteration: 178, loss: 0.33515475123236327\n",
      "Epoch: 15, iteration: 179, loss: 0.4616848319059474\n",
      "Epoch: 15, iteration: 180, loss: 0.4668464997397204\n",
      "Epoch: 15, iteration: 181, loss: 0.30599969972493796\n",
      "Epoch: 15, iteration: 182, loss: 0.2550857751371951\n",
      "Epoch: 15, iteration: 183, loss: 0.44254761963404077\n",
      "Epoch: 15, iteration: 184, loss: 0.31078777637862215\n",
      "Epoch: 15, iteration: 185, loss: 0.38029507377531835\n",
      "Epoch: 15, iteration: 186, loss: 0.27725126230176705\n",
      "Epoch: 15, iteration: 187, loss: 0.3547073454079975\n",
      "Epoch: 15, iteration: 188, loss: 0.3586907133962641\n",
      "14    0.3307261086 1.0634382203 0.6566366 0.6614270\n",
      "Epoch: 16, iteration: 1, loss: 0.3552399898648688\n",
      "Epoch: 16, iteration: 2, loss: 0.2650362566921216\n",
      "Epoch: 16, iteration: 3, loss: 0.28289676925727864\n",
      "Epoch: 16, iteration: 4, loss: 0.5063705673892873\n",
      "Epoch: 16, iteration: 5, loss: 0.2894423051744681\n",
      "Epoch: 16, iteration: 6, loss: 0.2810205044928216\n",
      "Epoch: 16, iteration: 7, loss: 0.3497583296721528\n",
      "Epoch: 16, iteration: 8, loss: 0.3190308043360805\n",
      "Epoch: 16, iteration: 9, loss: 0.28563866193961457\n",
      "Epoch: 16, iteration: 10, loss: 0.36144946920899906\n",
      "Epoch: 16, iteration: 11, loss: 0.32075435419682685\n",
      "Epoch: 16, iteration: 12, loss: 0.3562103606864594\n",
      "Epoch: 16, iteration: 13, loss: 0.5039794631132294\n",
      "Epoch: 16, iteration: 14, loss: 0.36623280364438227\n",
      "Epoch: 16, iteration: 15, loss: 0.6822826042434236\n",
      "Epoch: 16, iteration: 16, loss: 0.259563621189503\n",
      "Epoch: 16, iteration: 17, loss: 0.5520410869442793\n",
      "Epoch: 16, iteration: 18, loss: 0.3569193361010839\n",
      "Epoch: 16, iteration: 19, loss: 0.3026906938718502\n",
      "Epoch: 16, iteration: 20, loss: 0.31064322545449224\n",
      "Epoch: 16, iteration: 21, loss: 0.36703935389897957\n",
      "Epoch: 16, iteration: 22, loss: 0.2793748904958408\n",
      "Epoch: 16, iteration: 23, loss: 0.3026536902540397\n",
      "Epoch: 16, iteration: 24, loss: 0.2870034948185018\n",
      "Epoch: 16, iteration: 25, loss: 0.3032421951995887\n",
      "Epoch: 16, iteration: 26, loss: 0.3013079923719374\n",
      "Epoch: 16, iteration: 27, loss: 0.5421622268028068\n",
      "Epoch: 16, iteration: 28, loss: 0.41359040330054936\n",
      "Epoch: 16, iteration: 29, loss: 0.38821147679092616\n",
      "Epoch: 16, iteration: 30, loss: 0.36868334690048205\n",
      "Epoch: 16, iteration: 31, loss: 0.538394670773574\n",
      "Epoch: 16, iteration: 32, loss: 0.2883940006652819\n",
      "Epoch: 16, iteration: 33, loss: 0.39452208932398053\n",
      "Epoch: 16, iteration: 34, loss: 0.5121711806671848\n",
      "Epoch: 16, iteration: 35, loss: 0.37235100845528807\n",
      "Epoch: 16, iteration: 36, loss: 0.29952866588441596\n",
      "Epoch: 16, iteration: 37, loss: 0.3705218642692227\n",
      "Epoch: 16, iteration: 38, loss: 0.3744130790697356\n",
      "Epoch: 16, iteration: 39, loss: 0.545669253032996\n",
      "Epoch: 16, iteration: 40, loss: 0.30670710016127406\n",
      "Epoch: 16, iteration: 41, loss: 0.2758662484295069\n",
      "Epoch: 16, iteration: 42, loss: 0.36110804229460325\n",
      "Epoch: 16, iteration: 43, loss: 0.3703512143854905\n",
      "Epoch: 16, iteration: 44, loss: 0.3699891029285987\n",
      "Epoch: 16, iteration: 45, loss: 0.41850571534819087\n",
      "Epoch: 16, iteration: 46, loss: 0.429030671986895\n",
      "Epoch: 16, iteration: 47, loss: 0.2713820000972511\n",
      "Epoch: 16, iteration: 48, loss: 0.463942212077593\n",
      "Epoch: 16, iteration: 49, loss: 0.31530762997344497\n",
      "Epoch: 16, iteration: 50, loss: 0.39909387757229614\n",
      "Epoch: 16, iteration: 51, loss: 0.31423139371960895\n",
      "Epoch: 16, iteration: 52, loss: 0.28602176572183213\n",
      "Epoch: 16, iteration: 53, loss: 0.3941781611675422\n",
      "Epoch: 16, iteration: 54, loss: 0.357740571794375\n",
      "Epoch: 16, iteration: 55, loss: 0.3809645847452826\n",
      "Epoch: 16, iteration: 56, loss: 0.3256436228694871\n",
      "Epoch: 16, iteration: 57, loss: 0.24111665608769747\n",
      "Epoch: 16, iteration: 58, loss: 0.2888050051818273\n",
      "Epoch: 16, iteration: 59, loss: 0.4140593782695265\n",
      "Epoch: 16, iteration: 60, loss: 0.46217932355120744\n",
      "Epoch: 16, iteration: 61, loss: 0.2779458927764456\n",
      "Epoch: 16, iteration: 62, loss: 0.3044354550697362\n",
      "Epoch: 16, iteration: 63, loss: 0.2820611028493961\n",
      "Epoch: 16, iteration: 64, loss: 0.3850584061776867\n",
      "Epoch: 16, iteration: 65, loss: 0.4199404946790633\n",
      "Epoch: 16, iteration: 66, loss: 0.32034149203528023\n",
      "Epoch: 16, iteration: 67, loss: 0.2998099439682313\n",
      "Epoch: 16, iteration: 68, loss: 0.36172394336355224\n",
      "Epoch: 16, iteration: 69, loss: 0.25298855732410025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, iteration: 70, loss: 0.48886734505017976\n",
      "Epoch: 16, iteration: 71, loss: 0.59332736466499\n",
      "Epoch: 16, iteration: 72, loss: 0.2879349827476208\n",
      "Epoch: 16, iteration: 73, loss: 0.3885101959578515\n",
      "Epoch: 16, iteration: 74, loss: 0.35962700482225246\n",
      "Epoch: 16, iteration: 75, loss: 0.3470101862098495\n",
      "Epoch: 16, iteration: 76, loss: 0.3262662282610753\n",
      "Epoch: 16, iteration: 77, loss: 0.42415562189111133\n",
      "Epoch: 16, iteration: 78, loss: 0.3507870487089078\n",
      "Epoch: 16, iteration: 79, loss: 0.2918974147203241\n",
      "Epoch: 16, iteration: 80, loss: 0.2868521841405965\n",
      "Epoch: 16, iteration: 81, loss: 0.41629367967024156\n",
      "Epoch: 16, iteration: 82, loss: 0.4113364926466134\n",
      "Epoch: 16, iteration: 83, loss: 0.3701015817763555\n",
      "Epoch: 16, iteration: 84, loss: 0.512212587039103\n",
      "Epoch: 16, iteration: 85, loss: 0.30469304574849476\n",
      "Epoch: 16, iteration: 86, loss: 0.27968128300364964\n",
      "Epoch: 16, iteration: 87, loss: 0.35227983006337493\n",
      "Epoch: 16, iteration: 88, loss: 0.3966204213940837\n",
      "Epoch: 16, iteration: 89, loss: 0.3076778819000991\n",
      "Epoch: 16, iteration: 90, loss: 0.4079711699276741\n",
      "Epoch: 16, iteration: 91, loss: 0.27913907217080314\n",
      "Epoch: 16, iteration: 92, loss: 0.3774667580732521\n",
      "Epoch: 16, iteration: 93, loss: 0.38039646369905894\n",
      "Epoch: 16, iteration: 94, loss: 0.6639018095214163\n",
      "Epoch: 16, iteration: 95, loss: 0.6353261906253295\n",
      "Epoch: 16, iteration: 96, loss: 0.36829627395121306\n",
      "Epoch: 16, iteration: 97, loss: 0.30671095848039054\n",
      "Epoch: 16, iteration: 98, loss: 0.3387753419125779\n",
      "Epoch: 16, iteration: 99, loss: 0.3067205849465892\n",
      "Epoch: 16, iteration: 100, loss: 0.7388926585363413\n",
      "Epoch: 16, iteration: 101, loss: 0.3500085250820734\n",
      "Epoch: 16, iteration: 102, loss: 0.26552491512410875\n",
      "Epoch: 16, iteration: 103, loss: 0.40124138240489227\n",
      "Epoch: 16, iteration: 104, loss: 0.41134709360802324\n",
      "Epoch: 16, iteration: 105, loss: 0.5109730016944101\n",
      "Epoch: 16, iteration: 106, loss: 0.4244287147585628\n",
      "Epoch: 16, iteration: 107, loss: 0.31491638867621097\n",
      "Epoch: 16, iteration: 108, loss: 0.31178821350472985\n",
      "Epoch: 16, iteration: 109, loss: 0.35501068277235864\n",
      "Epoch: 16, iteration: 110, loss: 0.25558429195082144\n",
      "Epoch: 16, iteration: 111, loss: 0.37624979173100614\n",
      "Epoch: 16, iteration: 112, loss: 0.3351874912300993\n",
      "Epoch: 16, iteration: 113, loss: 0.7383184675190817\n",
      "Epoch: 16, iteration: 114, loss: 0.36306260640122145\n",
      "Epoch: 16, iteration: 115, loss: 0.35290044299524603\n",
      "Epoch: 16, iteration: 116, loss: 0.31666867213439226\n",
      "Epoch: 16, iteration: 117, loss: 0.45125451111358433\n",
      "Epoch: 16, iteration: 118, loss: 0.29086487496521807\n",
      "Epoch: 16, iteration: 119, loss: 0.30731086449562234\n",
      "Epoch: 16, iteration: 120, loss: 0.326581301066422\n",
      "Epoch: 16, iteration: 121, loss: 0.3521795812615079\n",
      "Epoch: 16, iteration: 122, loss: 0.33475787059491213\n",
      "Epoch: 16, iteration: 123, loss: 0.41280171070947375\n",
      "Epoch: 16, iteration: 124, loss: 0.38932618776849476\n",
      "Epoch: 16, iteration: 125, loss: 0.3813527336838875\n",
      "Epoch: 16, iteration: 126, loss: 0.26344050436583133\n",
      "Epoch: 16, iteration: 127, loss: 0.37267371236169794\n",
      "Epoch: 16, iteration: 128, loss: 0.5994249795077169\n",
      "Epoch: 16, iteration: 129, loss: 0.3735847299806423\n",
      "Epoch: 16, iteration: 130, loss: 0.30799870183087763\n",
      "Epoch: 16, iteration: 131, loss: 0.3469630481390048\n",
      "Epoch: 16, iteration: 132, loss: 0.35408674561947345\n",
      "Epoch: 16, iteration: 133, loss: 0.2883167916382928\n",
      "Epoch: 16, iteration: 134, loss: 0.3905379734417603\n",
      "Epoch: 16, iteration: 135, loss: 0.31964104379926367\n",
      "Epoch: 16, iteration: 136, loss: 0.3616598190873198\n",
      "Epoch: 16, iteration: 137, loss: 0.4804481644899804\n",
      "Epoch: 16, iteration: 138, loss: 0.4099696003952743\n",
      "Epoch: 16, iteration: 139, loss: 0.3815800046506693\n",
      "Epoch: 16, iteration: 140, loss: 0.3010231467430349\n",
      "Epoch: 16, iteration: 141, loss: 0.5122828006098461\n",
      "Epoch: 16, iteration: 142, loss: 0.42501140056658554\n",
      "Epoch: 16, iteration: 143, loss: 0.3352680057761019\n",
      "Epoch: 16, iteration: 144, loss: 0.4153441620098629\n",
      "Epoch: 16, iteration: 145, loss: 0.3225763669810462\n",
      "Epoch: 16, iteration: 146, loss: 0.2945510969366671\n",
      "Epoch: 16, iteration: 147, loss: 0.2690682365191474\n",
      "Epoch: 16, iteration: 148, loss: 0.32660003557490547\n",
      "Epoch: 16, iteration: 149, loss: 0.6415571832959266\n",
      "Epoch: 16, iteration: 150, loss: 0.40449425603757805\n",
      "Epoch: 16, iteration: 151, loss: 0.2754238490440345\n",
      "Epoch: 16, iteration: 152, loss: 0.2984571418581835\n",
      "Epoch: 16, iteration: 153, loss: 0.5817932565254468\n",
      "Epoch: 16, iteration: 154, loss: 0.4727936076524947\n",
      "Epoch: 16, iteration: 155, loss: 0.35715013412093977\n",
      "Epoch: 16, iteration: 156, loss: 0.3486184440436343\n",
      "Epoch: 16, iteration: 157, loss: 0.28206670648262777\n",
      "Epoch: 16, iteration: 158, loss: 0.30360821600041316\n",
      "Epoch: 16, iteration: 159, loss: 0.3482547516194066\n",
      "Epoch: 16, iteration: 160, loss: 0.3992767053211755\n",
      "Epoch: 16, iteration: 161, loss: 0.6766475695923856\n",
      "Epoch: 16, iteration: 162, loss: 0.34872715313941793\n",
      "Epoch: 16, iteration: 163, loss: 0.5399848435237221\n",
      "Epoch: 16, iteration: 164, loss: 0.3804561761143146\n",
      "Epoch: 16, iteration: 165, loss: 0.342168171427757\n",
      "Epoch: 16, iteration: 166, loss: 0.3054727957958095\n",
      "Epoch: 16, iteration: 167, loss: 0.3633474177227164\n",
      "Epoch: 16, iteration: 168, loss: 0.36262900666249437\n",
      "Epoch: 16, iteration: 169, loss: 0.3341829461193902\n",
      "Epoch: 16, iteration: 170, loss: 0.4139909073034281\n",
      "Epoch: 16, iteration: 171, loss: 0.36587729427470467\n",
      "Epoch: 16, iteration: 172, loss: 0.5876019458161094\n",
      "Epoch: 16, iteration: 173, loss: 0.4920399004437686\n",
      "Epoch: 16, iteration: 174, loss: 0.32252458974935544\n",
      "Epoch: 16, iteration: 175, loss: 0.36879836555432416\n",
      "Epoch: 16, iteration: 176, loss: 0.4638578145737513\n",
      "Epoch: 16, iteration: 177, loss: 0.3458570984663673\n",
      "Epoch: 16, iteration: 178, loss: 0.39211155040324236\n",
      "Epoch: 16, iteration: 179, loss: 0.5797037718642887\n",
      "Epoch: 16, iteration: 180, loss: 0.39478318468710416\n",
      "Epoch: 16, iteration: 181, loss: 0.3019076197233769\n",
      "Epoch: 16, iteration: 182, loss: 0.3924757548288245\n",
      "Epoch: 16, iteration: 183, loss: 0.3138899930691832\n",
      "Epoch: 16, iteration: 184, loss: 0.35997873241571127\n",
      "Epoch: 16, iteration: 185, loss: 0.45232949343499806\n",
      "Epoch: 16, iteration: 186, loss: 0.3315290296866535\n",
      "Epoch: 16, iteration: 187, loss: 0.36388985847205074\n",
      "Epoch: 16, iteration: 188, loss: 0.3947958879436972\n",
      "15    0.3592129942 1.1895010983 0.6439029 0.6455438\n",
      "Epoch: 17, iteration: 1, loss: 0.442473405623498\n",
      "Epoch: 17, iteration: 2, loss: 0.3010857759970891\n",
      "Epoch: 17, iteration: 3, loss: 0.37896927291587207\n",
      "Epoch: 17, iteration: 4, loss: 0.2827525289801538\n",
      "Epoch: 17, iteration: 5, loss: 0.3299827900883073\n",
      "Epoch: 17, iteration: 6, loss: 0.30418850899315797\n",
      "Epoch: 17, iteration: 7, loss: 0.3503477874650858\n",
      "Epoch: 17, iteration: 8, loss: 0.4082690716801915\n",
      "Epoch: 17, iteration: 9, loss: 0.3318055648054612\n",
      "Epoch: 17, iteration: 10, loss: 0.32282582074635396\n",
      "Epoch: 17, iteration: 11, loss: 0.43099645809696924\n",
      "Epoch: 17, iteration: 12, loss: 0.458498460409011\n",
      "Epoch: 17, iteration: 13, loss: 0.313796341164591\n",
      "Epoch: 17, iteration: 14, loss: 0.29910738420728566\n",
      "Epoch: 17, iteration: 15, loss: 0.3314589247539523\n",
      "Epoch: 17, iteration: 16, loss: 0.49330237364397717\n",
      "Epoch: 17, iteration: 17, loss: 0.38193551746925974\n",
      "Epoch: 17, iteration: 18, loss: 0.34423989480699047\n",
      "Epoch: 17, iteration: 19, loss: 0.29918418981717215\n",
      "Epoch: 17, iteration: 20, loss: 0.3077411034945102\n",
      "Epoch: 17, iteration: 21, loss: 0.4149764940958733\n",
      "Epoch: 17, iteration: 22, loss: 0.2921057281429173\n",
      "Epoch: 17, iteration: 23, loss: 0.5803999130652991\n",
      "Epoch: 17, iteration: 24, loss: 0.37703133092119245\n",
      "Epoch: 17, iteration: 25, loss: 0.3008733686940107\n",
      "Epoch: 17, iteration: 26, loss: 0.3397358504949272\n",
      "Epoch: 17, iteration: 27, loss: 0.29667131715010153\n",
      "Epoch: 17, iteration: 28, loss: 0.3812224564752723\n",
      "Epoch: 17, iteration: 29, loss: 0.31162094039616295\n",
      "Epoch: 17, iteration: 30, loss: 0.4290858003509661\n",
      "Epoch: 17, iteration: 31, loss: 0.3318342702661006\n",
      "Epoch: 17, iteration: 32, loss: 0.3250621955986798\n",
      "Epoch: 17, iteration: 33, loss: 0.4978253179616985\n",
      "Epoch: 17, iteration: 34, loss: 0.32734346243454826\n",
      "Epoch: 17, iteration: 35, loss: 0.38110073391507304\n",
      "Epoch: 17, iteration: 36, loss: 0.49950521860129127\n",
      "Epoch: 17, iteration: 37, loss: 0.5818543852373675\n",
      "Epoch: 17, iteration: 38, loss: 0.3258809147326081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, iteration: 39, loss: 0.255254936352361\n",
      "Epoch: 17, iteration: 40, loss: 0.2861643031985179\n",
      "Epoch: 17, iteration: 41, loss: 0.37337944963958825\n",
      "Epoch: 17, iteration: 42, loss: 0.46582073673552604\n",
      "Epoch: 17, iteration: 43, loss: 0.2933631318917494\n",
      "Epoch: 17, iteration: 44, loss: 0.7470749079586105\n",
      "Epoch: 17, iteration: 45, loss: 0.44437882040208454\n",
      "Epoch: 17, iteration: 46, loss: 0.3271812842734015\n",
      "Epoch: 17, iteration: 47, loss: 0.28552355568253773\n",
      "Epoch: 17, iteration: 48, loss: 0.3126682788627091\n",
      "Epoch: 17, iteration: 49, loss: 0.3943230416689608\n",
      "Epoch: 17, iteration: 50, loss: 0.329544602167257\n",
      "Epoch: 17, iteration: 51, loss: 0.3657450146474447\n",
      "Epoch: 17, iteration: 52, loss: 0.3450015609523653\n",
      "Epoch: 17, iteration: 53, loss: 0.299602979310783\n",
      "Epoch: 17, iteration: 54, loss: 0.30172758098377617\n",
      "Epoch: 17, iteration: 55, loss: 0.33428610776550166\n",
      "Epoch: 17, iteration: 56, loss: 0.3596093991856193\n",
      "Epoch: 17, iteration: 57, loss: 0.27516636816996315\n",
      "Epoch: 17, iteration: 58, loss: 0.4094149315663278\n",
      "Epoch: 17, iteration: 59, loss: 0.6263551048779459\n",
      "Epoch: 17, iteration: 60, loss: 0.3222108574648503\n",
      "Epoch: 17, iteration: 61, loss: 0.33058223531028225\n",
      "Epoch: 17, iteration: 62, loss: 0.4063994412936557\n",
      "Epoch: 17, iteration: 63, loss: 0.31902742558361014\n",
      "Epoch: 17, iteration: 64, loss: 0.28404809161455535\n",
      "Epoch: 17, iteration: 65, loss: 0.37393993982051876\n",
      "Epoch: 17, iteration: 66, loss: 0.3237357304850051\n",
      "Epoch: 17, iteration: 67, loss: 0.2974000870722367\n",
      "Epoch: 17, iteration: 68, loss: 0.36930200482605724\n",
      "Epoch: 17, iteration: 69, loss: 0.3546296626294796\n",
      "Epoch: 17, iteration: 70, loss: 0.34348540503236535\n",
      "Epoch: 17, iteration: 71, loss: 0.3558322095485125\n",
      "Epoch: 17, iteration: 72, loss: 0.36212541641228707\n",
      "Epoch: 17, iteration: 73, loss: 0.3134849087768748\n",
      "Epoch: 17, iteration: 74, loss: 0.42503656076664936\n",
      "Epoch: 17, iteration: 75, loss: 0.30781222923883883\n",
      "Epoch: 17, iteration: 76, loss: 0.31564891228677405\n",
      "Epoch: 17, iteration: 77, loss: 0.3046822463436181\n",
      "Epoch: 17, iteration: 78, loss: 0.4776827944512904\n",
      "Epoch: 17, iteration: 79, loss: 0.3037350743615822\n",
      "Epoch: 17, iteration: 80, loss: 0.2876227150438775\n",
      "Epoch: 17, iteration: 81, loss: 0.3409062380821949\n",
      "Epoch: 17, iteration: 82, loss: 0.33483617928073767\n",
      "Epoch: 17, iteration: 83, loss: 0.34702512851296513\n",
      "Epoch: 17, iteration: 84, loss: 0.3136612182065505\n",
      "Epoch: 17, iteration: 85, loss: 0.2657959463987497\n",
      "Epoch: 17, iteration: 86, loss: 0.3086552270998757\n",
      "Epoch: 17, iteration: 87, loss: 0.32161235880941347\n",
      "Epoch: 17, iteration: 88, loss: 0.49636796760279783\n",
      "Epoch: 17, iteration: 89, loss: 0.3477017237054501\n",
      "Epoch: 17, iteration: 90, loss: 0.38313695262397024\n",
      "Epoch: 17, iteration: 91, loss: 0.2943945464693826\n",
      "Epoch: 17, iteration: 92, loss: 0.3458768984793835\n",
      "Epoch: 17, iteration: 93, loss: 0.32961000695953596\n",
      "Epoch: 17, iteration: 94, loss: 0.38310705650244975\n",
      "Epoch: 17, iteration: 95, loss: 0.27232578536025714\n",
      "Epoch: 17, iteration: 96, loss: 0.26198227684447023\n",
      "Epoch: 17, iteration: 97, loss: 0.33309795544649473\n",
      "Epoch: 17, iteration: 98, loss: 0.253793228931261\n",
      "Epoch: 17, iteration: 99, loss: 0.3304849490422096\n",
      "Epoch: 17, iteration: 100, loss: 0.2983890495912288\n",
      "Epoch: 17, iteration: 101, loss: 0.3131008064760565\n",
      "Epoch: 17, iteration: 102, loss: 0.301956188187455\n",
      "Epoch: 17, iteration: 103, loss: 0.277429042887788\n",
      "Epoch: 17, iteration: 104, loss: 0.33760265839564557\n",
      "Epoch: 17, iteration: 105, loss: 0.4321501081394762\n",
      "Epoch: 17, iteration: 106, loss: 0.5139914638498152\n",
      "Epoch: 17, iteration: 107, loss: 0.39417314867132874\n",
      "Epoch: 17, iteration: 108, loss: 0.5275299614699486\n",
      "Epoch: 17, iteration: 109, loss: 0.4192187082811429\n",
      "Epoch: 17, iteration: 110, loss: 0.3202495283965943\n",
      "Epoch: 17, iteration: 111, loss: 0.36955418487802205\n",
      "Epoch: 17, iteration: 112, loss: 0.591662746296429\n",
      "Epoch: 17, iteration: 113, loss: 0.32341341725799017\n",
      "Epoch: 17, iteration: 114, loss: 0.3277146606455781\n",
      "Epoch: 17, iteration: 115, loss: 0.32519339893626054\n",
      "Epoch: 17, iteration: 116, loss: 0.3943378512140499\n",
      "Epoch: 17, iteration: 117, loss: 0.37144188323324945\n",
      "Epoch: 17, iteration: 118, loss: 0.387958494864866\n",
      "Epoch: 17, iteration: 119, loss: 0.4298021802617665\n",
      "Epoch: 17, iteration: 120, loss: 0.3552732822318141\n",
      "Epoch: 17, iteration: 121, loss: 0.2906852781322787\n",
      "Epoch: 17, iteration: 122, loss: 0.2906285983943364\n",
      "Epoch: 17, iteration: 123, loss: 0.432290652540575\n",
      "Epoch: 17, iteration: 124, loss: 0.3956413987658861\n",
      "Epoch: 17, iteration: 125, loss: 0.29193728347747927\n",
      "Epoch: 17, iteration: 126, loss: 0.3984292910720062\n",
      "Epoch: 17, iteration: 127, loss: 0.41132013691668945\n",
      "Epoch: 17, iteration: 128, loss: 0.4165531261074418\n",
      "Epoch: 17, iteration: 129, loss: 0.31421549928850345\n",
      "Epoch: 17, iteration: 130, loss: 0.3392504708184391\n",
      "Epoch: 17, iteration: 131, loss: 0.3814873725710304\n",
      "Epoch: 17, iteration: 132, loss: 0.4976774696138843\n",
      "Epoch: 17, iteration: 133, loss: 0.47755027280779966\n",
      "Epoch: 17, iteration: 134, loss: 0.3246162756307543\n",
      "Epoch: 17, iteration: 135, loss: 0.38831370167418255\n",
      "Epoch: 17, iteration: 136, loss: 0.48546402639469793\n",
      "Epoch: 17, iteration: 137, loss: 0.26290295104719164\n",
      "Epoch: 17, iteration: 138, loss: 0.47271711573967157\n",
      "Epoch: 17, iteration: 139, loss: 0.3166838898503022\n",
      "Epoch: 17, iteration: 140, loss: 0.34806247052457934\n",
      "Epoch: 17, iteration: 141, loss: 0.3793371447299311\n",
      "Epoch: 17, iteration: 142, loss: 0.28962712519104944\n",
      "Epoch: 17, iteration: 143, loss: 0.42443885759839156\n",
      "Epoch: 17, iteration: 144, loss: 0.2828131848279351\n",
      "Epoch: 17, iteration: 145, loss: 0.31248423031832273\n",
      "Epoch: 17, iteration: 146, loss: 0.31705864792583965\n",
      "Epoch: 17, iteration: 147, loss: 0.30551216908223294\n",
      "Epoch: 17, iteration: 148, loss: 0.3501918077657674\n",
      "Epoch: 17, iteration: 149, loss: 0.39969947057574895\n",
      "Epoch: 17, iteration: 150, loss: 0.35584426997030383\n",
      "Epoch: 17, iteration: 151, loss: 0.28878336596735393\n",
      "Epoch: 17, iteration: 152, loss: 0.24678054460382073\n",
      "Epoch: 17, iteration: 153, loss: 0.5470187797299099\n",
      "Epoch: 17, iteration: 154, loss: 0.3181139455935153\n",
      "Epoch: 17, iteration: 155, loss: 0.31578469678536547\n",
      "Epoch: 17, iteration: 156, loss: 0.3468267994055717\n",
      "Epoch: 17, iteration: 157, loss: 0.3150191947956853\n",
      "Epoch: 17, iteration: 158, loss: 0.33052171980364004\n",
      "Epoch: 17, iteration: 159, loss: 0.26147521100161303\n",
      "Epoch: 17, iteration: 160, loss: 0.5812432018739018\n",
      "Epoch: 17, iteration: 161, loss: 0.35622240636306846\n",
      "Epoch: 17, iteration: 162, loss: 0.40428775110048315\n",
      "Epoch: 17, iteration: 163, loss: 0.381734930874478\n",
      "Epoch: 17, iteration: 164, loss: 0.353805744988946\n",
      "Epoch: 17, iteration: 165, loss: 0.3490168647023564\n",
      "Epoch: 17, iteration: 166, loss: 0.36404716992600283\n",
      "Epoch: 17, iteration: 167, loss: 0.2880555999101842\n",
      "Epoch: 17, iteration: 168, loss: 0.31687080730553185\n",
      "Epoch: 17, iteration: 169, loss: 0.2469413933638468\n",
      "Epoch: 17, iteration: 170, loss: 0.32751798596437504\n",
      "Epoch: 17, iteration: 171, loss: 0.32982240288656195\n",
      "Epoch: 17, iteration: 172, loss: 0.28684511503445886\n",
      "Epoch: 17, iteration: 173, loss: 0.25343679511434475\n",
      "Epoch: 17, iteration: 174, loss: 0.36646423072154044\n",
      "Epoch: 17, iteration: 175, loss: 0.48808088720057374\n",
      "Epoch: 17, iteration: 176, loss: 0.30382357302367247\n",
      "Epoch: 17, iteration: 177, loss: 0.4146437205916219\n",
      "Epoch: 17, iteration: 178, loss: 0.39334346411494564\n",
      "Epoch: 17, iteration: 179, loss: 0.2835808528494478\n",
      "Epoch: 17, iteration: 180, loss: 0.3948340771662517\n",
      "Epoch: 17, iteration: 181, loss: 0.24137550406031927\n",
      "Epoch: 17, iteration: 182, loss: 0.3563724071713342\n",
      "Epoch: 17, iteration: 183, loss: 0.3463474486956109\n",
      "Epoch: 17, iteration: 184, loss: 0.25625369253183\n",
      "Epoch: 17, iteration: 185, loss: 0.35652363278931243\n",
      "Epoch: 17, iteration: 186, loss: 0.3440986845783483\n",
      "Epoch: 17, iteration: 187, loss: 0.3120381702294281\n",
      "Epoch: 17, iteration: 188, loss: 0.4812409616352732\n",
      "16    0.3165564834 1.2113630723 0.6545364 0.6564700\n",
      "Epoch: 18, iteration: 1, loss: 0.30077119467532976\n",
      "Epoch: 18, iteration: 2, loss: 0.2842987967837671\n",
      "Epoch: 18, iteration: 3, loss: 0.3209000548531442\n",
      "Epoch: 18, iteration: 4, loss: 0.33664893279491037\n",
      "Epoch: 18, iteration: 5, loss: 0.2905593545589288\n",
      "Epoch: 18, iteration: 6, loss: 0.3325239890982619\n",
      "Epoch: 18, iteration: 7, loss: 0.313228648473858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, iteration: 8, loss: 0.2407768338189768\n",
      "Epoch: 18, iteration: 9, loss: 0.36403424107959653\n",
      "Epoch: 18, iteration: 10, loss: 0.37833666980274316\n",
      "Epoch: 18, iteration: 11, loss: 0.3398395893850863\n",
      "Epoch: 18, iteration: 12, loss: 0.3283273647340267\n",
      "Epoch: 18, iteration: 13, loss: 0.3781197059988364\n",
      "Epoch: 18, iteration: 14, loss: 0.33602350659708624\n",
      "Epoch: 18, iteration: 15, loss: 0.331931462747754\n",
      "Epoch: 18, iteration: 16, loss: 0.3925361005139226\n",
      "Epoch: 18, iteration: 17, loss: 0.2674638906873642\n",
      "Epoch: 18, iteration: 18, loss: 0.3024989869323129\n",
      "Epoch: 18, iteration: 19, loss: 0.27651967515152226\n",
      "Epoch: 18, iteration: 20, loss: 0.2939068402913423\n",
      "Epoch: 18, iteration: 21, loss: 0.2874253057113892\n",
      "Epoch: 18, iteration: 22, loss: 0.24563726929863022\n",
      "Epoch: 18, iteration: 23, loss: 0.32727183939689314\n",
      "Epoch: 18, iteration: 24, loss: 0.2895979227987354\n",
      "Epoch: 18, iteration: 25, loss: 0.3150143834510763\n",
      "Epoch: 18, iteration: 26, loss: 0.425420599822478\n",
      "Epoch: 18, iteration: 27, loss: 0.3774621347966728\n",
      "Epoch: 18, iteration: 28, loss: 0.3218069270903967\n",
      "Epoch: 18, iteration: 29, loss: 0.2709021875184151\n",
      "Epoch: 18, iteration: 30, loss: 0.38482391757309903\n",
      "Epoch: 18, iteration: 31, loss: 0.31177188410937584\n",
      "Epoch: 18, iteration: 32, loss: 0.2589300660083769\n",
      "Epoch: 18, iteration: 33, loss: 0.25445566283714893\n",
      "Epoch: 18, iteration: 34, loss: 0.27596130793210644\n",
      "Epoch: 18, iteration: 35, loss: 0.3161877467294618\n",
      "Epoch: 18, iteration: 36, loss: 0.36974919101410664\n",
      "Epoch: 18, iteration: 37, loss: 0.2145602614699216\n",
      "Epoch: 18, iteration: 38, loss: 0.26951004109707777\n",
      "Epoch: 18, iteration: 39, loss: 0.7161608995942662\n",
      "Epoch: 18, iteration: 40, loss: 0.3982338339751751\n",
      "Epoch: 18, iteration: 41, loss: 0.2752940950267506\n",
      "Epoch: 18, iteration: 42, loss: 0.28920936361979377\n",
      "Epoch: 18, iteration: 43, loss: 0.305460543667447\n",
      "Epoch: 18, iteration: 44, loss: 0.2235561914508235\n",
      "Epoch: 18, iteration: 45, loss: 0.31152539183720696\n",
      "Epoch: 18, iteration: 46, loss: 0.3406406362469355\n",
      "Epoch: 18, iteration: 47, loss: 0.29361399196476806\n",
      "Epoch: 18, iteration: 48, loss: 0.40480906486013957\n",
      "Epoch: 18, iteration: 49, loss: 0.3241987187751978\n",
      "Epoch: 18, iteration: 50, loss: 0.2688027221736951\n",
      "Epoch: 18, iteration: 51, loss: 0.31202570019662657\n",
      "Epoch: 18, iteration: 52, loss: 0.37276798121887034\n",
      "Epoch: 18, iteration: 53, loss: 0.36375922437662705\n",
      "Epoch: 18, iteration: 54, loss: 0.27886365010743686\n",
      "Epoch: 18, iteration: 55, loss: 0.32546701288723473\n",
      "Epoch: 18, iteration: 56, loss: 0.3514111451000055\n",
      "Epoch: 18, iteration: 57, loss: 0.3151126354612495\n",
      "Epoch: 18, iteration: 58, loss: 0.3625989629431338\n",
      "Epoch: 18, iteration: 59, loss: 0.34712785120551287\n",
      "Epoch: 18, iteration: 60, loss: 0.2929075808594654\n",
      "Epoch: 18, iteration: 61, loss: 0.28343273234685057\n",
      "Epoch: 18, iteration: 62, loss: 0.294855990523955\n",
      "Epoch: 18, iteration: 63, loss: 0.285814291698787\n",
      "Epoch: 18, iteration: 64, loss: 0.3406632164635014\n",
      "Epoch: 18, iteration: 65, loss: 0.2779421329109443\n",
      "Epoch: 18, iteration: 66, loss: 0.25553458449969646\n",
      "Epoch: 18, iteration: 67, loss: 0.39995512854195714\n",
      "Epoch: 18, iteration: 68, loss: 0.5541661216983449\n",
      "Epoch: 18, iteration: 69, loss: 0.280140556034751\n",
      "Epoch: 18, iteration: 70, loss: 0.3262951416685784\n",
      "Epoch: 18, iteration: 71, loss: 0.23787736976860605\n",
      "Epoch: 18, iteration: 72, loss: 0.3188291072121914\n",
      "Epoch: 18, iteration: 73, loss: 0.3431928773287438\n",
      "Epoch: 18, iteration: 74, loss: 0.3887686007381103\n",
      "Epoch: 18, iteration: 75, loss: 0.2794997360135909\n",
      "Epoch: 18, iteration: 76, loss: 0.2606763280298124\n",
      "Epoch: 18, iteration: 77, loss: 0.3008610185690007\n",
      "Epoch: 18, iteration: 78, loss: 0.38358437274200513\n",
      "Epoch: 18, iteration: 79, loss: 0.3532720857266239\n",
      "Epoch: 18, iteration: 80, loss: 0.32808461270146333\n",
      "Epoch: 18, iteration: 81, loss: 0.2667520936704665\n",
      "Epoch: 18, iteration: 82, loss: 0.2694527674595937\n",
      "Epoch: 18, iteration: 83, loss: 0.5083693471508344\n",
      "Epoch: 18, iteration: 84, loss: 0.46339943964277786\n",
      "Epoch: 18, iteration: 85, loss: 0.34945090492639425\n",
      "Epoch: 18, iteration: 86, loss: 0.28534085654839336\n",
      "Epoch: 18, iteration: 87, loss: 0.32000440754416376\n",
      "Epoch: 18, iteration: 88, loss: 0.3323709887420172\n",
      "Epoch: 18, iteration: 89, loss: 0.34394531050823474\n",
      "Epoch: 18, iteration: 90, loss: 0.31438162084292437\n",
      "Epoch: 18, iteration: 91, loss: 0.5197118715628277\n",
      "Epoch: 18, iteration: 92, loss: 0.36750538453724346\n",
      "Epoch: 18, iteration: 93, loss: 0.26917267486304686\n",
      "Epoch: 18, iteration: 94, loss: 0.30566780944470745\n",
      "Epoch: 18, iteration: 95, loss: 0.29004970238696864\n",
      "Epoch: 18, iteration: 96, loss: 0.30230290295134843\n",
      "Epoch: 18, iteration: 97, loss: 0.2862969769032671\n",
      "Epoch: 18, iteration: 98, loss: 0.35092337221541975\n",
      "Epoch: 18, iteration: 99, loss: 0.33062208652257336\n",
      "Epoch: 18, iteration: 100, loss: 0.29818648365474687\n",
      "Epoch: 18, iteration: 101, loss: 0.34419261126936107\n",
      "Epoch: 18, iteration: 102, loss: 0.30552947821837\n",
      "Epoch: 18, iteration: 103, loss: 0.3988335691036906\n",
      "Epoch: 18, iteration: 104, loss: 0.2912621140458529\n",
      "Epoch: 18, iteration: 105, loss: 0.3123123308142518\n",
      "Epoch: 18, iteration: 106, loss: 0.3226509596819131\n",
      "Epoch: 18, iteration: 107, loss: 0.3756356935633071\n",
      "Epoch: 18, iteration: 108, loss: 0.6350095679726897\n",
      "Epoch: 18, iteration: 109, loss: 0.2678258501631229\n",
      "Epoch: 18, iteration: 110, loss: 0.35287483119894353\n",
      "Epoch: 18, iteration: 111, loss: 0.27880061904336517\n",
      "Epoch: 18, iteration: 112, loss: 0.2793839955880555\n",
      "Epoch: 18, iteration: 113, loss: 0.3027470714866242\n",
      "Epoch: 18, iteration: 114, loss: 0.30947345629092987\n",
      "Epoch: 18, iteration: 115, loss: 0.3437263175345164\n",
      "Epoch: 18, iteration: 116, loss: 0.23148942155280855\n",
      "Epoch: 18, iteration: 117, loss: 0.33197646550503795\n",
      "Epoch: 18, iteration: 118, loss: 0.32533559114644756\n",
      "Epoch: 18, iteration: 119, loss: 0.33087640862633416\n",
      "Epoch: 18, iteration: 120, loss: 0.2874125369960967\n",
      "Epoch: 18, iteration: 121, loss: 0.3403694920530631\n",
      "Epoch: 18, iteration: 122, loss: 0.22564890066783264\n",
      "Epoch: 18, iteration: 123, loss: 0.26120570090049905\n",
      "Epoch: 18, iteration: 124, loss: 0.3219575795734743\n",
      "Epoch: 18, iteration: 125, loss: 0.4392481702275921\n",
      "Epoch: 18, iteration: 126, loss: 0.25541469831823493\n",
      "Epoch: 18, iteration: 127, loss: 0.2989264129932538\n",
      "Epoch: 18, iteration: 128, loss: 0.31391607673707994\n",
      "Epoch: 18, iteration: 129, loss: 0.287005010654888\n",
      "Epoch: 18, iteration: 130, loss: 0.3727513365571587\n",
      "Epoch: 18, iteration: 131, loss: 0.31480838756841395\n",
      "Epoch: 18, iteration: 132, loss: 0.2852271247995829\n",
      "Epoch: 18, iteration: 133, loss: 0.4006528233687654\n",
      "Epoch: 18, iteration: 134, loss: 0.3872618001313254\n",
      "Epoch: 18, iteration: 135, loss: 0.5717994764805834\n",
      "Epoch: 18, iteration: 136, loss: 0.2969327950221647\n",
      "Epoch: 18, iteration: 137, loss: 0.2782925811619506\n",
      "Epoch: 18, iteration: 138, loss: 0.32392515662550014\n",
      "Epoch: 18, iteration: 139, loss: 0.33810774294298723\n",
      "Epoch: 18, iteration: 140, loss: 0.2983734483628379\n",
      "Epoch: 18, iteration: 141, loss: 0.35482800606824894\n",
      "Epoch: 18, iteration: 142, loss: 0.27673181348255343\n",
      "Epoch: 18, iteration: 143, loss: 0.3847161229614042\n",
      "Epoch: 18, iteration: 144, loss: 0.20141408303464772\n",
      "Epoch: 18, iteration: 145, loss: 0.5401000603197526\n",
      "Epoch: 18, iteration: 146, loss: 0.25782112072553265\n",
      "Epoch: 18, iteration: 147, loss: 0.28264418022740456\n",
      "Epoch: 18, iteration: 148, loss: 0.2894913918310852\n",
      "Epoch: 18, iteration: 149, loss: 0.29993868220466363\n",
      "Epoch: 18, iteration: 150, loss: 0.31305235734482606\n",
      "Epoch: 18, iteration: 151, loss: 0.35852103538555385\n",
      "Epoch: 18, iteration: 152, loss: 0.2911388944150228\n",
      "Epoch: 18, iteration: 153, loss: 0.29782619280502987\n",
      "Epoch: 18, iteration: 154, loss: 0.2806130263914328\n",
      "Epoch: 18, iteration: 155, loss: 0.39540619281433\n",
      "Epoch: 18, iteration: 156, loss: 0.310637267990059\n",
      "Epoch: 18, iteration: 157, loss: 0.3053787928899171\n",
      "Epoch: 18, iteration: 158, loss: 0.4926125252944955\n",
      "Epoch: 18, iteration: 159, loss: 0.38482008140068147\n",
      "Epoch: 18, iteration: 160, loss: 0.27752751912171325\n",
      "Epoch: 18, iteration: 161, loss: 0.2950937958493673\n",
      "Epoch: 18, iteration: 162, loss: 0.2993859895323501\n",
      "Epoch: 18, iteration: 163, loss: 0.23458616954763253\n",
      "Epoch: 18, iteration: 164, loss: 0.7603174097643951\n",
      "Epoch: 18, iteration: 165, loss: 0.2984582286390664\n",
      "Epoch: 18, iteration: 166, loss: 0.3506704441367197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, iteration: 167, loss: 0.2909654743510043\n",
      "Epoch: 18, iteration: 168, loss: 0.2528408368244098\n",
      "Epoch: 18, iteration: 169, loss: 0.34433873343918464\n",
      "Epoch: 18, iteration: 170, loss: 0.3547920470908826\n",
      "Epoch: 18, iteration: 171, loss: 0.2946415666516906\n",
      "Epoch: 18, iteration: 172, loss: 0.28192540338236083\n",
      "Epoch: 18, iteration: 173, loss: 0.2766915188162635\n",
      "Epoch: 18, iteration: 174, loss: 0.3734887934932639\n",
      "Epoch: 18, iteration: 175, loss: 0.269237139134762\n",
      "Epoch: 18, iteration: 176, loss: 0.3054420200358533\n",
      "Epoch: 18, iteration: 177, loss: 0.37310876185882136\n",
      "Epoch: 18, iteration: 178, loss: 0.33968394172177174\n",
      "Epoch: 18, iteration: 179, loss: 0.3139818134115654\n",
      "Epoch: 18, iteration: 180, loss: 0.285012129195457\n",
      "Epoch: 18, iteration: 181, loss: 0.31691965745764905\n",
      "Epoch: 18, iteration: 182, loss: 0.3072388278760015\n",
      "Epoch: 18, iteration: 183, loss: 0.27994696666010344\n",
      "Epoch: 18, iteration: 184, loss: 0.2920795077809256\n",
      "Epoch: 18, iteration: 185, loss: 0.39871847567363267\n",
      "Epoch: 18, iteration: 186, loss: 0.3382243455961795\n",
      "Epoch: 18, iteration: 187, loss: 0.21960670704501548\n",
      "Epoch: 18, iteration: 188, loss: 0.2743206869375354\n",
      "17    0.2888707745 1.0634427388 0.6714507 0.6588718\n",
      "Epoch: 19, iteration: 1, loss: 0.41483064073197706\n",
      "Epoch: 19, iteration: 2, loss: 0.26328409475161413\n",
      "Epoch: 19, iteration: 3, loss: 0.2453715511850317\n",
      "Epoch: 19, iteration: 4, loss: 0.34339707781450984\n",
      "Epoch: 19, iteration: 5, loss: 0.26354343272620584\n",
      "Epoch: 19, iteration: 6, loss: 0.9011634842295309\n",
      "Epoch: 19, iteration: 7, loss: 0.2475207503127927\n",
      "Epoch: 19, iteration: 8, loss: 0.3444210412689042\n",
      "Epoch: 19, iteration: 9, loss: 0.25804542897643545\n",
      "Epoch: 19, iteration: 10, loss: 0.29559723339494665\n",
      "Epoch: 19, iteration: 11, loss: 0.26577332990722463\n",
      "Epoch: 19, iteration: 12, loss: 0.30347489192929855\n",
      "Epoch: 19, iteration: 13, loss: 0.23737122641188274\n",
      "Epoch: 19, iteration: 14, loss: 0.31529777684565136\n",
      "Epoch: 19, iteration: 15, loss: 0.41846217690555876\n",
      "Epoch: 19, iteration: 16, loss: 0.24469097928876027\n",
      "Epoch: 19, iteration: 17, loss: 0.30570187192805365\n",
      "Epoch: 19, iteration: 18, loss: 0.2257448046255311\n",
      "Epoch: 19, iteration: 19, loss: 0.32356224377272397\n",
      "Epoch: 19, iteration: 20, loss: 0.28557096196657195\n",
      "Epoch: 19, iteration: 21, loss: 0.37992504186662407\n",
      "Epoch: 19, iteration: 22, loss: 0.3156977738249268\n",
      "Epoch: 19, iteration: 23, loss: 0.37404751403163294\n",
      "Epoch: 19, iteration: 24, loss: 0.3104373188147029\n",
      "Epoch: 19, iteration: 25, loss: 0.25194236656001134\n",
      "Epoch: 19, iteration: 26, loss: 0.3005701028750487\n",
      "Epoch: 19, iteration: 27, loss: 0.33333448463304244\n",
      "Epoch: 19, iteration: 28, loss: 0.46661431651552226\n",
      "Epoch: 19, iteration: 29, loss: 0.3370448792511294\n",
      "Epoch: 19, iteration: 30, loss: 0.2508735550398087\n",
      "Epoch: 19, iteration: 31, loss: 0.6921582418464384\n",
      "Epoch: 19, iteration: 32, loss: 0.4540252043049117\n",
      "Epoch: 19, iteration: 33, loss: 0.30322593242443785\n",
      "Epoch: 19, iteration: 34, loss: 0.2672095005442117\n",
      "Epoch: 19, iteration: 35, loss: 0.29931675653743267\n",
      "Epoch: 19, iteration: 36, loss: 0.20236164387178984\n",
      "Epoch: 19, iteration: 37, loss: 0.32900577010007087\n",
      "Epoch: 19, iteration: 38, loss: 0.24405735127668288\n",
      "Epoch: 19, iteration: 39, loss: 0.36030191867887085\n",
      "Epoch: 19, iteration: 40, loss: 0.28936413746327627\n",
      "Epoch: 19, iteration: 41, loss: 0.22124383267759903\n",
      "Epoch: 19, iteration: 42, loss: 0.29061790953136163\n",
      "Epoch: 19, iteration: 43, loss: 0.27668912622631414\n",
      "Epoch: 19, iteration: 44, loss: 0.31565204467279645\n",
      "Epoch: 19, iteration: 45, loss: 0.2857947026519289\n",
      "Epoch: 19, iteration: 46, loss: 0.3032479995878214\n",
      "Epoch: 19, iteration: 47, loss: 0.31026357683049444\n",
      "Epoch: 19, iteration: 48, loss: 0.2285911797298601\n",
      "Epoch: 19, iteration: 49, loss: 0.25846862620593736\n",
      "Epoch: 19, iteration: 50, loss: 0.2606903065421987\n",
      "Epoch: 19, iteration: 51, loss: 0.2974833553142272\n",
      "Epoch: 19, iteration: 52, loss: 0.26481191810512617\n",
      "Epoch: 19, iteration: 53, loss: 0.27997550365574425\n",
      "Epoch: 19, iteration: 54, loss: 0.27490787407757894\n",
      "Epoch: 19, iteration: 55, loss: 0.33912732615053254\n",
      "Epoch: 19, iteration: 56, loss: 0.3201302113557886\n",
      "Epoch: 19, iteration: 57, loss: 0.2388580378804441\n",
      "Epoch: 19, iteration: 58, loss: 0.3005618895723457\n",
      "Epoch: 19, iteration: 59, loss: 0.3474041864037447\n",
      "Epoch: 19, iteration: 60, loss: 0.3021310656635563\n",
      "Epoch: 19, iteration: 61, loss: 0.34764281312451073\n",
      "Epoch: 19, iteration: 62, loss: 0.2767767929322927\n",
      "Epoch: 19, iteration: 63, loss: 0.2587230987330024\n",
      "Epoch: 19, iteration: 64, loss: 0.28787717469446183\n",
      "Epoch: 19, iteration: 65, loss: 0.279301329080137\n",
      "Epoch: 19, iteration: 66, loss: 0.2362152085427605\n",
      "Epoch: 19, iteration: 67, loss: 0.2739630299603465\n",
      "Epoch: 19, iteration: 68, loss: 0.35939280155695763\n",
      "Epoch: 19, iteration: 69, loss: 0.2721298360144255\n",
      "Epoch: 19, iteration: 70, loss: 0.31989097142308376\n",
      "Epoch: 19, iteration: 71, loss: 0.3910331245734749\n",
      "Epoch: 19, iteration: 72, loss: 0.3022987053433467\n",
      "Epoch: 19, iteration: 73, loss: 0.3618892874202086\n",
      "Epoch: 19, iteration: 74, loss: 0.21876643890869615\n",
      "Epoch: 19, iteration: 75, loss: 0.2814809813894937\n",
      "Epoch: 19, iteration: 76, loss: 0.4120752114967229\n",
      "Epoch: 19, iteration: 77, loss: 0.273317958642835\n",
      "Epoch: 19, iteration: 78, loss: 0.43029358410632285\n",
      "Epoch: 19, iteration: 79, loss: 0.2925996938707186\n",
      "Epoch: 19, iteration: 80, loss: 0.32943937114568167\n",
      "Epoch: 19, iteration: 81, loss: 0.47687149880102225\n",
      "Epoch: 19, iteration: 82, loss: 0.3133562551603476\n",
      "Epoch: 19, iteration: 83, loss: 0.264941140825739\n",
      "Epoch: 19, iteration: 84, loss: 0.29812281336612856\n",
      "Epoch: 19, iteration: 85, loss: 0.2832994363606105\n",
      "Epoch: 19, iteration: 86, loss: 0.23425609026729297\n",
      "Epoch: 19, iteration: 87, loss: 0.39769508029602685\n",
      "Epoch: 19, iteration: 88, loss: 0.27883369235938615\n",
      "Epoch: 19, iteration: 89, loss: 0.39613077396294794\n",
      "Epoch: 19, iteration: 90, loss: 0.4041176399050798\n",
      "Epoch: 19, iteration: 91, loss: 0.2614394354435246\n",
      "Epoch: 19, iteration: 92, loss: 0.32440744037633756\n",
      "Epoch: 19, iteration: 93, loss: 0.4390525138062506\n",
      "Epoch: 19, iteration: 94, loss: 0.347953982306542\n",
      "Epoch: 19, iteration: 95, loss: 0.3752246119482015\n",
      "Epoch: 19, iteration: 96, loss: 0.3198324945040911\n",
      "Epoch: 19, iteration: 97, loss: 0.2443238309021847\n",
      "Epoch: 19, iteration: 98, loss: 0.3868033652726927\n",
      "Epoch: 19, iteration: 99, loss: 0.3399331250738231\n",
      "Epoch: 19, iteration: 100, loss: 0.2869853980794817\n",
      "Epoch: 19, iteration: 101, loss: 0.3273210539075019\n",
      "Epoch: 19, iteration: 102, loss: 0.342439198469057\n",
      "Epoch: 19, iteration: 103, loss: 0.2755122263238504\n",
      "Epoch: 19, iteration: 104, loss: 0.3109890837871049\n",
      "Epoch: 19, iteration: 105, loss: 0.2520252752731495\n",
      "Epoch: 19, iteration: 106, loss: 0.23347206048917393\n",
      "Epoch: 19, iteration: 107, loss: 0.32103497300586625\n",
      "Epoch: 19, iteration: 108, loss: 0.22910571308610075\n",
      "Epoch: 19, iteration: 109, loss: 0.32612642706083084\n",
      "Epoch: 19, iteration: 110, loss: 0.3075249377533921\n",
      "Epoch: 19, iteration: 111, loss: 0.20703788365571413\n",
      "Epoch: 19, iteration: 112, loss: 0.3085122557684646\n",
      "Epoch: 19, iteration: 113, loss: 0.264855997882383\n",
      "Epoch: 19, iteration: 114, loss: 0.21943812619251923\n",
      "Epoch: 19, iteration: 115, loss: 0.29881000382291817\n",
      "Epoch: 19, iteration: 116, loss: 0.337059922821352\n",
      "Epoch: 19, iteration: 117, loss: 0.34955428206996403\n",
      "Epoch: 19, iteration: 118, loss: 0.30532828906737636\n",
      "Epoch: 19, iteration: 119, loss: 0.24595109037320087\n",
      "Epoch: 19, iteration: 120, loss: 0.40328278311932414\n",
      "Epoch: 19, iteration: 121, loss: 0.213713897062263\n",
      "Epoch: 19, iteration: 122, loss: 0.3175243458690289\n",
      "Epoch: 19, iteration: 123, loss: 0.24566856670405576\n",
      "Epoch: 19, iteration: 124, loss: 0.2764463967887384\n",
      "Epoch: 19, iteration: 125, loss: 0.28558621697082937\n",
      "Epoch: 19, iteration: 126, loss: 0.25578950563224734\n",
      "Epoch: 19, iteration: 127, loss: 0.2074996377229438\n",
      "Epoch: 19, iteration: 128, loss: 0.2753482646118758\n",
      "Epoch: 19, iteration: 129, loss: 0.2947529228094945\n",
      "Epoch: 19, iteration: 130, loss: 0.3134136348032579\n",
      "Epoch: 19, iteration: 131, loss: 0.3260038773980495\n",
      "Epoch: 19, iteration: 132, loss: 0.27499635692030866\n",
      "Epoch: 19, iteration: 133, loss: 0.2589970123972707\n",
      "Epoch: 19, iteration: 134, loss: 0.2850098581226762\n",
      "Epoch: 19, iteration: 135, loss: 0.32923060064030846\n",
      "Epoch: 19, iteration: 136, loss: 0.18054297448845866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, iteration: 137, loss: 0.28273632625775164\n",
      "Epoch: 19, iteration: 138, loss: 0.38405974725088055\n",
      "Epoch: 19, iteration: 139, loss: 0.2129766223393533\n",
      "Epoch: 19, iteration: 140, loss: 0.27097982094849526\n",
      "Epoch: 19, iteration: 141, loss: 0.24630553824583504\n",
      "Epoch: 19, iteration: 142, loss: 0.3212266161518046\n",
      "Epoch: 19, iteration: 143, loss: 0.32541750904659333\n",
      "Epoch: 19, iteration: 144, loss: 0.317612444308289\n",
      "Epoch: 19, iteration: 145, loss: 0.29235417800355223\n",
      "Epoch: 19, iteration: 146, loss: 0.2725124755047494\n",
      "Epoch: 19, iteration: 147, loss: 0.30477124233856323\n",
      "Epoch: 19, iteration: 148, loss: 0.355633411303183\n",
      "Epoch: 19, iteration: 149, loss: 0.3146162635043888\n",
      "Epoch: 19, iteration: 150, loss: 0.22396518781550662\n",
      "Epoch: 19, iteration: 151, loss: 0.30134705509757787\n",
      "Epoch: 19, iteration: 152, loss: 0.3486658335521556\n",
      "Epoch: 19, iteration: 153, loss: 0.2639557834289565\n",
      "Epoch: 19, iteration: 154, loss: 0.24661797451571402\n",
      "Epoch: 19, iteration: 155, loss: 0.2744406862162277\n",
      "Epoch: 19, iteration: 156, loss: 0.2568604078665204\n",
      "Epoch: 19, iteration: 157, loss: 0.39986023796533793\n",
      "Epoch: 19, iteration: 158, loss: 0.28121133924556957\n",
      "Epoch: 19, iteration: 159, loss: 0.2658960016369671\n",
      "Epoch: 19, iteration: 160, loss: 0.29297363302333984\n",
      "Epoch: 19, iteration: 161, loss: 0.24504537653334443\n",
      "Epoch: 19, iteration: 162, loss: 0.203913047069781\n",
      "Epoch: 19, iteration: 163, loss: 0.2528902555297339\n",
      "Epoch: 19, iteration: 164, loss: 0.36108628642334256\n",
      "Epoch: 19, iteration: 165, loss: 0.2085946084888518\n",
      "Epoch: 19, iteration: 166, loss: 0.2683701325558754\n",
      "Epoch: 19, iteration: 167, loss: 0.6451043028689334\n",
      "Epoch: 19, iteration: 168, loss: 0.2955542762906514\n",
      "Epoch: 19, iteration: 169, loss: 0.28090733740986035\n",
      "Epoch: 19, iteration: 170, loss: 0.34244611323240326\n",
      "Epoch: 19, iteration: 171, loss: 0.2481658451207\n",
      "Epoch: 19, iteration: 172, loss: 0.23064814611333392\n",
      "Epoch: 19, iteration: 173, loss: 0.2684547348691112\n",
      "Epoch: 19, iteration: 174, loss: 0.22787334779596116\n",
      "Epoch: 19, iteration: 175, loss: 0.24311815400481443\n",
      "Epoch: 19, iteration: 176, loss: 0.49477296149303296\n",
      "Epoch: 19, iteration: 177, loss: 0.3174432861070988\n",
      "Epoch: 19, iteration: 178, loss: 0.38088446596233017\n",
      "Epoch: 19, iteration: 179, loss: 0.22224347227207686\n",
      "Epoch: 19, iteration: 180, loss: 0.4550997000230325\n",
      "Epoch: 19, iteration: 181, loss: 0.27605813783902944\n",
      "Epoch: 19, iteration: 182, loss: 0.4036076975863001\n",
      "Epoch: 19, iteration: 183, loss: 0.25852778411625477\n",
      "Epoch: 19, iteration: 184, loss: 0.25814758012208366\n",
      "Epoch: 19, iteration: 185, loss: 0.293549264422722\n",
      "Epoch: 19, iteration: 186, loss: 0.24389754192525598\n",
      "Epoch: 19, iteration: 187, loss: 0.2702131387364316\n",
      "Epoch: 19, iteration: 188, loss: 0.4867429738717524\n",
      "18    0.2603200647 1.2267572816 0.6912607 0.6812329\n",
      "Epoch: 20, iteration: 1, loss: 0.2700773857718642\n",
      "Epoch: 20, iteration: 2, loss: 0.22749326164791797\n",
      "Epoch: 20, iteration: 3, loss: 0.29050712247179433\n",
      "Epoch: 20, iteration: 4, loss: 0.31162544159958383\n",
      "Epoch: 20, iteration: 5, loss: 0.3135945448233236\n",
      "Epoch: 20, iteration: 6, loss: 0.35932382318794615\n",
      "Epoch: 20, iteration: 7, loss: 0.2238413381067706\n",
      "Epoch: 20, iteration: 8, loss: 0.2745551900262858\n",
      "Epoch: 20, iteration: 9, loss: 0.26939460063935644\n",
      "Epoch: 20, iteration: 10, loss: 0.32873536142562354\n",
      "Epoch: 20, iteration: 11, loss: 0.30427387212205814\n",
      "Epoch: 20, iteration: 12, loss: 0.2747102226569917\n",
      "Epoch: 20, iteration: 13, loss: 0.24723297800678878\n",
      "Epoch: 20, iteration: 14, loss: 0.250706843200232\n",
      "Epoch: 20, iteration: 15, loss: 0.23993392766639254\n",
      "Epoch: 20, iteration: 16, loss: 0.21776239146281168\n",
      "Epoch: 20, iteration: 17, loss: 0.32698929411779026\n",
      "Epoch: 20, iteration: 18, loss: 0.29685491532375685\n",
      "Epoch: 20, iteration: 19, loss: 0.3181791662720342\n",
      "Epoch: 20, iteration: 20, loss: 0.2969806463049792\n",
      "Epoch: 20, iteration: 21, loss: 0.36439538697802887\n",
      "Epoch: 20, iteration: 22, loss: 0.3967902597039292\n",
      "Epoch: 20, iteration: 23, loss: 0.3869214396293465\n",
      "Epoch: 20, iteration: 24, loss: 0.2864382417045241\n",
      "Epoch: 20, iteration: 25, loss: 0.41538801230597117\n",
      "Epoch: 20, iteration: 26, loss: 0.34728598247102993\n",
      "Epoch: 20, iteration: 27, loss: 0.32084724598685377\n",
      "Epoch: 20, iteration: 28, loss: 0.22404706881034742\n",
      "Epoch: 20, iteration: 29, loss: 0.28347419331061785\n",
      "Epoch: 20, iteration: 30, loss: 0.270233179050099\n",
      "Epoch: 20, iteration: 31, loss: 0.2723462231965188\n",
      "Epoch: 20, iteration: 32, loss: 0.20391667390932225\n",
      "Epoch: 20, iteration: 33, loss: 0.358441653294563\n",
      "Epoch: 20, iteration: 34, loss: 0.2510492156894749\n",
      "Epoch: 20, iteration: 35, loss: 0.38028165382064927\n",
      "Epoch: 20, iteration: 36, loss: 0.3218734114873764\n",
      "Epoch: 20, iteration: 37, loss: 0.2990349469368206\n",
      "Epoch: 20, iteration: 38, loss: 0.7996707873000968\n",
      "Epoch: 20, iteration: 39, loss: 0.2499107386384022\n",
      "Epoch: 20, iteration: 40, loss: 0.36361086248124164\n",
      "Epoch: 20, iteration: 41, loss: 0.2607357185336473\n",
      "Epoch: 20, iteration: 42, loss: 0.31993829499196985\n",
      "Epoch: 20, iteration: 43, loss: 0.26471837693541206\n",
      "Epoch: 20, iteration: 44, loss: 0.2599582704078248\n",
      "Epoch: 20, iteration: 45, loss: 0.28524891227529736\n",
      "Epoch: 20, iteration: 46, loss: 0.23297642699054288\n",
      "Epoch: 20, iteration: 47, loss: 0.30602961293753184\n",
      "Epoch: 20, iteration: 48, loss: 0.42335735065977115\n",
      "Epoch: 20, iteration: 49, loss: 0.22914097230722574\n",
      "Epoch: 20, iteration: 50, loss: 0.30183608854070515\n",
      "Epoch: 20, iteration: 51, loss: 0.20513124690736836\n",
      "Epoch: 20, iteration: 52, loss: 0.26654879678660304\n",
      "Epoch: 20, iteration: 53, loss: 0.2827028365555753\n",
      "Epoch: 20, iteration: 54, loss: 0.2577251661503719\n",
      "Epoch: 20, iteration: 55, loss: 0.2226690411803352\n",
      "Epoch: 20, iteration: 56, loss: 0.30353132666246707\n",
      "Epoch: 20, iteration: 57, loss: 0.24282210429885578\n",
      "Epoch: 20, iteration: 58, loss: 0.259284020373025\n",
      "Epoch: 20, iteration: 59, loss: 0.25401155614943666\n",
      "Epoch: 20, iteration: 60, loss: 0.372314916051802\n",
      "Epoch: 20, iteration: 61, loss: 0.32602524161416185\n",
      "Epoch: 20, iteration: 62, loss: 0.2133900703923917\n",
      "Epoch: 20, iteration: 63, loss: 0.2935778783727116\n",
      "Epoch: 20, iteration: 64, loss: 0.2976208405963336\n",
      "Epoch: 20, iteration: 65, loss: 0.25637823901469997\n",
      "Epoch: 20, iteration: 66, loss: 0.35184768975650943\n",
      "Epoch: 20, iteration: 67, loss: 0.33558380038854563\n",
      "Epoch: 20, iteration: 68, loss: 0.30686445154285896\n",
      "Epoch: 20, iteration: 69, loss: 0.20557289703339984\n",
      "Epoch: 20, iteration: 70, loss: 0.23448893111642025\n",
      "Epoch: 20, iteration: 71, loss: 0.28150569460606567\n",
      "Epoch: 20, iteration: 72, loss: 0.39764583807478504\n",
      "Epoch: 20, iteration: 73, loss: 0.2299796629850662\n",
      "Epoch: 20, iteration: 74, loss: 0.2989194146737034\n",
      "Epoch: 20, iteration: 75, loss: 0.3313348605938303\n",
      "Epoch: 20, iteration: 76, loss: 0.21787808313784662\n",
      "Epoch: 20, iteration: 77, loss: 0.35239173856657735\n",
      "Epoch: 20, iteration: 78, loss: 0.2921134862722867\n",
      "Epoch: 20, iteration: 79, loss: 0.20624040999222504\n",
      "Epoch: 20, iteration: 80, loss: 0.3078374672840604\n",
      "Epoch: 20, iteration: 81, loss: 0.342410710181291\n",
      "Epoch: 20, iteration: 82, loss: 0.27108064053868725\n",
      "Epoch: 20, iteration: 83, loss: 0.2676635431470913\n",
      "Epoch: 20, iteration: 84, loss: 0.30815396943432444\n",
      "Epoch: 20, iteration: 85, loss: 0.22996985239364662\n",
      "Epoch: 20, iteration: 86, loss: 0.2862537351974025\n",
      "Epoch: 20, iteration: 87, loss: 0.3033309897309836\n",
      "Epoch: 20, iteration: 88, loss: 0.4031327278889638\n",
      "Epoch: 20, iteration: 89, loss: 0.27019952439727546\n",
      "Epoch: 20, iteration: 90, loss: 0.2650390468846238\n",
      "Epoch: 20, iteration: 91, loss: 0.22327738025129745\n",
      "Epoch: 20, iteration: 92, loss: 0.2126007075791149\n",
      "Epoch: 20, iteration: 93, loss: 0.24688875217039094\n",
      "Epoch: 20, iteration: 94, loss: 0.29391607385460483\n",
      "Epoch: 20, iteration: 95, loss: 0.2180882174846751\n",
      "Epoch: 20, iteration: 96, loss: 0.2313367303467309\n",
      "Epoch: 20, iteration: 97, loss: 0.22404535004396295\n",
      "Epoch: 20, iteration: 98, loss: 0.4280699417313485\n",
      "Epoch: 20, iteration: 99, loss: 0.23754691145434\n",
      "Epoch: 20, iteration: 100, loss: 0.3827568236270346\n",
      "Epoch: 20, iteration: 101, loss: 0.26217400475805486\n",
      "Epoch: 20, iteration: 102, loss: 0.3494463956888498\n",
      "Epoch: 20, iteration: 103, loss: 0.21936931435957208\n",
      "Epoch: 20, iteration: 104, loss: 0.27897290758995447\n",
      "Epoch: 20, iteration: 105, loss: 0.37432672440514053\n",
      "Epoch: 20, iteration: 106, loss: 0.2555489281444051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, iteration: 107, loss: 0.2404506801814551\n",
      "Epoch: 20, iteration: 108, loss: 0.2910506488005902\n",
      "Epoch: 20, iteration: 109, loss: 0.23079596658334067\n",
      "Epoch: 20, iteration: 110, loss: 0.27969324324878947\n",
      "Epoch: 20, iteration: 111, loss: 0.24478637635201897\n",
      "Epoch: 20, iteration: 112, loss: 0.24853316177420673\n",
      "Epoch: 20, iteration: 113, loss: 0.22376710282792509\n",
      "Epoch: 20, iteration: 114, loss: 0.291786931771005\n",
      "Epoch: 20, iteration: 115, loss: 0.24831928403806783\n",
      "Epoch: 20, iteration: 116, loss: 0.43997652983906144\n",
      "Epoch: 20, iteration: 117, loss: 0.30143524907528557\n",
      "Epoch: 20, iteration: 118, loss: 0.35868817923872826\n",
      "Epoch: 20, iteration: 119, loss: 0.44877021582676696\n",
      "Epoch: 20, iteration: 120, loss: 0.25201604962779817\n",
      "Epoch: 20, iteration: 121, loss: 0.2299514221990578\n",
      "Epoch: 20, iteration: 122, loss: 0.806797162669034\n",
      "Epoch: 20, iteration: 123, loss: 0.2782972177543697\n",
      "Epoch: 20, iteration: 124, loss: 0.2679663762299836\n",
      "Epoch: 20, iteration: 125, loss: 0.20154561176772787\n",
      "Epoch: 20, iteration: 126, loss: 0.22210494121091623\n",
      "Epoch: 20, iteration: 127, loss: 0.24586578487649008\n",
      "Epoch: 20, iteration: 128, loss: 0.249683476954119\n",
      "Epoch: 20, iteration: 129, loss: 0.26607556845979763\n",
      "Epoch: 20, iteration: 130, loss: 0.24538835671856477\n",
      "Epoch: 20, iteration: 131, loss: 0.27386891247692746\n",
      "Epoch: 20, iteration: 132, loss: 0.25086118259230017\n",
      "Epoch: 20, iteration: 133, loss: 0.2670248345376875\n",
      "Epoch: 20, iteration: 134, loss: 0.34990676187185926\n",
      "Epoch: 20, iteration: 135, loss: 0.2241965291556652\n",
      "Epoch: 20, iteration: 136, loss: 0.25720537198499466\n",
      "Epoch: 20, iteration: 137, loss: 0.3014099809958946\n",
      "Epoch: 20, iteration: 138, loss: 0.3123838368909087\n",
      "Epoch: 20, iteration: 139, loss: 0.256181216281011\n",
      "Epoch: 20, iteration: 140, loss: 0.299904119016072\n",
      "Epoch: 20, iteration: 141, loss: 0.23998163062235647\n",
      "Epoch: 20, iteration: 142, loss: 0.4197018875566424\n",
      "Epoch: 20, iteration: 143, loss: 0.3156969001749168\n",
      "Epoch: 20, iteration: 144, loss: 0.2584322217861515\n",
      "Epoch: 20, iteration: 145, loss: 0.22603554274067672\n",
      "Epoch: 20, iteration: 146, loss: 0.23221653019137412\n",
      "Epoch: 20, iteration: 147, loss: 0.2477556422666495\n",
      "Epoch: 20, iteration: 148, loss: 0.2657924684776092\n",
      "Epoch: 20, iteration: 149, loss: 0.2481948481972571\n",
      "Epoch: 20, iteration: 150, loss: 0.3203167252175808\n",
      "Epoch: 20, iteration: 151, loss: 0.29402426208649324\n",
      "Epoch: 20, iteration: 152, loss: 0.28110943736699906\n",
      "Epoch: 20, iteration: 153, loss: 0.271763466842344\n",
      "Epoch: 20, iteration: 154, loss: 0.27895917976610185\n",
      "Epoch: 20, iteration: 155, loss: 0.21469784009933904\n",
      "Epoch: 20, iteration: 156, loss: 0.3071033075445939\n",
      "Epoch: 20, iteration: 157, loss: 0.250864536511463\n",
      "Epoch: 20, iteration: 158, loss: 0.21104320496458268\n",
      "Epoch: 20, iteration: 159, loss: 0.2673793935658\n",
      "Epoch: 20, iteration: 160, loss: 0.2052455804443154\n",
      "Epoch: 20, iteration: 161, loss: 0.20649579047314037\n",
      "Epoch: 20, iteration: 162, loss: 0.24596148972712178\n",
      "Epoch: 20, iteration: 163, loss: 0.3126994337083999\n",
      "Epoch: 20, iteration: 164, loss: 0.2777789833834969\n",
      "Epoch: 20, iteration: 165, loss: 0.20986171504225298\n",
      "Epoch: 20, iteration: 166, loss: 0.20806577982707017\n",
      "Epoch: 20, iteration: 167, loss: 0.24031699134293028\n",
      "Epoch: 20, iteration: 168, loss: 0.24116763641377006\n",
      "Epoch: 20, iteration: 169, loss: 0.30367470772217847\n",
      "Epoch: 20, iteration: 170, loss: 0.23565338165737384\n",
      "Epoch: 20, iteration: 171, loss: 0.22286335518145295\n",
      "Epoch: 20, iteration: 172, loss: 0.24758025415318988\n",
      "Epoch: 20, iteration: 173, loss: 0.22378248945116677\n",
      "Epoch: 20, iteration: 174, loss: 0.32667690640486124\n",
      "Epoch: 20, iteration: 175, loss: 0.21343099348534722\n",
      "Epoch: 20, iteration: 176, loss: 0.25338229889389347\n",
      "Epoch: 20, iteration: 177, loss: 0.23110760402875347\n",
      "Epoch: 20, iteration: 178, loss: 0.3274859741173396\n",
      "Epoch: 20, iteration: 179, loss: 0.21537131016816297\n",
      "Epoch: 20, iteration: 180, loss: 0.22451971740358956\n",
      "Epoch: 20, iteration: 181, loss: 0.28028347217444727\n",
      "Epoch: 20, iteration: 182, loss: 0.30672973555489574\n",
      "Epoch: 20, iteration: 183, loss: 0.25778591488793057\n",
      "Epoch: 20, iteration: 184, loss: 0.28539637339337065\n",
      "Epoch: 20, iteration: 185, loss: 0.26188691324818847\n",
      "Epoch: 20, iteration: 186, loss: 0.24752835977811974\n",
      "Epoch: 20, iteration: 187, loss: 0.2763750729825104\n",
      "Epoch: 20, iteration: 188, loss: 0.2685531848899016\n",
      "19    0.2480879796 1.3877969273 0.7040957 0.6885895\n",
      "Done training!\n",
      "CPU times: user 1h 18min 38s, sys: 42min 42s, total: 2h 1min 21s\n",
      "Wall time: 38min 10s\n"
     ]
    }
   ],
   "source": [
    "%time train(train_dataloader, valid_dataloader, model, optimizer, scheduler, criterion, device, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(valid_dataloader, model, criterion, device, threshold=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for finding the optimal threshold.\n",
    "# Haven't proved out to be very useful yet.\n",
    "if True:\n",
    "    f1_scores = []\n",
    "    for threshold in np.arange(0.05, 1, 0.05):\n",
    "        _, f1 = evaluate(valid_dataloader, model, criterion, device, threshold=threshold)\n",
    "        f1_scores.append(f1)\n",
    "        print(f'threshold: {threshold}, f1 score: {f1}')\n",
    "\n",
    "    plt.plot(np.arange(0.05, 1, 0.05), f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some images with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, device, train_dataloader, n_to_show=10, threshold=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re_predict = True\n",
    "\n",
    "if re_predict:\n",
    "\n",
    "    # Predict\n",
    "    y_true, y_pred = predict(model, device, valid_dataloader)\n",
    "    np.save(f'../data/valid_true_labels.npy', y_true)\n",
    "    np.save(f'../data/valid_pred_labels.npy', y_pred)\n",
    "    \n",
    "    # Save classification report\n",
    "    with open(f'../data/valid_classification_report.txt', 'w') as file:\n",
    "        file.write(skm.classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Save confusion matrix plot\n",
    "    labels = [k for k, v in get_class_map().items()]\n",
    "    visualize_confusion_matrix(y_true, y_pred, labels, f'../data/valid_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification report\n",
    "with open(f'../data/valid_classification_report.txt', 'r') as file:\n",
    "    report = ''.join(file.readlines())\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confus|ion matrix plot\n",
    "IPython_Image(filename=f'../data/valid_confusion_matrix.png', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
