{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import vgg16, vgg16_bn, resnet18, resnet34\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from IPython.display import display, clear_output, Image as IPython_Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../data/images'\n",
    "LABEL_PATH = '../data/annotations'\n",
    "\n",
    "# GIVEN DATASET\n",
    "MEAN = (0.43672, 0.40107, 0.36762)\n",
    "STD = (0.30139, 0.28781, 0.29236)\n",
    "\n",
    "# IMAGENET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# RESNET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "       \n",
    "DEFAULT_THRESHOLDS = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "# Define default pos_weights for nn.BCEWithLogitsLoss(pos_weights). If the get_dataloader() is run, is will \n",
    "# overwrite this variable based on the statistics of the test set.\n",
    "label_pos_weights_for_loss = np.array([209.52631579, 55.87203791, 58.40594059, 16.77777778, 44.80152672, 5.25, 25.14379085, 5.75675676, 33.09090909, 2.15540363, 5.51465798, 163.38356164, 119., 37.46153846], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_classes():\n",
    "    return len(listdir(LABEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_map():\n",
    "    ret = {}\n",
    "\n",
    "    i = 0\n",
    "    for fname in listdir(LABEL_PATH):\n",
    "        img_class, _ = fname.split('.')\n",
    "        ret[img_class] = i\n",
    "        i += 1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels_to_csv(name_of_set, label_array):\n",
    "    filepath = f'../data/labels_{name_of_set}.csv'\n",
    "    \n",
    "    label_arr = np.array(label_array).astype(int)\n",
    "\n",
    "    # Save 2D numpy array to csv file\n",
    "    np.savetxt(filepath, label_arr, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_statistics(label_array, total_n_images):\n",
    "    label_arr = np.array(label_array).astype(int)\n",
    "    pos_label_counts_all = np.sum(label_arr, axis=0)\n",
    "    neg_label_counts_all = total_n_images - pos_label_counts_all\n",
    "    relative_label_occurrances =  pos_label_counts_all/total_n_images\n",
    "    pos_weights = neg_label_counts_all / pos_label_counts_all\n",
    "    \n",
    "    return pos_weights, pos_label_counts_all, neg_label_counts_all, relative_label_occurrances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_statistics(name_of_set, pos_weights, pos_label_counts_all, neg_label_counts_all, relative_label_occurrances):    \n",
    "    print(f'\\nData set: {name_of_set}')\n",
    "    print(' Positive label counts\\n  ', pos_label_counts_all)\n",
    "    print(' Negative label counts\\n  ', neg_label_counts_all)\n",
    "    print(' Relative label occurrances\\n  ', relative_label_occurrances)\n",
    "    print(' Pos_weights\\n  ', pos_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(bs=256, train_fr=.6, max_images_per_class=1e9, use_no_label_imgs = True, transformations=None):\n",
    "    \n",
    "    # by default only transform to tensor\n",
    "    if transformations is None:\n",
    "        transformations = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    # mapping from class names to integers\n",
    "    class_map = get_class_map()\n",
    "\n",
    "    # create a dictionary to hold our label vectors\n",
    "    n_classes = len(class_map.keys())\n",
    "    img_to_class = defaultdict(lambda: np.zeros(n_classes))\n",
    "\n",
    "    # another dictionary to hold the actual image data\n",
    "    img_to_data = dict()\n",
    "    \n",
    "    # loop through all the annotations\n",
    "    for fname in listdir(LABEL_PATH):\n",
    "        img_class, _ = fname.split('.')\n",
    "        print(f'Reading class: {img_class}')\n",
    "        \n",
    "        # open the annotation file\n",
    "        with open(f'{LABEL_PATH}/{fname}', 'r') as fh:\n",
    "\n",
    "            # get image ids from annotation file\n",
    "            img_ids = fh.read().splitlines()\n",
    "            \n",
    "            # gather the images with labels\n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                \n",
    "                # let's not process images unnecessarily\n",
    "                if not img_id in img_to_data:\n",
    "\n",
    "                    img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "                    img = Image.open(img_path)\n",
    "\n",
    "                    # apply transformations\n",
    "                    img_tensor = transformations(img.convert('RGB')).numpy() \n",
    "\n",
    "                    # append to dict\n",
    "                    img_to_data[img_id] = img_tensor\n",
    "\n",
    "                # get one-hot encoded vector of image classes\n",
    "                img_classes = img_to_class[img_id]\n",
    "\n",
    "                # add new class to image vector\n",
    "                img_class_id = class_map[img_class]\n",
    "                img_classes[img_class_id] = 1\n",
    "\n",
    "                # store the updated vector back\n",
    "                img_to_class[img_id] = img_classes\n",
    "\n",
    "                if i >= max_images_per_class:\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "    # If use_no_label_imgs is on, load also all the images that do not have any labels\n",
    "    i = 0\n",
    "    for fname in listdir(IMAGE_PATH):\n",
    "        m = re.match('im(\\d+)', fname)\n",
    "        img_id = m.group(1)\n",
    "\n",
    "        if img_id not in img_to_data:\n",
    "            img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # apply transformations\n",
    "            img_tensor = transformations(img.convert('RGB')).numpy() \n",
    "\n",
    "            # append to dict\n",
    "            img_to_data[img_id] = img_tensor\n",
    "\n",
    "            if i >= max_images_per_class:\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "    # collect data to a single array\n",
    "    X = []\n",
    "    y = []\n",
    "    for img_id in img_to_data.keys():\n",
    "        X.append(img_to_data[img_id])\n",
    "        y.append(img_to_class[img_id])\n",
    "\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, train_size=train_fr, random_state=42)\n",
    "    X_test, X_valid, y_test, y_valid = train_test_split(X_tmp, y_tmp, train_size=.5, test_size=.5, random_state=42)\n",
    "    \n",
    "    # Write labels to csv file\n",
    "    #write_labels_to_csv('all', y)\n",
    "    #write_labels_to_csv('train', y_train)\n",
    "    #write_labels_to_csv('validation', y_valid)\n",
    "    #write_labels_to_csv('test', y_test)\n",
    "    \n",
    "    # Calculate and print out label statistics to be able to analyze label imbalance\n",
    "    pos_weights_all, pos_label_counts_all, neg_label_counts_all, relative_label_occurrances_all = calculate_label_statistics(y, len(X))\n",
    "    pos_weights_train, pos_label_counts_train, neg_label_counts_train, relative_label_occurrances_train = calculate_label_statistics(y_train, len(X_train))\n",
    "    pos_weights_valid, pos_label_counts_valid, neg_label_counts_valid, relative_label_occurrances_valid = calculate_label_statistics(y_valid, len(X_valid))\n",
    "    pos_weights_test, pos_label_counts_test, neg_label_counts_test, relative_label_occurrances_test = calculate_label_statistics(y_test, len(X_test))\n",
    "    \n",
    "    print_label_statistics('all', pos_weights_all, pos_label_counts_all, neg_label_counts_all, relative_label_occurrances_all)\n",
    "    print_label_statistics('train', pos_weights_train, pos_label_counts_train, neg_label_counts_train, relative_label_occurrances_train)\n",
    "    print_label_statistics('validation', pos_weights_valid, pos_label_counts_valid, neg_label_counts_valid, relative_label_occurrances_valid)\n",
    "    print_label_statistics('test', pos_weights_test, pos_label_counts_test, neg_label_counts_test, relative_label_occurrances_test)\n",
    "    \n",
    "    # Set the pos_weights to be used in the loss function\n",
    "    label_pos_weights_for_loss = pos_weights_train\n",
    "    \n",
    "    train_dataloader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float),\n",
    "        torch.tensor(y_train, dtype=torch.float)),\n",
    "        shuffle=True,\n",
    "        batch_size=bs)           \n",
    "\n",
    "    valid_dataloader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_valid, dtype=torch.float),\n",
    "        torch.tensor(y_valid, dtype=torch.float)),\n",
    "        shuffle=True,\n",
    "        batch_size=bs)\n",
    "\n",
    "    test_dataloader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float),\n",
    "        torch.tensor(y_test, dtype=torch.float)),\n",
    "        shuffle=True,\n",
    "        batch_size=bs)      \n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_classes):\n",
    "        super().__init__()\n",
    "        self.bs = bs\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden1 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden2 = nn.Linear(n_hidden2, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.hidden2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class OneLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class ConvNetModel(nn.Module):\n",
    "    def __init__(self, n_classes, keep_prob=.5):\n",
    "        super(ConvNetModel, self).__init__()\n",
    "        # Common layers used multiple times\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=1-keep_prob)\n",
    "        \n",
    "        # Unique layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=3, stride=1, padding=1) #(n samples, channels, height, width)\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=14, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc3 = nn.Linear(in_features=256*4*14, out_features=n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3, 128, 128)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)  # Flatten for FC\n",
    "        out = self.fc3(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, device, thresholds=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    f1_scores = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                score = f1_score(y.cpu() == 1, y_pred.cpu() > thresholds, average='micro')\n",
    "                f1_scores.append(score)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, model, optimizer, criterion, device, n_epochs=50, verbose=True):\n",
    "    model.train()\n",
    "\n",
    "    if verbose:\n",
    "        fmt = '{:<5} {:12} {:12} {:<9} {:<9}'\n",
    "        print(fmt.format('Epoch', 'Train loss', 'Valid loss', 'Train F1', 'Valid F1'))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            X, y = batch\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f'Epoch: {epoch+1}, iteration: {i+1}, loss: {loss}')\n",
    "\n",
    "        if verbose:\n",
    "            train_loss, train_score = evaluate(train_dataloader, model, criterion, device)\n",
    "            valid_loss, valid_score = evaluate(valid_dataloader, model, criterion, device)\n",
    "\n",
    "            fmt = '{:<5} {:03.10f} {:03.10f} {:02.7f} {:02.7f}'\n",
    "            print(fmt.format(epoch, train_loss, valid_loss, train_score, valid_score))\n",
    "            \n",
    "    print('Done training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, device, dataloader, mean=MEAN, std=STD, n_to_show=3, threshold=0.5):\n",
    "    \n",
    "    class_to_label = { v: k for k, v in get_class_map().items() }\n",
    "    \n",
    "    # https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/5\n",
    "    inv_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean = -1 * np.multiply(mean, std), std=np.divide(1, std))\n",
    "    ])\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        n_shown = 0\n",
    "        \n",
    "        X, y = batch\n",
    "        X = X.to(device)\n",
    "\n",
    "        y_pred_raw = model(X).cpu()\n",
    "        y_pred = y_pred_raw > threshold \n",
    "        y = y == 1\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            pred_classes = np.where(y_pred[i] == 1)[0]\n",
    "            true_classes = np.where(y[i] == 1)[0]\n",
    "            \n",
    "            true_classes_str = ', '.join([class_to_label[i] for i in true_classes])\n",
    "            pred_classes_str = ', '.join([class_to_label[i] for i in pred_classes])\n",
    "\n",
    "            img = inv_transform(X[i].cpu())              # inverse transforms\n",
    "            img = img.permute(2, 1, 0)                   # BGR -> RGB\n",
    "            img = np.rot90(img, 3)\n",
    "                                    \n",
    "            plt.title(f'True: {true_classes_str}, Predictions: {pred_classes_str}')\n",
    "            plt.imshow(img)\n",
    "            plt.pause(0.001)\n",
    "\n",
    "            if n_shown >= n_to_show:\n",
    "                return\n",
    "\n",
    "            n_shown += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to use same naming conventions everywhere. Might be a good idea to rename `y_hat` to `y_pred` or vice versa everywhere (it makes it easier to combine the notebooks). I don't care which way it is. Also the `Xs` and `ys` etc.\n",
    "\n",
    "The threshold value here is different than elsewhere. Maybe it should be moved to a global or something?\n",
    "\n",
    "Good idea could be also to check from `visualize_predictions` function how the actual predictions can be easily obtained from `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def predict_X(fr, threshold=0.25):\n",
    "    \n",
    "    y_hat = fr > threshold\n",
    "    \n",
    "    return y_hat if (np.sum(y_hat) > 0) else fr == np.max(fr)\n",
    "\n",
    "def predict(model, device, dataloader):\n",
    "    \n",
    "    ys_all = []  # Array of np.array(14) \n",
    "    y_hats_all = []\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        Xs, ys = batch\n",
    "        Xs = model(Xs.to(device))\n",
    "        y_hats = np.apply_along_axis(predict_X, axis=1, arr=Xs.cpu().detach().numpy())\n",
    "        \n",
    "        y_hats_all.extend(y_hat for y_hat in y_hats)\n",
    "        ys_all.extend(y.numpy() for y in ys==1)\n",
    "\n",
    "    return np.array(ys_all), np.array(y_hats_all)\n",
    "\n",
    "def visualize_confusion_matrix(y_true, y_pred, labels, file_path):\n",
    "\n",
    "    plt.ioff()\n",
    "    \n",
    "    # Get confusion matrices\n",
    "    cn_tensor = skm.multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Get precision, recall, f1-score\n",
    "    scores = skm.classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=5, ncols=3,sharey=True, figsize=(20, 20), \n",
    "                           gridspec_kw={'hspace': 0.3, 'wspace': 0.0})\n",
    "    gn = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    n = cn_tensor[0].sum()\n",
    "    \n",
    "    # Loop all labels\n",
    "    for i, cn_matrix in enumerate(cn_tensor):\n",
    "\n",
    "        j, k = int(i/3), i%3\n",
    "        \n",
    "        # Annotations\n",
    "        annot = np.asarray(\n",
    "            ['{}\\n{:0.0f}\\n{:.2%}'.format(gn[i], x, x/n) for i, x in enumerate(cn_matrix.flatten())]\n",
    "        ).reshape(2,2)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sn.heatmap(cn_matrix, annot=annot, fmt='', cmap='Blues', ax=ax[j, k])\n",
    "        \n",
    "        # Precision, recall, f1-score\n",
    "        title = '{}\\nprec.={:.3}, rec.={:.3}, f1={:.3}'.format(\n",
    "            labels[i], scores[str(i)]['precision'], scores[str(i)]['recall'], scores[str(i)]['f1-score'])\n",
    "        ax[j, k].set_title(title)\n",
    "        \n",
    "    plt.savefig(file_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Using GPU!')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "lr = 0.01\n",
    "n_epochs = 5\n",
    "bs = 256\n",
    "n_classes = len(get_class_map().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save / load dataloaders from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test the models that do not seem to work with 128x128 images change the transformation to do the resize.\n",
    "\n",
    "However, it should be investigated if, for example RESNET, can be made to work with 128x128 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#max_images_per_class = 7000\n",
    "use_no_label_imgs = True\n",
    "max_images_per_class = 1e9\n",
    "affix = \"_\" + str(max_images_per_class) if max_images_per_class < 1e9 else \"\"\n",
    "\n",
    "transformations = {\n",
    "    'train' : transforms.Compose([\n",
    "            # torchvision.transforms.Resize((224, 224)), # for testing RESNET\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=MEAN, std=STD)\n",
    "        ]),\n",
    "}\n",
    "transformations['valid'] = transformations['test'] = transformations['train']\n",
    "\n",
    "try:\n",
    "    train_dataloader = torch.load(f'../data/train_dataloader{affix}.dat')\n",
    "    valid_dataloader = torch.load(f'../data/valid_dataloader{affix}.dat')\n",
    "    test_dataloader = torch.load(f'../data/test_dataloader{affix}.dat')\n",
    "except:\n",
    "    train_dataloader, valid_dataloader, test_dataloader = get_dataloader(\n",
    "        bs=bs, \n",
    "        transformations=transformations['train'],\n",
    "        max_images_per_class=max_images_per_class,\n",
    "        use_no_label_imgs=use_no_label_imgs\n",
    "    )\n",
    "    torch.save(train_dataloader, f'../data/train_dataloader{affix}.dat')\n",
    "    torch.save(valid_dataloader, f'../data/valid_dataloader{affix}.dat')\n",
    "    torch.save(test_dataloader, f'../data/test_dataloader{affix}.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: The mean and std in transformations most probably need to be the same as for VGG and RESNET. Not 100% sure about this. Something to investigate!\n",
    "\n",
    "More models here: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "\n",
    "If the models do not start to converge, try lowering the learning rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_VGG16_\n",
    "\n",
    "This works with 128x128 and 224x224 images.\n",
    "\n",
    "I think I achieved the best validation f1 score with 128x128 images and vgg16, around 0.76.\n",
    "\n",
    "Surprisingly after quick testing the vgg16_bn (with BatchNorm layers) did not do as well? Maybe more to investigate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = vgg16(pretrained=True).to(device)\n",
    "\n",
    "    #for param in model.parameters():\n",
    "    #    param.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential( \n",
    "        nn.Linear(8192, 4096),\n",
    "        # change input layer to this to work with 224x224 images\n",
    "        #nn.Linear(25088, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4096, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 14),\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_RESNET_\n",
    "\n",
    "I haven't got RESNET models yet working with 128x128 images.\n",
    "\n",
    "Currently you can add a transform to resize images to 224x224 for testing, achieving validation f1 score of around 0.6. Note that the dataloading was really whopping slow, so I decided to test just with max 200 images from each class. Something to investigate here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet18(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet34(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model or load an existing model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = '../data/TwoLayerModel_state.dict'\n",
    "model_path = f'../data/ConvNetModel_state{affix}.dict'\n",
    "\n",
    "#model = OneLayerModel(224*224*3, 256, n_classes).to(device)\n",
    "#model = TwoLayerModel(128*128*3, 512, 256, n_classes).to(device)\n",
    "#model = ConvNetModel(n_classes=n_classes, keep_prob=.5).to(device)\n",
    "\n",
    "pos_weight = torch.from_numpy(label_pos_weights_for_loss).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_save_path = '../data/vgg16-small-images-all-data-epoch-11-adam.pth'\n",
    "#torch.save(model.state_dict(), model_save_path)\n",
    "#model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train loss   Valid loss   Train F1  Valid F1 \n",
      "Epoch: 1, iteration: 1, loss: 0.11133764684200287\n",
      "Epoch: 1, iteration: 2, loss: 0.11944253742694855\n",
      "Epoch: 1, iteration: 3, loss: 0.11825712025165558\n",
      "Epoch: 1, iteration: 4, loss: 0.13183706998825073\n",
      "Epoch: 1, iteration: 5, loss: 0.11629673838615417\n",
      "Epoch: 1, iteration: 6, loss: 0.12455219030380249\n",
      "Epoch: 1, iteration: 7, loss: 0.10060308128595352\n",
      "Epoch: 1, iteration: 8, loss: 0.11667706817388535\n",
      "Epoch: 1, iteration: 9, loss: 0.10968227684497833\n",
      "Epoch: 1, iteration: 10, loss: 0.10341773927211761\n",
      "Epoch: 1, iteration: 11, loss: 0.09721873700618744\n",
      "Epoch: 1, iteration: 12, loss: 0.09442616254091263\n",
      "Epoch: 1, iteration: 13, loss: 0.10747431218624115\n",
      "Epoch: 1, iteration: 14, loss: 0.12612053751945496\n",
      "Epoch: 1, iteration: 15, loss: 0.09541165083646774\n",
      "Epoch: 1, iteration: 16, loss: 0.10812090337276459\n",
      "Epoch: 1, iteration: 17, loss: 0.11860832571983337\n",
      "Epoch: 1, iteration: 18, loss: 0.1036168709397316\n",
      "Epoch: 1, iteration: 19, loss: 0.09528942406177521\n",
      "Epoch: 1, iteration: 20, loss: 0.12131373584270477\n",
      "Epoch: 1, iteration: 21, loss: 0.10090187191963196\n",
      "Epoch: 1, iteration: 22, loss: 0.13953274488449097\n",
      "Epoch: 1, iteration: 23, loss: 0.1088496670126915\n",
      "Epoch: 1, iteration: 24, loss: 0.1130223497748375\n",
      "Epoch: 1, iteration: 25, loss: 0.1172763928771019\n",
      "Epoch: 1, iteration: 26, loss: 0.09043920785188675\n",
      "Epoch: 1, iteration: 27, loss: 0.1315988004207611\n",
      "Epoch: 1, iteration: 28, loss: 0.11315443366765976\n",
      "Epoch: 1, iteration: 29, loss: 0.10242827981710434\n",
      "Epoch: 1, iteration: 30, loss: 0.12681210041046143\n",
      "Epoch: 1, iteration: 31, loss: 0.09813982993364334\n",
      "Epoch: 1, iteration: 32, loss: 0.14771853387355804\n",
      "Epoch: 1, iteration: 33, loss: 0.1349930316209793\n",
      "Epoch: 1, iteration: 34, loss: 0.10367801040410995\n",
      "Epoch: 1, iteration: 35, loss: 0.11494497209787369\n",
      "Epoch: 1, iteration: 36, loss: 0.12112190574407578\n",
      "Epoch: 1, iteration: 37, loss: 0.10798884183168411\n",
      "Epoch: 1, iteration: 38, loss: 0.10911471396684647\n",
      "Epoch: 1, iteration: 39, loss: 0.12166384607553482\n",
      "Epoch: 1, iteration: 40, loss: 0.11262796819210052\n",
      "Epoch: 1, iteration: 41, loss: 0.11158861964941025\n",
      "Epoch: 1, iteration: 42, loss: 0.09491774439811707\n",
      "Epoch: 1, iteration: 43, loss: 0.11498197913169861\n",
      "Epoch: 1, iteration: 44, loss: 0.10704134404659271\n",
      "Epoch: 1, iteration: 45, loss: 0.11815405637025833\n",
      "Epoch: 1, iteration: 46, loss: 0.12860316038131714\n",
      "Epoch: 1, iteration: 47, loss: 0.10992134362459183\n",
      "0     0.0970673561 4.4098625183 0.8673242 0.7308899\n",
      "Done training!\n",
      "CPU times: user 1min 22s, sys: 24.5 s, total: 1min 47s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%time train(train_dataloader, valid_dataloader, model, optimizer, criterion, device, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.05, f1 score: 0.7228536605834961\n",
      "threshold: 0.1, f1 score: 0.7233600616455078\n",
      "threshold: 0.15000000000000002, f1 score: 0.724076509475708\n",
      "threshold: 0.2, f1 score: 0.7257993817329407\n",
      "threshold: 0.25, f1 score: 0.7258560061454773\n",
      "threshold: 0.3, f1 score: 0.7269453406333923\n",
      "threshold: 0.35000000000000003, f1 score: 0.7278744578361511\n",
      "threshold: 0.4, f1 score: 0.7288583517074585\n",
      "threshold: 0.45, f1 score: 0.7302254438400269\n",
      "threshold: 0.5, f1 score: 0.7307531237602234\n",
      "threshold: 0.55, f1 score: 0.7323186993598938\n",
      "threshold: 0.6000000000000001, f1 score: 0.7318224906921387\n",
      "threshold: 0.6500000000000001, f1 score: 0.7334409356117249\n",
      "threshold: 0.7000000000000001, f1 score: 0.7331785559654236\n",
      "threshold: 0.7500000000000001, f1 score: 0.7351773381233215\n",
      "threshold: 0.8, f1 score: 0.7345784902572632\n",
      "threshold: 0.8500000000000001, f1 score: 0.7348579168319702\n",
      "threshold: 0.9000000000000001, f1 score: 0.7346250414848328\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is for finding the optimal threshold.\n",
    "# Haven't proved out to be very useful yet.\n",
    "if False:\n",
    "    f1_scores = []\n",
    "    for threshold in np.arange(0.05, 1, 0.05):\n",
    "        _, f1 = evaluate(valid_dataloader, model, criterion, device, thresholds=threshold)\n",
    "        f1_scores.append(f1)\n",
    "        print(f'threshold: {threshold}, f1 score: {f1}')\n",
    "\n",
    "    plt.plot(np.arange(0.05, 0.95, 0.05), f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT DO THIS.\n",
    "# When clicking 'Restart & Run all' this will overwrite the just trained model\n",
    "# PS. Is there a reason for the model.eval() calls here?\n",
    "\n",
    "# try:\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     model.eval()\n",
    "#     print(\"Loaded model\")\n",
    "# except:\n",
    "#     train(train_dataloader, valid_dataloader, model, optimizer, criterion, device, n_epochs=n_epochs)\n",
    "#     model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep training an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: This is better to do by just re-running the earlier training -cell. You can change the number of epochs there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep training existing model with a certain number of epochs\n",
    "# additional_epochs = 5\n",
    "\n",
    "# if additional_epochs > 0:\n",
    "#     model.train()\n",
    "#     train(train_dataloader, valid_dataloader, model, optimizer, criterion, device, n_epochs=additional_epochs)\n",
    "#     model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some images with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, device, valid_dataloader, n_to_show=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re_predict = True\n",
    "\n",
    "if re_predict:\n",
    "\n",
    "    # Predict\n",
    "    y_true, y_pred = predict(model, device, valid_dataloader)\n",
    "    np.save(f'../data/valid_true_labels{affix}.npy', y_true)\n",
    "    np.save(f'../data/valid_pred_labels{affix}.npy', y_pred)\n",
    "    \n",
    "    # Save classification report\n",
    "    with open(f'../data/valid_classification_report{affix}.txt', 'w') as file:\n",
    "        file.write(skm.classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Save confusion matrix plot\n",
    "    labels = [k for k, v in get_class_map().items()]\n",
    "    visualize_confusion_matrix(y_true, y_pred, labels, f'../data/valid_confusion_matrix{affix}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification report\n",
    "with open(f'../data/valid_classification_report{affix}.txt', 'r') as file:\n",
    "    report = ''.join(file.readlines())\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confus|ion matrix plot\n",
    "IPython_Image(filename=f'../data/valid_confusion_matrix{affix}.png', width=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
