{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import vgg16, vgg16_bn, resnet18, resnet34\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from IPython.display import display, clear_output, Image as IPython_Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../data/images'\n",
    "LABEL_PATH = '../data/annotations'\n",
    "\n",
    "# GIVEN DATASET\n",
    "MEAN = (0.43672, 0.40107, 0.36762)\n",
    "STD = (0.30139, 0.28781, 0.29236)\n",
    "\n",
    "# IMAGENET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# RESNET\n",
    "#MEAN = (0.485, 0.456, 0.406)\n",
    "#STD = (0.229, 0.224, 0.225)\n",
    "       \n",
    "DEFAULT_THRESHOLDS = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "# Define default pos_weights for nn.BCEWithLogitsLoss(pos_weights). If the get_dataloader() is run, is will \n",
    "# overwrite this variable based on the statistics of the test set.\n",
    "label_pos_weights_for_loss = np.array([209.52631579, 55.87203791, 58.40594059, 16.77777778, 44.80152672, 5.25, 25.14379085, 5.75675676, 33.09090909, 2.15540363, 5.51465798, 163.38356164, 119., 37.46153846], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_classes():\n",
    "    return len(listdir(LABEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_map():\n",
    "    ret = {}\n",
    "\n",
    "    i = 0\n",
    "    for fname in listdir(LABEL_PATH):\n",
    "        img_class, _ = fname.split('.')\n",
    "        ret[img_class] = i\n",
    "        i += 1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels_to_csv(name_of_set, label_array):\n",
    "    filepath = f'../data/labels_{name_of_set}.csv'\n",
    "    \n",
    "    label_arr = np.array(label_array).astype(int)\n",
    "\n",
    "    # Save 2D numpy array to csv file\n",
    "    np.savetxt(filepath, label_arr, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_statistics(label_array, total_n_images):\n",
    "    label_arr = np.array(label_array).astype(int)\n",
    "    pos_label_counts_all = np.sum(label_arr, axis=0)\n",
    "    neg_label_counts_all = total_n_images - pos_label_counts_all\n",
    "    relative_label_occurrances =  pos_label_counts_all/total_n_images\n",
    "    pos_weights = neg_label_counts_all / pos_label_counts_all\n",
    "    \n",
    "    return pos_weights, pos_label_counts_all, neg_label_counts_all, relative_label_occurrances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_statistics(name_of_set, pos_weights, pos_label_counts_all, neg_label_counts_all, relative_label_occurrances):    \n",
    "    print(f'\\nData set: {name_of_set}')\n",
    "    print(' Positive label counts\\n  ', pos_label_counts_all)\n",
    "    print(' Negative label counts\\n  ', neg_label_counts_all)\n",
    "    print(' Relative label occurrances\\n  ', relative_label_occurrances)\n",
    "    print(' Pos_weights\\n  ', pos_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(bs=256, train_fr=.6, max_images_per_class=1e9, use_no_label_imgs = True, transformations=None):\n",
    "    \n",
    "    # by default only transform to tensor\n",
    "    if transformations is None:\n",
    "        transformations = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    # mapping from class names to integers\n",
    "    class_map = get_class_map()\n",
    "\n",
    "    # create a dictionary to hold our label vectors\n",
    "    n_classes = len(class_map.keys())\n",
    "    img_to_class = defaultdict(lambda: np.zeros(n_classes))\n",
    "\n",
    "    # another dictionary to hold the actual image data\n",
    "    img_to_data = dict()\n",
    "    \n",
    "    # loop through all the annotations\n",
    "    for fname in listdir(LABEL_PATH):\n",
    "        img_class, _ = fname.split('.')\n",
    "        print(f'Reading class: {img_class}')\n",
    "        \n",
    "        # open the annotation file\n",
    "        with open(f'{LABEL_PATH}/{fname}', 'r') as fh:\n",
    "\n",
    "            # get image ids from annotation file\n",
    "            img_ids = fh.read().splitlines()\n",
    "            \n",
    "            # gather the images with labels\n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                \n",
    "                # let's not process images unnecessarily\n",
    "                if not img_id in img_to_data:\n",
    "\n",
    "                    img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "                    img = Image.open(img_path)\n",
    "\n",
    "                    # apply transformations\n",
    "                    img_tensor = transformations(img.convert('RGB')).numpy() \n",
    "\n",
    "                    # append to dict\n",
    "                    img_to_data[img_id] = img_tensor\n",
    "\n",
    "                # get one-hot encoded vector of image classes\n",
    "                img_classes = img_to_class[img_id]\n",
    "\n",
    "                # add new class to image vector\n",
    "                img_class_id = class_map[img_class]\n",
    "                img_classes[img_class_id] = 1\n",
    "\n",
    "                # store the updated vector back\n",
    "                img_to_class[img_id] = img_classes\n",
    "\n",
    "                if i >= max_images_per_class:\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "    # If use_no_label_imgs is on, load also all the images that do not have any labels\n",
    "    i = 0\n",
    "    for fname in listdir(IMAGE_PATH):\n",
    "        m = re.match('im(\\d+)', fname)\n",
    "        img_id = m.group(1)\n",
    "\n",
    "        if img_id not in img_to_data:\n",
    "            img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # apply transformations\n",
    "            img_tensor = transformations(img.convert('RGB')).numpy() \n",
    "\n",
    "            # append to dict\n",
    "            img_to_data[img_id] = img_tensor\n",
    "\n",
    "            if i >= max_images_per_class:\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "    # collect data to a single array\n",
    "    X = []\n",
    "    y = []\n",
    "    for img_id in img_to_data.keys():\n",
    "        X.append(img_to_data[img_id])\n",
    "        y.append(img_to_class[img_id])\n",
    "\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, train_size=train_fr, random_state=42)\n",
    "    X_test, X_valid, y_test, y_valid = train_test_split(X_tmp, y_tmp, train_size=.5, test_size=.5, random_state=42)\n",
    "    \n",
    "    # Write labels to csv file\n",
    "    #write_labels_to_csv('all', y)\n",
    "    #write_labels_to_csv('train', y_train)\n",
    "    #write_labels_to_csv('validation', y_valid)\n",
    "    #write_labels_to_csv('test', y_test)\n",
    "    \n",
    "    # Calculate and print out label statistics to be able to analyze label imbalance\n",
    "    pos_weights_all, pos_label_counts_all, neg_label_counts_all, relative_label_occurrances_all = calculate_label_statistics(y, len(X))\n",
    "    pos_weights_train, pos_label_counts_train, neg_label_counts_train, relative_label_occurrances_train = calculate_label_statistics(y_train, len(X_train))\n",
    "    pos_weights_valid, pos_label_counts_valid, neg_label_counts_valid, relative_label_occurrances_valid = calculate_label_statistics(y_valid, len(X_valid))\n",
    "    pos_weights_test, pos_label_counts_test, neg_label_counts_test, relative_label_occurrances_test = calculate_label_statistics(y_test, len(X_test))\n",
    "    \n",
    "    print_label_statistics('all', pos_weights_all, pos_label_counts_all, neg_label_counts_all, relative_label_occurrances_all)\n",
    "    print_label_statistics('train', pos_weights_train, pos_label_counts_train, neg_label_counts_train, relative_label_occurrances_train)\n",
    "    print_label_statistics('validation', pos_weights_valid, pos_label_counts_valid, neg_label_counts_valid, relative_label_occurrances_valid)\n",
    "    print_label_statistics('test', pos_weights_test, pos_label_counts_test, neg_label_counts_test, relative_label_occurrances_test)\n",
    "    \n",
    "    # Set the pos_weights to be used in the loss function\n",
    "    label_pos_weights_for_loss = pos_weights_train\n",
    "    \n",
    "    train_dataloader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float),\n",
    "        torch.tensor(y_train, dtype=torch.float)),\n",
    "        shuffle=True,\n",
    "        batch_size=bs)           \n",
    "\n",
    "    valid_dataloader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_valid, dtype=torch.float),\n",
    "        torch.tensor(y_valid, dtype=torch.float)),\n",
    "        shuffle=True,\n",
    "        batch_size=bs)\n",
    "\n",
    "    test_dataloader = DataLoader(TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float),\n",
    "        torch.tensor(y_test, dtype=torch.float)),\n",
    "        shuffle=True,\n",
    "        batch_size=bs)      \n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_classes):\n",
    "        super().__init__()\n",
    "        self.bs = bs\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden1 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden2 = nn.Linear(n_hidden2, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.hidden2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class OneLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class ConvNetModel(nn.Module):\n",
    "    def __init__(self, n_classes, keep_prob=.5):\n",
    "        super(ConvNetModel, self).__init__()\n",
    "        # Common layers used multiple times\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=1-keep_prob)\n",
    "        \n",
    "        # Unique layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=3, stride=1, padding=1) #(n samples, channels, height, width)\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=14, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc3 = nn.Linear(in_features=256*4*14, out_features=n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3, 128, 128)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)  # Flatten for FC\n",
    "        out = self.fc3(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, device, thresholds=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    f1_scores = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                score = f1_score(y.cpu() == 1, y_pred.cpu() > thresholds, average='micro')\n",
    "                f1_scores.append(score)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, model, optimizer, criterion, device, n_epochs=50, verbose=True):\n",
    "    model.train()\n",
    "\n",
    "    if verbose:\n",
    "        fmt = '{:<5} {:12} {:12} {:<9} {:<9}'\n",
    "        print(fmt.format('Epoch', 'Train loss', 'Valid loss', 'Train F1', 'Valid F1'))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            X, y = batch\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f'Epoch: {epoch+1}, iteration: {i+1}, loss: {loss}')\n",
    "\n",
    "        if verbose:\n",
    "            train_loss, train_score = evaluate(train_dataloader, model, criterion, device)\n",
    "            valid_loss, valid_score = evaluate(valid_dataloader, model, criterion, device)\n",
    "\n",
    "            fmt = '{:<5} {:03.10f} {:03.10f} {:02.7f} {:02.7f}'\n",
    "            print(fmt.format(epoch, train_loss, valid_loss, train_score, valid_score))\n",
    "            \n",
    "    print('Done training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, device, dataloader, mean=MEAN, std=STD, n_to_show=3, threshold=0.5):\n",
    "    \n",
    "    class_to_label = { v: k for k, v in get_class_map().items() }\n",
    "    \n",
    "    # https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/5\n",
    "    inv_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean = -1 * np.multiply(mean, std), std=np.divide(1, std))\n",
    "    ])\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        n_shown = 0\n",
    "        \n",
    "        X, y = batch\n",
    "        X = X.to(device)\n",
    "\n",
    "        y_pred_raw = model(X).cpu()\n",
    "        y_pred = y_pred_raw > threshold \n",
    "        y = y == 1\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            pred_classes = np.where(y_pred[i] == 1)[0]\n",
    "            true_classes = np.where(y[i] == 1)[0]\n",
    "            \n",
    "            true_classes_str = ', '.join([class_to_label[i] for i in true_classes])\n",
    "            pred_classes_str = ', '.join([class_to_label[i] for i in pred_classes])\n",
    "\n",
    "            img = inv_transform(X[i].cpu())              # inverse transforms\n",
    "            img = img.permute(2, 1, 0)                   # BGR -> RGB\n",
    "            img = np.rot90(img, 3)\n",
    "                                    \n",
    "            plt.title(f'True: {true_classes_str}, Predictions: {pred_classes_str}')\n",
    "            plt.imshow(img)\n",
    "            plt.pause(0.001)\n",
    "\n",
    "            if n_shown >= n_to_show:\n",
    "                return\n",
    "\n",
    "            n_shown += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to use same naming conventions everywhere. Might be a good idea to rename `y_hat` to `y_pred` or vice versa everywhere (it makes it easier to combine the notebooks). I don't care which way it is. Also the `Xs` and `ys` etc.\n",
    "\n",
    "The threshold value here is different than elsewhere. Maybe it should be moved to a global or something?\n",
    "\n",
    "Good idea could be also to check from `visualize_predictions` function how the actual predictions can be easily obtained from `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def predict_X(fr, threshold=0.25):\n",
    "    \n",
    "    y_hat = fr > threshold\n",
    "    \n",
    "    return y_hat if (np.sum(y_hat) > 0) else fr == np.max(fr)\n",
    "\n",
    "def predict(model, device, dataloader):\n",
    "    \n",
    "    ys_all = []  # Array of np.array(14) \n",
    "    y_hats_all = []\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        Xs, ys = batch\n",
    "        Xs = model(Xs.to(device))\n",
    "        y_hats = np.apply_along_axis(predict_X, axis=1, arr=Xs.cpu().detach().numpy())\n",
    "        \n",
    "        y_hats_all.extend(y_hat for y_hat in y_hats)\n",
    "        ys_all.extend(y.numpy() for y in ys==1)\n",
    "\n",
    "    return np.array(ys_all), np.array(y_hats_all)\n",
    "\n",
    "def visualize_confusion_matrix(y_true, y_pred, labels, file_path):\n",
    "\n",
    "    plt.ioff()\n",
    "    \n",
    "    # Get confusion matrices\n",
    "    cn_tensor = skm.multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Get precision, recall, f1-score\n",
    "    scores = skm.classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=5, ncols=3,sharey=True, figsize=(20, 20), \n",
    "                           gridspec_kw={'hspace': 0.3, 'wspace': 0.0})\n",
    "    gn = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    n = cn_tensor[0].sum()\n",
    "    \n",
    "    # Loop all labels\n",
    "    for i, cn_matrix in enumerate(cn_tensor):\n",
    "\n",
    "        j, k = int(i/3), i%3\n",
    "        \n",
    "        # Annotations\n",
    "        annot = np.asarray(\n",
    "            ['{}\\n{:0.0f}\\n{:.2%}'.format(gn[i], x, x/n) for i, x in enumerate(cn_matrix.flatten())]\n",
    "        ).reshape(2,2)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sn.heatmap(cn_matrix, annot=annot, fmt='', cmap='Blues', ax=ax[j, k])\n",
    "        \n",
    "        # Precision, recall, f1-score\n",
    "        title = '{}\\nprec.={:.3}, rec.={:.3}, f1={:.3}'.format(\n",
    "            labels[i], scores[str(i)]['precision'], scores[str(i)]['recall'], scores[str(i)]['f1-score'])\n",
    "        ax[j, k].set_title(title)\n",
    "        \n",
    "    plt.savefig(file_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Using GPU!')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "lr = 0.01\n",
    "n_epochs = 5\n",
    "bs = 256\n",
    "n_classes = len(get_class_map().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save / load dataloaders from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test the models that do not seem to work with 128x128 images change the transformation to do the resize.\n",
    "\n",
    "However, it should be investigated if, for example RESNET, can be made to work with 128x128 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#max_images_per_class = 7000\n",
    "use_no_label_imgs = True\n",
    "max_images_per_class = 200\n",
    "affix = \"_\" + str(max_images_per_class) if max_images_per_class < 1e9 else \"\"\n",
    "\n",
    "transformations = {\n",
    "    'train' : transforms.Compose([\n",
    "            # torchvision.transforms.Resize((224, 224)), # for testing RESNET\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=MEAN, std=STD)\n",
    "        ]),\n",
    "}\n",
    "transformations['valid'] = transformations['test'] = transformations['train']\n",
    "\n",
    "try:\n",
    "    train_dataloader = torch.load(f'../data/train_dataloader{affix}.dat')\n",
    "    valid_dataloader = torch.load(f'../data/valid_dataloader{affix}.dat')\n",
    "    test_dataloader = torch.load(f'../data/test_dataloader{affix}.dat')\n",
    "except:\n",
    "    train_dataloader, valid_dataloader, test_dataloader = get_dataloader(\n",
    "        bs=bs, \n",
    "        transformations=transformations['train'],\n",
    "        max_images_per_class=max_images_per_class,\n",
    "        use_no_label_imgs=use_no_label_imgs\n",
    "    )\n",
    "    torch.save(train_dataloader, f'../data/train_dataloader{affix}.dat')\n",
    "    torch.save(valid_dataloader, f'../data/valid_dataloader{affix}.dat')\n",
    "    torch.save(test_dataloader, f'../data/test_dataloader{affix}.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: The mean and std in transformations most probably need to be the same as for VGG and RESNET. Not 100% sure about this. Something to investigate!\n",
    "\n",
    "More models here: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "\n",
    "If the models do not start to converge, try lowering the learning rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_VGG16_\n",
    "\n",
    "This works with 128x128 and 224x224 images.\n",
    "\n",
    "I think I achieved the best validation f1 score with 128x128 images and vgg16, around 0.76.\n",
    "\n",
    "Surprisingly after quick testing the vgg16_bn (with BatchNorm layers) did not do as well? Maybe more to investigate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = vgg16(pretrained=True).to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential( \n",
    "        nn.Linear(8192, 4096),\n",
    "        # change input layer to this to work with 224x224 images\n",
    "        #nn.Linear(25088, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4096, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 14),\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_RESNET_\n",
    "\n",
    "I haven't got RESNET models yet working with 128x128 images.\n",
    "\n",
    "Currently you can add a transform to resize images to 224x224 for testing, achieving validation f1 score of around 0.6. Note that the dataloading was really whopping slow, so I decided to test just with max 200 images from each class. Something to investigate here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet18(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = resnet34(pretrained=True).to(device)\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(512, 14).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model or load an existing model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = '../data/TwoLayerModel_state.dict'\n",
    "model_path = f'../data/ConvNetModel_state{affix}.dict'\n",
    "\n",
    "#model = OneLayerModel(224*224*3, 256, n_classes).to(device)\n",
    "#model = TwoLayerModel(128*128*3, 512, 256, n_classes).to(device)\n",
    "#model = ConvNetModel(n_classes=n_classes, keep_prob=.5).to(device)\n",
    "\n",
    "pos_weight = torch.from_numpy(label_pos_weights_for_loss).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataloader, valid_dataloader, model, optimizer, criterion, device, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for finding the optimal threshold.\n",
    "# Haven't proved out to be very useful yet.\n",
    "if False:\n",
    "    f1_scores = []\n",
    "    for threshold in np.arange(0.05, 0.95, 0.05):\n",
    "        _, f1 = evaluate(valid_dataloader, model, criterion, device, thresholds=threshold)\n",
    "        f1_scores.append(f1)\n",
    "        print(f'threshold: {threshold}, f1 score: {f1}')\n",
    "\n",
    "    plt.plot(np.arange(0.05, 0.95, 0.05), f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT DO THIS.\n",
    "# When clicking 'Restart & Run all' this will overwrite the just trained model\n",
    "# PS. Is there a reason for the model.eval() calls here?\n",
    "\n",
    "# try:\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     model.eval()\n",
    "#     print(\"Loaded model\")\n",
    "# except:\n",
    "#     train(train_dataloader, valid_dataloader, model, optimizer, criterion, device, n_epochs=n_epochs)\n",
    "#     model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep training an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: This is better to do by just re-running the earlier training -cell. You can change the number of epochs there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep training existing model with a certain number of epochs\n",
    "# additional_epochs = 5\n",
    "\n",
    "# if additional_epochs > 0:\n",
    "#     model.train()\n",
    "#     train(train_dataloader, valid_dataloader, model, optimizer, criterion, device, n_epochs=additional_epochs)\n",
    "#     model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some images with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, device, valid_dataloader, n_to_show=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re_predict = True\n",
    "\n",
    "if re_predict:\n",
    "\n",
    "    # Predict\n",
    "    y_true, y_pred = predict(model, device, valid_dataloader)\n",
    "    np.save(f'../data/valid_true_labels{affix}.npy', y_true)\n",
    "    np.save(f'../data/valid_pred_labels{affix}.npy', y_pred)\n",
    "    \n",
    "    # Save classification report\n",
    "    with open(f'../data/valid_classification_report{affix}.txt', 'w') as file:\n",
    "        file.write(skm.classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Save confusion matrix plot\n",
    "    labels = [k for k, v in get_class_map().items()]\n",
    "    visualize_confusion_matrix(y_true, y_pred, labels, f'../data/valid_confusion_matrix{affix}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification report\n",
    "with open(f'../data/valid_classification_report{affix}.txt', 'r') as file:\n",
    "    report = ''.join(file.readlines())\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confus|ion matrix plot\n",
    "IPython_Image(filename=f'../data/valid_confusion_matrix{affix}.png', width=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
