{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import sys\n",
    "from os import listdir, mkdir\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../data/images'\n",
    "ORIG_LABEL_PATH = '../data/annotations'\n",
    "LABEL_PATH = '../data/annotations2'\n",
    "RANDOM_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_map():\n",
    "    ret = {}\n",
    "\n",
    "    i = 0\n",
    "    for fname in listdir(ORIG_LABEL_PATH):\n",
    "        img_class, _ = fname.split('.')\n",
    "        ret[img_class] = i\n",
    "        i += 1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 367, 158, 0 lines (of 525) in tree.txt (train, validation, test)\n",
      "Wrote 2258, 969, 0 lines (of 3227) in female.txt (train, validation, test)\n",
      "Wrote 532, 229, 0 lines (of 761) in flower.txt (train, validation, test)\n",
      "Wrote 313, 135, 0 lines (of 448) in dog.txt (train, validation, test)\n",
      "Wrote 121, 52, 0 lines (of 173) in sea.txt (train, validation, test)\n",
      "Wrote 4482, 1921, 0 lines (of 6403) in people.txt (train, validation, test)\n",
      "Wrote 2184, 937, 0 lines (of 3121) in portrait.txt (train, validation, test)\n",
      "Wrote 223, 96, 0 lines (of 319) in car.txt (train, validation, test)\n",
      "Wrote 766, 329, 0 lines (of 1095) in clouds.txt (train, validation, test)\n",
      "Wrote 2085, 894, 0 lines (of 2979) in male.txt (train, validation, test)\n",
      "Wrote 84, 36, 0 lines (of 120) in river.txt (train, validation, test)\n",
      "Wrote 252, 108, 0 lines (of 360) in bird.txt (train, validation, test)\n",
      "Wrote 418, 180, 0 lines (of 598) in night.txt (train, validation, test)\n",
      "Wrote 66, 29, 0 lines (of 95) in baby.txt (train, validation, test)\n"
     ]
    }
   ],
   "source": [
    "def train_valid_test_split(train_fr=.7, valid_fr=.3, test_fr=0):\n",
    "    \"\"\"\n",
    "    Creates subfolders train, test, validation. \n",
    "    Splits annotation files into train, validation and test sets.\n",
    "    \"\"\"\n",
    "    random.seed(RANDOM_SEED)\n",
    "    \n",
    "    try:\n",
    "        mkdir(f'{LABEL_PATH}')\n",
    "        mkdir(f'{LABEL_PATH}/train')\n",
    "        mkdir(f'{LABEL_PATH}/test')\n",
    "        mkdir(f'{LABEL_PATH}/validation')\n",
    "    except:\n",
    "        pass\n",
    "      \n",
    "    # Make the split\n",
    "    for fname in listdir(ORIG_LABEL_PATH):\n",
    "        with open(f'{ORIG_LABEL_PATH}/{fname}', 'r') as fh:\n",
    "            img_ids = fh.read().splitlines()\n",
    "            random.shuffle(img_ids)\n",
    "            split1, split2 = math.ceil(test_fr * len(img_ids)), math.ceil(valid_fr * len(img_ids))\n",
    "            test_ids, valid_ids, train_ids = img_ids[:split1], img_ids[split1:split2], img_ids[split2:]\n",
    "            with open(f'{LABEL_PATH}/test/{fname}', 'w') as outfile:\n",
    "                outfile.write('\\n'.join(test_ids))\n",
    "            with open(f'{LABEL_PATH}/validation/{fname}', 'w') as outfile:\n",
    "                outfile.write('\\n'.join(valid_ids))\n",
    "            with open(f'{LABEL_PATH}/train/{fname}', 'w') as outfile:\n",
    "                outfile.writelines('\\n'.join(train_ids))\n",
    "            print(f'Wrote {len(train_ids)}, {len(valid_ids)}, {len(test_ids)} lines (of {len(img_ids)}) in {fname} (train, validation, test)')\n",
    "\n",
    "# Need to run only once. \n",
    "train_valid_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(set_path, bs=64, max_items=256):\n",
    "    data = []\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # mapping from class names to integers\n",
    "    class_map = get_class_map()\n",
    "\n",
    "    # loop through all the annotations\n",
    "    for fname in listdir(set_path):\n",
    "\n",
    "        img_class, _ = fname.split('.')\n",
    "        clear_output(wait=True)\n",
    "        display(f'Reading set: {set_path}, class: {img_class}')\n",
    "        #print(f'Reading class: {img_class}')\n",
    "\n",
    "        # open the annotation\n",
    "        with open(f'{set_path}/{fname}', 'r') as fh:\n",
    "\n",
    "            # get image ids from annotation file\n",
    "            img_ids = fh.read().splitlines()\n",
    "\n",
    "            # gather the images with labels\n",
    "            i = 0\n",
    "            for img_id in img_ids:\n",
    "                img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "                img = Image.open(img_path)\n",
    "                img_data = np.asarray(img)\n",
    "\n",
    "                # skip black-and-white images\n",
    "                if not len(img_data.shape) == 3:\n",
    "                    continue\n",
    "\n",
    "                img_data = img_data.flatten().astype(np.float32)\n",
    "\n",
    "                data.append([img_data, class_map[img_class]])\n",
    "\n",
    "                if i > max_items: break\n",
    "                i += 1\n",
    "                \n",
    "    return DataLoader(data, batch_size=bs, shuffle=True)\n",
    "\n",
    "def get_dataloaders(bs=64, max_items=256):\n",
    "   \n",
    "    train_loader = get_dataloader(f'{LABEL_PATH}/train', bs, max_items)\n",
    "    validation_loader = get_dataloader(f'{LABEL_PATH}/validation', bs, max_items)\n",
    "    #test_loader = get_dataloader(f'{LABEL_PATH}/test', bs, max_items)\n",
    "    \n",
    "    print(f'Read {len(train_loader.dataset)}, {len(validation_loader.dataset)} items in train, validation sets')\n",
    "    \n",
    "    return train_loader, validation_loader#, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden1 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden2 = nn.Linear(n_hidden2, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.hidden2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_input)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, device, n_epochs=50, losses=[]):\n",
    "\n",
    "    log = []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            X, y = batch\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss)\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            display(f'Epoch: {epoch+1}, iteration: {i+1}, loss: {loss}')\n",
    "\n",
    "        log.append(f'Epoch: {epoch+1}, loss: {loss}')\n",
    "    \n",
    "    print('\\n'.join(log))\n",
    "    print('\\nDone training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "\n",
    "device = torch.device('cuda') if use_cuda else torch.device('cpu')\n",
    "\n",
    "lr = 0.01\n",
    "n_epochs = 2 #10\n",
    "bs = 64 #256\n",
    "max_items = 256 #sys.maxsize\n",
    "\n",
    "n_classes = len(get_class_map().keys())\n",
    "\n",
    "#model = TwoLayerModel(128*128*3, 128, 64, n_classes).to(device)\n",
    "model = OneLayerModel(128*128*3, 128, n_classes).to(device)   # (self, D_in, H, D_out):\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reading set: ../data/annotations2/validation, class: baby'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 3034, 2262 items in train, validation sets\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, validation_dataloader = get_dataloaders(bs, max_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch: 1, iteration: 47, loss: 2.074885368347168'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.9990369081497192\n",
      "Epoch: 1, loss: 2.074885368347168\n",
      "\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, model, optimizer, criterion, device, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Adv ml",
   "language": "python",
   "name": "advml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
