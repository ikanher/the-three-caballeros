{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../data/images'\n",
    "NUMBER_OF_IMAGES = 20000\n",
    "LABEL_PATH = '../data/annotations'\n",
    "NUMBER_OF_CLASSES = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_map():\n",
    "    classnametoint = {}\n",
    "    inttoclassname = {}\n",
    "\n",
    "    i = 0\n",
    "    label_files = sorted(listdir(LABEL_PATH))\n",
    "    for fname in label_files:\n",
    "        img_class, _ = fname.split('.')\n",
    "        classnametoint[img_class] = i\n",
    "        inttoclassname[i] = img_class\n",
    "        i += 1\n",
    "\n",
    "    return classnametoint, inttoclassname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "#   bs: batch size (default 64)\n",
    "#   samples_to_print: number of samples to print when getting dataloader (default 0)\n",
    "\n",
    "def get_dataloader(bs=64, samples_to_print=0):\n",
    "    data = []\n",
    "    # 2d array where each row is an image and each column is a boolean indicating if label is active. \n",
    "    # Row number is image number. Row 0 will not be used since there is no image number 0.\n",
    "    class_array = np.zeros((NUMBER_OF_IMAGES + 1, NUMBER_OF_CLASSES), dtype=bool)\n",
    "                \n",
    "    # mapping from class names to integers\n",
    "    class_map, class_int_to_string = get_class_map()\n",
    "\n",
    "    # loop through all the annotations\n",
    "    label_files = sorted(listdir(LABEL_PATH))\n",
    "    for fname in label_files:\n",
    "        img_class, _ = fname.split('.')\n",
    "        class_int = class_map[img_class]\n",
    "        \n",
    "        # open the annotation\n",
    "        with open(f'{LABEL_PATH}/{fname}', 'r') as fh:\n",
    "\n",
    "            # get image ids from annotation file\n",
    "            img_ids = fh.read().splitlines()\n",
    "            \n",
    "            # For the image, set the bool corresponding to the class to True\n",
    "            for im_id in img_ids:\n",
    "                class_array[int(im_id)][class_int] = True\n",
    "    \n",
    "    # Now we have a complete array of image labels in class_array\n",
    "    #print(class_array[0:20,])\n",
    "    \n",
    "    # Let's iterate through the images and attach the labels vector to each image\n",
    "    nblackwhite = 0\n",
    "    nsinglechannel = 0 \n",
    "    ntwochannel = 0\n",
    "    nfourormorechannels = 0\n",
    "    skippedids = []\n",
    "    for img_id in range(1, NUMBER_OF_IMAGES + 1):\n",
    "        img_path = f'{IMAGE_PATH}/im{img_id}.jpg'\n",
    "        img = Image.open(img_path)\n",
    "        #img_grayscale = img.convert(\"L\") \n",
    "        img_rgb = img.convert(\"RGB\")\n",
    "        img_data = np.asarray(img_rgb)\n",
    "\n",
    "        # skip black-and-white images\n",
    "        imshape = img_data.shape\n",
    "        if not len(imshape) == 3:\n",
    "            if len(imshape) == 1:\n",
    "                nsinglechannel += 1\n",
    "            elif len(imshape) == 2:\n",
    "                ntwochannel += 1\n",
    "            elif len(imshape) > 3:\n",
    "                nfourormorechannels += 1\n",
    "            nblackwhite += 1\n",
    "            skippedids.append(img_id)\n",
    "            continue\n",
    "\n",
    "        img_data = img_data.flatten().astype(np.float32)\n",
    "\n",
    "        data.append([img_data, class_array[img_id]])\n",
    "\n",
    "    print(f'Skipped {nblackwhite} images that were black and white.')\n",
    "    print(f'   of which {nsinglechannel} images had one channel.')\n",
    "    print(f'   of which {ntwochannel} images had two channels.')\n",
    "    print(f'   of which {nfourormorechannels} images had 4 or more channels.')\n",
    "    \n",
    "    # Print some samples of the data items if samples_to_print is set\n",
    "    if (samples_to_print > 0):\n",
    "        print(f'\\nFirst {samples_to_print} data items and their labels:')\n",
    "        for i in range(samples_to_print):\n",
    "            print(f'\\nImage number {i+1}')\n",
    "            print(data[i])\n",
    "            print('   Labels in text:')\n",
    "            print('      ', end = ' ')\n",
    "            for lab in range(NUMBER_OF_CLASSES):\n",
    "                if(data[i][1][lab]):\n",
    "                    print(class_int_to_string[lab], end = ' ')\n",
    "            print('')\n",
    "        \n",
    "    return DataLoader(data, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden1 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden2 = nn.Linear(n_hidden2, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        #x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, device, n_epochs=50, losses=[]):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            X, y = batch\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step\n",
    "            \n",
    "            losses.append(loss)\n",
    "\n",
    "        print(f'Epoch: {epoch}, loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-553476a02839>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mntoshow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdataloaderforvisu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntoshow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;31m#class_map_forvisu = get_class_map()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_class_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d488fcef47ef>\u001b[0m in \u001b[0;36mget_dataloader\u001b[1;34m(bs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mclass_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Img number as key, numpy bool array for labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# mapping from class names to integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "# NOT WORKING YET\n",
    "# Visualize some samples \n",
    "# Taken from transfer learning tutorial https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    #inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #mean = np.array([0.485, 0.456, 0.406])\n",
    "    #std = np.array([0.229, 0.224, 0.225])\n",
    "    #inp = std * inp + mean\n",
    "    #inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "ntoshow = 1\n",
    "dataloaderforvisu = get_dataloader(ntoshow)\n",
    "#class_map_forvisu = get_class_map()\n",
    "_, class_names = get_class_map()\n",
    "    \n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaderforvisu))\n",
    "\n",
    "inputs = inputs.reshape((ntoshow, 128, 128, 3))\n",
    "print(\"Inputs shape\")\n",
    "print(inputs.shape)\n",
    "print('Inputs:')\n",
    "print(inputs)\n",
    "\n",
    "print(\"Array data type\")\n",
    "print(inputs.dtype)\n",
    "\n",
    "print('Classes:')\n",
    "print(classes)\n",
    "print(\"Classes shape\")\n",
    "print(classes.shape)\n",
    "\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "\n",
    "imshow(out)\n",
    "# TODO: show multiple labels per image\n",
    "#imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "device = torch.device('cuda') if use_cuda else torch.device('cpu')\n",
    "\n",
    "lr = 0.05\n",
    "n_epochs = 5\n",
    "bs = 256\n",
    "class_map, _ = get_class_map()\n",
    "n_classes = len(class_map.keys())\n",
    "\n",
    "model = TwoLayerModel(128*128*3, 1024, 512, n_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 images that were black and white.\n",
      "   of which 0 images had one channel.\n",
      "   of which 0 images had two channels.\n",
      "   of which 0 images had 4 or more channels.\n",
      "\n",
      "First 1 data items and their labels:\n",
      "\n",
      "Image number 1\n",
      "[array([201., 196., 177., ...,   8.,  13.,   6.], dtype=float32), array([False, False, False, False, False,  True, False, False, False,\n",
      "        True,  True, False, False, False])]\n",
      "   Labels in text:\n",
      "       female people portrait \n"
     ]
    }
   ],
   "source": [
    "dataloader = get_dataloader(bs, samples_to_print=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataloader, model, optimizer, criterion, device, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
